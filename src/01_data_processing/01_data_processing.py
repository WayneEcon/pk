#!/usr/bin/env python3
"""
ç¾å›½èƒ½æºç‹¬ç«‹æ”¿ç­–çš„å›½é™…å½±å“ç ”ç©¶ - æ•°æ®å¤„ç†è„šæœ¬

åŠŸèƒ½ï¼š
1. è¯»å–å¹¶åˆå¹¶2001-2024å¹´UN ComtradeåŸå§‹æ•°æ®
2. ç­›é€‰å››å¤§èƒ½æºäº§å“(2701ç…¤ç‚­, 2709åŸæ²¹, 2710æˆå“æ²¹, 2711å¤©ç„¶æ°”)
3. æ ‡å‡†åŒ–å›½å®¶ä»£ç å’Œæ¸…ç†å¼‚å¸¸å€¼
4. ç”Ÿæˆæ¸…æ´—åçš„æ•°æ®é›†ä¾›ç½‘ç»œåˆ†æä½¿ç”¨

ä½œè€…ï¼šç ”ç©¶å›¢é˜Ÿ
åˆ›å»ºæ—¥æœŸï¼š2025-08-13
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥å›½å®¶ä»£ç éªŒè¯æ¨¡å—
from country_code_validator import filter_valid_trade_data, get_data_quality_report

def setup_directories():
    """åˆ›å»ºå¿…è¦çš„è¾“å‡ºç›®å½•"""
    base_dir = Path(__file__).parent.parent.parent  # é¡¹ç›®æ ¹ç›®å½•
    directories = [
        base_dir / "data" / "processed_data",
        base_dir / "outputs" / "figures",
        base_dir / "outputs" / "tables",
        base_dir / "logs"
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
    
    return base_dir

def load_raw_data(raw_data_dir):
    """
    è¯»å–å¹¶åˆå¹¶æ‰€æœ‰å¹´ä»½çš„åŸå§‹æ•°æ®
    
    å‚æ•°:
        raw_data_dir: åŸå§‹æ•°æ®ç›®å½•è·¯å¾„
    
    è¿”å›:
        DataFrame: åˆå¹¶åçš„åŸå§‹æ•°æ®
    """
    print("æ­£åœ¨è¯»å–åŸå§‹æ•°æ®...")
    
    all_data = []
    years = range(2001, 2025)
    
    for year in years:
        file_path = raw_data_dir / f"{year}.csv"
        if file_path.exists():
            try:
                df = pd.read_csv(file_path, low_memory=False)
                print(f"  - {year}: {len(df):,} æ¡è®°å½•")
                all_data.append(df)
            except Exception as e:
                print(f"  - {year}: è¯»å–å¤±è´¥ - {e}")
        else:
            print(f"  - {year}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    if not all_data:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_data = pd.concat(all_data, ignore_index=True)
    print(f"\nåŸå§‹æ•°æ®åˆå¹¶å®Œæˆï¼š{len(combined_data):,} æ¡è®°å½•")
    
    return combined_data

def filter_energy_products(df):
    """
    ç­›é€‰å››å¤§èƒ½æºäº§å“çš„è´¸æ˜“æ•°æ®
    
    å‚æ•°:
        df: åŸå§‹è´¸æ˜“æ•°æ®DataFrame
    
    è¿”å›:
        DataFrame: ç­›é€‰åçš„èƒ½æºäº§å“æ•°æ®
    """
    print("\næ­£åœ¨ç­›é€‰èƒ½æºäº§å“...")
    
    # å››å¤§èƒ½æºäº§å“çš„HSä»£ç ï¼ˆè½¬æ¢ä¸ºæ•´æ•°ç±»å‹ï¼‰
    energy_codes = [2701, 2709, 2710, 2711]
    
    # ç­›é€‰æ¡ä»¶
    energy_data = df[df['cmdCode'].isin(energy_codes)].copy()
    
    print(f"ç­›é€‰åçš„èƒ½æºæ•°æ®ï¼š{len(energy_data):,} æ¡è®°å½•")
    
    # å„äº§å“åˆ†å¸ƒç»Ÿè®¡
    product_stats = energy_data.groupby(['cmdCode', 'cmdDesc']).size().reset_index(name='è®°å½•æ•°')
    print("\nå„èƒ½æºäº§å“åˆ†å¸ƒï¼š")
    for _, row in product_stats.iterrows():
        print(f"  - {row['cmdCode']}: {row['è®°å½•æ•°']:,} æ¡ ({row['cmdDesc']})")
    
    return energy_data

def apply_country_whitelist_filter(df):
    """
    åº”ç”¨æƒå¨å›½å®¶ä»£ç ç™½åå•è¿‡æ»¤ï¼Œå‰”é™¤åŒºåŸŸæ€§æ±‡æ€»å®ä½“
    
    è¿™æ˜¯æ•°æ®æºæ±¡æŸ“ä¿®å¤çš„æ ¸å¿ƒæ­¥éª¤ï¼Œç¡®ä¿åªä¿ç•™çœŸå®çš„å›½å®¶/åœ°åŒºå®ä½“
    
    å‚æ•°:
        df: åŸå§‹è´¸æ˜“æ•°æ®DataFrame
    
    è¿”å›:
        DataFrame: è¿‡æ»¤åçš„å¹²å‡€æ•°æ®
    """
    print("\nğŸ” ç¬¬ä¸€æ­¥ï¼šåº”ç”¨æƒå¨å›½å®¶ä»£ç ç™½åå•è¿‡æ»¤...")
    print("    ç›®æ ‡ï¼šå‰”é™¤æ‰€æœ‰åŒºåŸŸæ€§æ±‡æ€»å®ä½“ (å¦‚ EU-27, Africa nes ç­‰)")
    
    # ç”Ÿæˆè¿‡æ»¤å‰çš„æ•°æ®è´¨é‡æŠ¥å‘Š
    print("\nğŸ“Š è¿‡æ»¤å‰æ•°æ®è´¨é‡åˆ†æ:")
    pre_filter_report = get_data_quality_report(df)
    print(f"    - æ€»å®ä½“æ•°: {pre_filter_report['total_entities']}")
    print(f"    - æœ‰æ•ˆå›½å®¶æ•°: {pre_filter_report['valid_entities_count']}")
    print(f"    - åŒºåŸŸæ±‡æ€»å®ä½“æ•°: {pre_filter_report['invalid_entities_count']}")
    print(f"    - æ•°æ®æ±¡æŸ“ç‡: {(1-pre_filter_report['valid_ratio'])*100:.1f}%")
    
    if pre_filter_report['invalid_entities_count'] > 0:
        print(f"    - æ£€æµ‹åˆ°çš„åŒºåŸŸæ±‡æ€»å®ä½“ç¤ºä¾‹: {pre_filter_report['invalid_entities_list'][:15]}")
    
    # åº”ç”¨ä¸¥æ ¼è¿‡æ»¤
    df_filtered = filter_valid_trade_data(df, 'reporterISO', 'partnerISO')
    
    # ç”Ÿæˆè¿‡æ»¤åçš„è´¨é‡æŠ¥å‘Š
    print("\nâœ… è¿‡æ»¤å®Œæˆï¼Œæ•°æ®æºæ±¡æŸ“é—®é¢˜å·²ä¿®å¤")
    
    return df_filtered

def clean_country_codes(df):
    """
    æ¸…ç†å’Œæ ‡å‡†åŒ–å›½å®¶ä»£ç ï¼ˆåœ¨ç™½åå•è¿‡æ»¤åè¿›è¡ŒåŸºç¡€æ¸…ç†ï¼‰
    
    å‚æ•°:
        df: å·²é€šè¿‡ç™½åå•è¿‡æ»¤çš„èƒ½æºè´¸æ˜“æ•°æ®DataFrame
    
    è¿”å›:
        DataFrame: æ¸…ç†åçš„æ•°æ®
    """
    print("\nğŸ§¹ ç¬¬äºŒæ­¥ï¼šåŸºç¡€å›½å®¶ä»£ç æ¸…ç†...")
    
    initial_count = len(df)
    
    # ç§»é™¤è‡ªè´¸æ˜“è®°å½•ï¼ˆåŒä¸€å›½å®¶å†…éƒ¨è´¸æ˜“ï¼‰
    df = df[df['reporterISO'] != df['partnerISO']].copy()
    self_trade_removed = initial_count - len(df)
    print(f"    - ç§»é™¤è‡ªè´¸æ˜“è®°å½•ï¼š{self_trade_removed:,} æ¡")
    
    # ç§»é™¤ç¼ºå¤±å›½å®¶ä»£ç çš„è®°å½•
    before_iso_clean = len(df)
    df = df.dropna(subset=['reporterISO', 'partnerISO']).copy()
    iso_missing_removed = before_iso_clean - len(df)
    print(f"    - ç§»é™¤ç¼ºå¤±ISOä»£ç è®°å½•ï¼š{iso_missing_removed:,} æ¡")
    
    # ç§»é™¤ISOä»£ç é•¿åº¦ä¸ç­‰äº3çš„è®°å½•
    before_iso_length = len(df)
    df = df[(df['reporterISO'].str.len() == 3) & (df['partnerISO'].str.len() == 3)].copy()
    iso_length_removed = before_iso_length - len(df)
    print(f"    - ç§»é™¤ISOä»£ç æ ¼å¼é”™è¯¯è®°å½•ï¼š{iso_length_removed:,} æ¡")
    
    # ç»Ÿè®¡å”¯ä¸€å›½å®¶æ•°
    unique_reporters = df['reporterISO'].nunique()
    unique_partners = df['partnerISO'].nunique()
    all_countries = pd.concat([df['reporterISO'], df['partnerISO']]).nunique()
    
    print(f"\nğŸ“ˆ æ¸…ç†åç»Ÿè®¡:")
    print(f"    - æŠ¥å‘Šå›½æ•°é‡ï¼š{unique_reporters}")
    print(f"    - è´¸æ˜“ä¼™ä¼´å›½æ•°é‡ï¼š{unique_partners}")
    print(f"    - æ€»ä½“å›½å®¶æ•°é‡ï¼š{all_countries}")
    print(f"    - é¢„æœŸèŒƒå›´ï¼š180-210ä¸ªå›½å®¶/åœ°åŒº (vs ä¿®å¤å‰çš„230+)")
    
    return df

def clean_trade_values(df):
    """
    æ¸…ç†è´¸æ˜“ä»·å€¼æ•°æ®
    
    å‚æ•°:
        df: èƒ½æºè´¸æ˜“æ•°æ®DataFrame
    
    è¿”å›:
        DataFrame: æ¸…ç†åçš„æ•°æ®
    """
    print("\næ­£åœ¨æ¸…ç†è´¸æ˜“ä»·å€¼æ•°æ®...")
    
    initial_count = len(df)
    
    # é€‰æ‹©ä¸»è¦ä»·å€¼å­—æ®µï¼Œä¼˜å…ˆä½¿ç”¨primaryValue
    df['trade_value'] = df['primaryValue']
    
    # å¦‚æœprimaryValueç¼ºå¤±ï¼Œä½¿ç”¨cifvalueæˆ–fobvalue
    mask_missing_primary = df['trade_value'].isna()
    df.loc[mask_missing_primary, 'trade_value'] = df.loc[mask_missing_primary, 'cifvalue']
    
    mask_still_missing = df['trade_value'].isna()
    df.loc[mask_still_missing, 'trade_value'] = df.loc[mask_still_missing, 'fobvalue']
    
    # ç§»é™¤ä»ç„¶ç¼ºå¤±è´¸æ˜“ä»·å€¼çš„è®°å½•
    before_value_clean = len(df)
    df = df.dropna(subset=['trade_value']).copy()
    value_missing_removed = before_value_clean - len(df)
    print(f"  - ç§»é™¤ç¼ºå¤±è´¸æ˜“ä»·å€¼è®°å½•ï¼š{value_missing_removed:,} æ¡")
    
    # ç§»é™¤è´Ÿå€¼å’Œé›¶å€¼
    before_positive = len(df)
    df = df[df['trade_value'] > 0].copy()
    non_positive_removed = before_positive - len(df)
    print(f"  - ç§»é™¤éæ­£å€¼è®°å½•ï¼š{non_positive_removed:,} æ¡")
    
    # ç§»é™¤å¼‚å¸¸å¤§å€¼ï¼ˆä½¿ç”¨99.9%åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼ï¼‰
    threshold = df['trade_value'].quantile(0.999)
    before_outlier = len(df)
    df = df[df['trade_value'] <= threshold].copy()
    outlier_removed = before_outlier - len(df)
    print(f"  - ç§»é™¤å¼‚å¸¸å¤§å€¼è®°å½•ï¼ˆ>{threshold:,.0f}ç¾å…ƒï¼‰ï¼š{outlier_removed:,} æ¡")
    
    # ç»Ÿè®¡è´¸æ˜“ä»·å€¼åˆ†å¸ƒ
    print(f"\nè´¸æ˜“ä»·å€¼ç»Ÿè®¡ï¼š")
    print(f"  - æœ€å°å€¼ï¼š${df['trade_value'].min():,.0f}")
    print(f"  - ä¸­ä½æ•°ï¼š${df['trade_value'].median():,.0f}")
    print(f"  - å¹³å‡å€¼ï¼š${df['trade_value'].mean():,.0f}")
    print(f"  - æœ€å¤§å€¼ï¼š${df['trade_value'].max():,.0f}")
    
    return df

def create_final_dataset(df):
    """
    åˆ›å»ºæœ€ç»ˆçš„æ¸…æ´—æ•°æ®é›†
    
    å‚æ•°:
        df: æ¸…ç†åçš„èƒ½æºè´¸æ˜“æ•°æ®
    
    è¿”å›:
        DataFrame: æœ€ç»ˆæ•°æ®é›†
    """
    print("\næ­£åœ¨åˆ›å»ºæœ€ç»ˆæ•°æ®é›†...")
    
    # é€‰æ‹©æ ¸å¿ƒå­—æ®µ
    core_columns = [
        'refYear', 'reporterISO', 'reporterDesc', 'partnerISO', 'partnerDesc',
        'flowCode', 'flowDesc', 'cmdCode', 'cmdDesc', 'trade_value'
    ]
    
    # ç¡®ä¿æ‰€æœ‰æ ¸å¿ƒå­—æ®µéƒ½å­˜åœ¨
    available_columns = [col for col in core_columns if col in df.columns]
    final_df = df[available_columns].copy()
    
    # é‡å‘½åå­—æ®µä»¥ä¾¿åç»­åˆ†æ
    rename_mapping = {
        'refYear': 'year',
        'reporterISO': 'reporter',
        'reporterDesc': 'reporter_name',
        'partnerISO': 'partner',
        'partnerDesc': 'partner_name',
        'flowCode': 'flow',
        'flowDesc': 'flow_name',
        'cmdCode': 'product_code',
        'cmdDesc': 'product_name',
        'trade_value': 'trade_value_raw_usd'
    }
    
    final_df = final_df.rename(columns=rename_mapping)
    
    # æ’åº
    final_df = final_df.sort_values(['year', 'reporter', 'partner', 'product_code']).reset_index(drop=True)
    
    print(f"æœ€ç»ˆæ•°æ®é›†ï¼š{len(final_df):,} æ¡è®°å½•")
    print(f"æ—¶é—´è·¨åº¦ï¼š{final_df['year'].min()} - {final_df['year'].max()}")
    
    return final_df

def generate_data_summary(df, output_dir):
    """
    ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Šå’Œç»Ÿè®¡æ‘˜è¦
    
    å‚æ•°:
        df: æœ€ç»ˆæ¸…æ´—åçš„æ•°æ®é›†
        output_dir: è¾“å‡ºç›®å½•
    """
    print("\næ­£åœ¨ç”Ÿæˆæ•°æ®ç»Ÿè®¡æ‘˜è¦...")
    
    # å¹´åº¦ç»Ÿè®¡
    yearly_stats = df.groupby('year').agg({
        'trade_value_raw_usd': ['count', 'sum', 'mean'],
        'reporter': 'nunique',
        'partner': 'nunique'
    }).round(2)
    
    yearly_stats.columns = ['è®°å½•æ•°', 'æ€»è´¸æ˜“é¢(ç¾å…ƒ)', 'å¹³å‡è´¸æ˜“é¢(ç¾å…ƒ)', 'æŠ¥å‘Šå›½æ•°', 'ä¼™ä¼´å›½æ•°']
    yearly_stats['æ€»è´¸æ˜“é¢(åäº¿ç¾å…ƒ)'] = yearly_stats['æ€»è´¸æ˜“é¢(ç¾å…ƒ)'] / 1e9
    
    # äº§å“ç»Ÿè®¡
    product_stats = df.groupby(['product_code', 'product_name']).agg({
        'trade_value_raw_usd': ['count', 'sum'],
        'year': ['min', 'max']
    }).round(2)
    
    product_stats.columns = ['è®°å½•æ•°', 'æ€»è´¸æ˜“é¢(ç¾å…ƒ)', 'å¼€å§‹å¹´', 'ç»“æŸå¹´']
    product_stats['æ€»è´¸æ˜“é¢(åäº¿ç¾å…ƒ)'] = product_stats['æ€»è´¸æ˜“é¢(ç¾å…ƒ)'] / 1e9
    
    # ä¸»è¦è´¸æ˜“å›½ç»Ÿè®¡ï¼ˆæŒ‰æ€»è´¸æ˜“é¢æ’åºï¼‰
    country_stats = df.groupby(['reporter', 'reporter_name'])['trade_value_raw_usd'].sum().sort_values(ascending=False).head(20)
    country_stats = country_stats / 1e9  # è½¬æ¢ä¸ºåäº¿ç¾å…ƒ
    country_stats.name = 'æ€»å‡ºå£é¢(åäº¿ç¾å…ƒ)'
    
    # ä¿å­˜ç»Ÿè®¡ç»“æœ
    with pd.ExcelWriter(output_dir / 'tables' / 'data_summary.xlsx') as writer:
        yearly_stats.to_excel(writer, sheet_name='å¹´åº¦ç»Ÿè®¡')
        product_stats.to_excel(writer, sheet_name='äº§å“ç»Ÿè®¡')
        country_stats.to_excel(writer, sheet_name='ä¸»è¦å‡ºå£å›½')
    
    # ä¿å­˜CSVæ ¼å¼
    yearly_stats.to_csv(output_dir / 'tables' / 'yearly_statistics.csv')
    product_stats.to_csv(output_dir / 'tables' / 'product_statistics.csv')
    country_stats.to_csv(output_dir / 'tables' / 'top_exporters.csv')
    
    print("ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜åˆ° outputs/tables/ ç›®å½•")
    
    return yearly_stats, product_stats, country_stats

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ç¾å›½èƒ½æºç‹¬ç«‹æ”¿ç­–çš„å›½é™…å½±å“ç ”ç©¶ - æ•°æ®å¤„ç†")
    print("=" * 60)
    
    # è®¾ç½®ç›®å½•
    base_dir = setup_directories()
    raw_data_dir = base_dir / "data" / "raw_data"
    processed_data_dir = base_dir / "data" / "processed_data"
    output_dir = base_dir / "outputs"
    
    try:
        # æ­¥éª¤1ï¼šè¯»å–åŸå§‹æ•°æ®
        raw_data = load_raw_data(raw_data_dir)
        
        # æ­¥éª¤2ï¼šç­›é€‰èƒ½æºäº§å“
        energy_data = filter_energy_products(raw_data)
        
        # æ­¥éª¤3ï¼šåº”ç”¨æƒå¨å›½å®¶ä»£ç ç™½åå•è¿‡æ»¤ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰
        whitelist_filtered_data = apply_country_whitelist_filter(energy_data)
        
        # æ­¥éª¤4ï¼šåŸºç¡€å›½å®¶ä»£ç æ¸…ç†
        cleaned_country_data = clean_country_codes(whitelist_filtered_data)
        
        # æ­¥éª¤5ï¼šæ¸…ç†è´¸æ˜“ä»·å€¼
        cleaned_value_data = clean_trade_values(cleaned_country_data)
        
        # æ­¥éª¤6ï¼šåˆ›å»ºæœ€ç»ˆæ•°æ®é›†
        final_dataset = create_final_dataset(cleaned_value_data)
        
        # æ­¥éª¤7ï¼šæŒ‰å¹´åº¦ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        print(f"\næ­£åœ¨æŒ‰å¹´åº¦ä¿å­˜æ¸…æ´—åçš„æ•°æ®...")
        for year in range(2001, 2025):
            year_data = final_dataset[final_dataset['year'] == year]
            if len(year_data) > 0:
                output_file = processed_data_dir / f"cleaned_energy_trade_{year}.csv"
                year_data.to_csv(output_file, index=False)
                print(f"  - {year}: {len(year_data):,} æ¡è®°å½• -> {output_file.name}")
        
        # æ­¥éª¤8ï¼šç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        yearly_stats, product_stats, country_stats = generate_data_summary(final_dataset, output_dir)
        
        # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("æ•°æ®å¤„ç†å®Œæˆï¼å…³é”®ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print("=" * 60)
        print(f"âœ… æ•°æ®æºæ±¡æŸ“ä¿®å¤å®Œæˆï¼")
        print(f"æ€»è®°å½•æ•°ï¼š{len(final_dataset):,}")
        print(f"æ—¶é—´è·¨åº¦ï¼š{final_dataset['year'].min()} - {final_dataset['year'].max()}")
        print(f"å›½å®¶æ•°é‡ï¼š{pd.concat([final_dataset['reporter'], final_dataset['partner']]).nunique()} (é¢„æœŸå·²é™è‡³åˆç†èŒƒå›´)")
        print(f"æ€»è´¸æ˜“é¢ï¼š${final_dataset['trade_value_raw_usd'].sum()/1e12:.2f} ä¸‡äº¿ç¾å…ƒ (æ— é‡å¤è®¡ç®—)")
        
        print("\nå¹´åº¦è®°å½•æ•°åˆ†å¸ƒï¼ˆå‰5å¹´å’Œå5å¹´ï¼‰ï¼š")
        year_counts = final_dataset['year'].value_counts().sort_index()
        for year in [2001, 2002, 2003, 2004, 2005]:
            if year in year_counts.index:
                print(f"  {year}: {year_counts[year]:,}")
        print("  ...")
        for year in [2020, 2021, 2022, 2023, 2024]:
            if year in year_counts.index:
                print(f"  {year}: {year_counts[year]:,}")
        
        print("\nç¾å›½ç›¸å…³è´¸æ˜“è®°å½•æ•°ï¼š")
        usa_records = final_dataset[(final_dataset['reporter'] == 'USA') | (final_dataset['partner'] == 'USA')]
        print(f"  æ€»è®¡ï¼š{len(usa_records):,} æ¡è®°å½•")
        
    except Exception as e:
        print(f"\né”™è¯¯ï¼š{e}")
        raise

if __name__ == "__main__":
    main()