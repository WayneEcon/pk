#!/usr/bin/env python3
"""
DLIåˆ†æä¸»æµç¨‹æ¨¡å— (Main Pipeline Module)
======================================

æœ¬æ¨¡å—æ˜¯åŠ¨æ€é”å®šæŒ‡æ•°(DLI)æ„å»ºä¸æ”¿ç­–å†²å‡»æ•ˆåº”éªŒè¯çš„ä¸»è¦æ‰§è¡Œæ¥å£ï¼Œ
æ•´åˆæ•°æ®å‡†å¤‡ã€DLIè®¡ç®—ã€ç»Ÿè®¡éªŒè¯ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½æ¨¡å—ã€‚

æä¾›ä»¥ä¸‹æ‰§è¡Œæ¨¡å¼ï¼š
1. å®Œæ•´åˆ†ææµç¨‹ - ä»åŸå§‹æ•°æ®åˆ°æœ€ç»ˆéªŒè¯æŠ¥å‘Š
2. DLIè®¡ç®—æ¨¡å¼ - ä»…è¿›è¡ŒDLIæŒ‡æ ‡è®¡ç®—
3. ç»Ÿè®¡éªŒè¯æ¨¡å¼ - åŸºäºå·²æœ‰DLIæ•°æ®è¿›è¡ŒDIDåˆ†æ
4. å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼ - ç”Ÿæˆå…³é”®ç»“æœæ‘˜è¦

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import sys
import os
from pathlib import Path
import logging
import argparse
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np

# æ·»åŠ srcè·¯å¾„ä»¥æ”¯æŒç›¸å¯¹å¯¼å…¥
sys.path.append(str(Path(__file__).parent.parent))

# å¯¼å…¥å„åŠŸèƒ½æ¨¡å—
from data_preparation import prepare_dli_dataset, export_prepared_data
from dli_calculator import generate_dli_panel_data
from statistical_verification import run_full_bidirectional_did_analysis

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def print_banner():
    """æ‰“å°æ¨¡å—banner"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    DLI åŠ¨æ€é”å®šæŒ‡æ•°åˆ†æç³»ç»Ÿ                     â•‘
    â•‘                Dynamic Locking Index Analysis                â•‘
    â•‘                                                              â•‘
    â•‘        ä»"å…³ç³»ç²˜æ€§"ç»´åº¦æ­ç¤ºç¾å›½èƒ½æºç‹¬ç«‹æ”¿ç­–çš„å›½é™…å½±å“           â•‘
    â•‘                                                              â•‘
    â•‘                   Version: 1.0.0                            â•‘
    â•‘                   Team: Energy Network Analysis             â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def _load_full_trade_data(base_dir: Path) -> Optional[pd.DataFrame]:
    """ä»01æ¨¡å—åŠ è½½å®Œæ•´çš„ã€å·²å¤„ç†çš„è´¸æ˜“æµæ•°æ®"""
    logger.info("... æ­£åœ¨åŠ è½½å®Œæ•´çš„è´¸æ˜“æµæ•°æ® ...")
    processed_data_dir = base_dir / "data" / "processed_data"
    trade_data_files = list(processed_data_dir.glob("cleaned_energy_trade_*.csv"))
    
    if not trade_data_files:
        logger.warning(f"âš ï¸ åœ¨ {processed_data_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½• 'cleaned_energy_trade_*.csv' æ–‡ä»¶ã€‚")
        return None
        
    try:
        trade_data_list = [pd.read_csv(file) for file in sorted(trade_data_files)]
        trade_data = pd.concat(trade_data_list, ignore_index=True)
        logger.info(f"âœ… æˆåŠŸåŠ è½½å¹¶åˆå¹¶ {len(trade_data_files)} ä¸ªè´¸æ˜“æ•°æ®æ–‡ä»¶ï¼Œå…± {len(trade_data)} è¡Œã€‚")
        return trade_data
    except Exception as e:
        logger.error(f"âŒ åŠ è½½å®Œæ•´è´¸æ˜“æ•°æ®å¤±è´¥: {e}")
        return None

def construct_node_dli_us(dli_data: pd.DataFrame, trade_data: pd.DataFrame, output_dir: Path) -> str:
    """
    æ„å»º Node-DLI_US (ç¾å›½é”šå®šåŠ¨æ€é”å®šæŒ‡æ•°)
    
    Args:
        dli_data (pd.DataFrame): æ¥è‡ªæœ¬æ¨¡å—çš„è¾¹çº§åˆ«DLIé¢æ¿æ•°æ®.
        trade_data (pd.DataFrame): æ¥è‡ª01æ¨¡å—çš„å®Œæ•´è´¸æ˜“æµæ•°æ®.
        output_dir (Path): è¾“å‡ºç›®å½•.

    Returns:
        str: ç”Ÿæˆçš„ node_dli_us.csv æ–‡ä»¶è·¯å¾„.
    """
    logger.info("   æ„å»º Node-DLI_US...")
    
    try:
        # ç­›é€‰ä¸ç¾å›½ç›¸å…³çš„è´¸æ˜“
        us_trade = trade_data[(trade_data['reporter'] == 'USA') | (trade_data['partner'] == 'USA')].copy()
        if len(us_trade) == 0:
            raise ValueError("æœªæ‰¾åˆ°ä¸ç¾å›½ç›¸å…³çš„è´¸æ˜“æ•°æ®")

        # è®¡ç®—å„å›½æ€»è¿›å£é¢
        total_imports = trade_data[trade_data['flow'] == 'M'].groupby(['year', 'reporter']).agg(
            total_imports=('trade_value_raw_usd', 'sum')
        ).reset_index()

        # è®¡ç®—å„å›½ä»ç¾å›½çš„è¿›å£é¢
        us_imports = us_trade[
            (us_trade['partner'] == 'USA') & (us_trade['flow'] == 'M')
        ].groupby(['year', 'reporter']).agg(
            us_imports=('trade_value_raw_usd', 'sum')
        ).reset_index()

        # åˆå¹¶è®¡ç®—çœŸå®è¿›å£ä»½é¢
        trade_shares = total_imports.merge(us_imports, on=['year', 'reporter'], how='left')
        trade_shares['us_imports'] = trade_shares['us_imports'].fillna(0)
        trade_shares['import_share_from_us'] = (trade_shares['us_imports'] / trade_shares['total_imports']).fillna(0).clip(0, 1)
        trade_shares.rename(columns={'reporter': 'country'}, inplace=True)
        
        logger.info(f"   è®¡ç®—äº† {len(trade_shares)} ä¸ªå›½å®¶-å¹´ä»½çš„çœŸå®è´¸æ˜“ä»½é¢")

        # åŸºäºçœŸå®DLIæ•°æ®æ„å»ºNode-DLI_US
        node_dli_records = []
        for _, trade_row in trade_shares.iterrows():
            year, country, s_imp = trade_row['year'], trade_row['country'], trade_row['import_share_from_us']
            
            dli_us_to_i = dli_data[(dli_data['year'] == year) & (dli_data['us_partner'] == country) & (dli_data['us_role'] == 'exporter')]['dli_score_adjusted'].mean()
            dli_i_to_us = dli_data[(dli_data['year'] == year) & (dli_data['us_partner'] == country) & (dli_data['us_role'] == 'importer')]['dli_score_adjusted'].mean()
            
            dli_us_to_i = dli_us_to_i if pd.notna(dli_us_to_i) else 0
            dli_i_to_us = dli_i_to_us if pd.notna(dli_i_to_us) else 0
            
            node_dli_us = s_imp * dli_us_to_i + (1 - s_imp) * dli_i_to_us
            
            node_dli_records.append({
                'year': year,
                'country': country,
                'node_dli_us': node_dli_us,
                'import_share_from_us': s_imp
            })
        
        node_dli_df = pd.DataFrame(node_dli_records)
        
        non_zero_dli = node_dli_df[node_dli_df['node_dli_us'] > 0]
        logger.info(f"   æœ‰æ•ˆNode-DLIè®°å½•: {len(non_zero_dli)}/{len(node_dli_df)}")
        
        output_path = output_dir / "node_dli_us.csv"
        node_dli_df.to_csv(output_path, index=False)
        
        logger.info(f"âœ… Node-DLI_USæ„å»ºå®Œæˆ: {len(node_dli_df)} è¡Œè®°å½•ï¼Œä¿å­˜è‡³ {output_path}")
        return str(output_path)
        
    except Exception as e:
        logger.error(f"âŒ Node-DLI_USæ„å»ºå¤±è´¥: {e}", exc_info=True)
        raise

def run_full_dli_analysis(data_dir: str = None,
                          output_dir: str = None,
                          skip_data_prep: bool = False,
                          skip_dli_calculation: bool = False,
                          skip_node_dli: bool = False,
                          skip_verification: bool = False) -> Dict[str, str]:
    """
    æ‰§è¡Œå®Œæ•´çš„DLIåˆ†ææµç¨‹
    
    è¿™æ˜¯æœ¬æ¨¡å—çš„æ ¸å¿ƒå‡½æ•°ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œå®Œæ•´åˆ†æï¼š
    1. æ•°æ®å‡†å¤‡ï¼šåŠ è½½å’Œé¢„å¤„ç†ç¾å›½èƒ½æºè´¸æ˜“æ•°æ®
    2. DLIè®¡ç®—ï¼šè®¡ç®—å››ç»´åº¦æŒ‡æ ‡å¹¶åˆæˆç»¼åˆæŒ‡æ ‡
    3. ç»Ÿè®¡éªŒè¯ï¼šä½¿ç”¨DIDæ–¹æ³•éªŒè¯æ”¿ç­–æ•ˆåº”
    
    Args:
        data_dir: åŸå§‹æ•°æ®ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é¡¹ç›®æ ‡å‡†è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰04_dli_analysisæ–‡ä»¶å¤¹
        skip_data_prep: è·³è¿‡æ•°æ®å‡†å¤‡æ­¥éª¤ï¼ˆä½¿ç”¨å·²æœ‰preparedæ•°æ®ï¼‰
        skip_dli_calculation: è·³è¿‡DLIè®¡ç®—æ­¥éª¤ï¼ˆä½¿ç”¨å·²æœ‰DLIé¢æ¿æ•°æ®ï¼‰
        skip_verification: è·³è¿‡ç»Ÿè®¡éªŒè¯æ­¥éª¤
        
    Returns:
        åŒ…å«æ‰€æœ‰è¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        
    Raises:
        Exception: å½“ä»»ä½•æ­¥éª¤å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    
    logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„DLIåˆ†ææµç¨‹...")
    logger.info("="*70)
    
    start_time = datetime.now()
    output_files = {}
    
    try:
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            base_dir = Path(__file__).parent.parent.parent
            output_dir = Path(__file__).parent / "outputs"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # ç¬¬1æ­¥ï¼šæ•°æ®å‡†å¤‡
        if not skip_data_prep:
            logger.info("\nğŸ”„ ç¬¬1æ­¥ï¼šæ•°æ®å‡†å¤‡é˜¶æ®µ...")
            logger.info("-" * 50)
            
            # å‡†å¤‡DLIåˆ†ææ•°æ®é›†
            prepared_data = prepare_dli_dataset(data_dir)
            
            # å¯¼å‡ºå‡†å¤‡å¥½çš„æ•°æ®
            prepared_data_path = export_prepared_data(
                prepared_data, 
                str(output_dir / "us_trade_prepared_for_dli.csv")
            )
            output_files['prepared_data'] = prepared_data_path
            
            logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(prepared_data):,} æ¡è®°å½•")
            logger.info(f"ğŸ“„ æ–‡ä»¶ä¿å­˜: {prepared_data_path}")
            
        else:
            logger.info("\nâ­ï¸ è·³è¿‡æ•°æ®å‡†å¤‡æ­¥éª¤ï¼ˆä½¿ç”¨å·²æœ‰æ•°æ®ï¼‰")
            prepared_data_path = str(output_dir / "us_trade_prepared_for_dli.csv")
            if not Path(prepared_data_path).exists():
                raise FileNotFoundError(f"è·³è¿‡æ•°æ®å‡†å¤‡ä½†æœªæ‰¾åˆ°å·²æœ‰æ•°æ®æ–‡ä»¶: {prepared_data_path}")
        
        # ç¬¬2æ­¥ï¼šDLIæŒ‡æ ‡è®¡ç®—
        if not skip_dli_calculation:
            logger.info("\nğŸ§® ç¬¬2æ­¥ï¼šDLIæŒ‡æ ‡è®¡ç®—é˜¶æ®µ...")
            logger.info("-" * 50)
            
            # ç”ŸæˆDLIé¢æ¿æ•°æ®
            dli_panel = generate_dli_panel_data(
                data_file_path=prepared_data_path,
                output_path=str(output_dir / "dli_panel_data.csv")
            )
            
            dli_panel_path = str(output_dir / "dli_panel_data.csv")
            weights_path = str(output_dir / "dli_weights_and_params.json")
            
            output_files['dli_panel_data'] = dli_panel_path
            output_files['dli_weights'] = weights_path
            
            logger.info(f"âœ… DLIè®¡ç®—å®Œæˆ: {len(dli_panel):,} æ¡è®°å½•")
            logger.info(f"ğŸ“„ é¢æ¿æ•°æ®: {dli_panel_path}")
            logger.info(f"ğŸ“„ æƒé‡å‚æ•°: {weights_path}")
            
            # æ˜¾ç¤ºDLIç»Ÿè®¡æ‘˜è¦
            logger.info("\nğŸ“Š DLIç»¼åˆæŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦:")
            dli_stats = dli_panel['dli_score_adjusted'].describe()
            logger.info(f"  å‡å€¼: {dli_stats['mean']:.4f}")
            logger.info(f"  æ ‡å‡†å·®: {dli_stats['std']:.4f}")
            logger.info(f"  èŒƒå›´: [{dli_stats['min']:.4f}, {dli_stats['max']:.4f}]")
            logger.info(f"  ä¸­ä½æ•°: {dli_stats['50%']:.4f}")
            
        else:
            logger.info("\nâ­ï¸ è·³è¿‡DLIè®¡ç®—æ­¥éª¤ï¼ˆä½¿ç”¨å·²æœ‰DLIæ•°æ®ï¼‰")
            dli_panel_path = str(output_dir / "dli_panel_data.csv")
            if not Path(dli_panel_path).exists():
                raise FileNotFoundError(f"è·³è¿‡DLIè®¡ç®—ä½†æœªæ‰¾åˆ°å·²æœ‰é¢æ¿æ•°æ®: {dli_panel_path}")
            dli_panel = pd.read_csv(dli_panel_path)

        # ç¬¬2.5æ­¥: Node-DLI_US æŒ‡æ ‡æ„å»º
        if not skip_node_dli:
            logger.info("\nğŸ—ï¸  ç¬¬2.5æ­¥: Node-DLI_US æŒ‡æ ‡æ„å»ºé˜¶æ®µ...")
            logger.info("-" * 50)
            
            full_trade_data = _load_full_trade_data(base_dir)
            if full_trade_data is not None:
                node_dli_us_path = construct_node_dli_us(dli_panel, full_trade_data, output_dir)
                output_files['node_dli_us'] = node_dli_us_path
            else:
                logger.warning("âš ï¸ å› æ— æ³•åŠ è½½å®Œæ•´è´¸æ˜“æ•°æ®ï¼Œè·³è¿‡Node-DLI_USæ„å»ºã€‚")
        else:
            logger.info("\nâ­ï¸ è·³è¿‡Node-DLI_USæ„å»ºæ­¥éª¤")

        # ç¬¬3æ­¥ï¼šç»Ÿè®¡éªŒè¯
        if not skip_verification:
            logger.info("\nğŸ“Š ç¬¬3æ­¥ï¼šç»Ÿè®¡éªŒè¯é˜¶æ®µ...")
            logger.info("-" * 50)
            
            # æ‰§è¡Œå®Œæ•´çš„DIDéªŒè¯åˆ†æ
            verification_files = run_full_bidirectional_did_analysis(
                dli_data_path=dli_panel_path,
                output_dir=str(output_dir)
            )
            
            # åˆå¹¶éªŒè¯ç»“æœæ–‡ä»¶
            output_files.update(verification_files)
            
            logger.info("âœ… ç»Ÿè®¡éªŒè¯å®Œæˆ")
            logger.info(f"ğŸ“„ éªŒè¯æŠ¥å‘Š: {verification_files['verification_report_md']}")
            logger.info(f"ğŸ“„ ç»“æœæ•°æ®: {verification_files['verification_results_csv']}")
            
        else:
            logger.info("\nâ­ï¸ è·³è¿‡ç»Ÿè®¡éªŒè¯æ­¥éª¤")
        
        # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        # è¾“å‡ºæœ€ç»ˆæ‘˜è¦
        logger.info("\n" + "="*70)
        logger.info("ğŸ‰ DLIå®Œæ•´åˆ†ææµç¨‹æˆåŠŸå®Œæˆ!")
        logger.info(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.1f} ç§’")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"ğŸ“Š ç”Ÿæˆæ–‡ä»¶æ•°: {len(output_files)}")
        
        logger.info("\nğŸ“‹ è¾“å‡ºæ–‡ä»¶æ¸…å•:")
        for file_type, file_path in output_files.items():
            logger.info(f"  {file_type}: {Path(file_path).name}")
        
        return output_files
        
    except Exception as e:
        logger.error(f"âŒ DLIåˆ†ææµç¨‹å¤±è´¥: {e}")
        logger.error(f"æ‰§è¡Œæ—¶é—´: {(datetime.now() - start_time).total_seconds():.1f} ç§’")
        raise

def run_dli_calculation_only(data_file: str = None, 
                            output_dir: str = None) -> str:
    """
    ä»…æ‰§è¡ŒDLIè®¡ç®—æ¨¡å¼
    
    é€‚ç”¨äºå·²æœ‰å‡†å¤‡å¥½çš„æ•°æ®ï¼Œä»…éœ€è®¡ç®—DLIæŒ‡æ ‡çš„åœºæ™¯
    
    Args:
        data_file: å‡†å¤‡å¥½çš„æ•°æ®æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        DLIé¢æ¿æ•°æ®æ–‡ä»¶è·¯å¾„
    """
    
    logger.info("ğŸ§® æ‰§è¡ŒDLIè®¡ç®—æ¨¡å¼...")
    
    try:
        if output_dir is None:
            base_dir = Path(__file__).parent.parent.parent
            output_dir = Path(__file__).parent / "outputs"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ‰§è¡ŒDLIè®¡ç®—
        dli_panel = generate_dli_panel_data(
            data_file_path=data_file,
            output_path=str(output_dir / "dli_panel_data.csv")
        )
        
        output_path = str(output_dir / "dli_panel_data.csv")
        
        logger.info(f"âœ… DLIè®¡ç®—å®Œæˆ: {output_path}")
        return output_path
        
    except Exception as e:
        logger.error(f"âŒ DLIè®¡ç®—å¤±è´¥: {e}")
        raise

def run_verification_only(dli_data_file: str = None,
                         output_dir: str = None) -> Dict[str, str]:
    """
    ä»…æ‰§è¡Œç»Ÿè®¡éªŒè¯æ¨¡å¼
    
    é€‚ç”¨äºå·²æœ‰DLIé¢æ¿æ•°æ®ï¼Œä»…éœ€è¿›è¡ŒDIDåˆ†æçš„åœºæ™¯
    
    Args:
        dli_data_file: DLIé¢æ¿æ•°æ®æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        éªŒè¯æŠ¥å‘Šæ–‡ä»¶è·¯å¾„å­—å…¸
    """
    
    logger.info("ğŸ“Š æ‰§è¡Œç»Ÿè®¡éªŒè¯æ¨¡å¼...")
    
    try:
        if output_dir is None:
            base_dir = Path(__file__).parent.parent.parent
            output_dir = Path(__file__).parent / "outputs"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ‰§è¡Œç»Ÿè®¡éªŒè¯
        verification_files = run_full_verification_analysis(
            dli_data_path=dli_data_file,
            output_dir=str(output_dir)
        )
        
        logger.info("âœ… ç»Ÿè®¡éªŒè¯å®Œæˆ")
        return verification_files
        
    except Exception as e:
        logger.error(f"âŒ ç»Ÿè®¡éªŒè¯å¤±è´¥: {e}")
        raise

def run_quick_demo() -> Dict[str, str]:
    """
    å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼
    
    æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹å¹¶ç”Ÿæˆå…³é”®ç»“æœæ‘˜è¦ï¼Œé€‚ç”¨äºå¿«é€ŸæŸ¥çœ‹åˆ†æèƒ½åŠ›
    
    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
    """
    
    logger.info("âš¡ æ‰§è¡Œå¿«é€Ÿæ¼”ç¤ºæ¨¡å¼...")
    
    try:
        # æ‰§è¡Œå®Œæ•´åˆ†æ
        output_files = run_full_dli_analysis()
        
        # ç”Ÿæˆæ¼”ç¤ºæ‘˜è¦
        logger.info("\n" + "ğŸŒŸ" * 30)
        logger.info("ğŸ“ˆ DLIåˆ†ææ¼”ç¤ºæ‘˜è¦")
        logger.info("ğŸŒŸ" * 30)
        
        # è¯»å–å…³é”®ç»“æœ
        verification_csv = Path(output_files['verification_results_csv'])
        if verification_csv.exists():
            import pandas as pd
            results_df = pd.read_csv(verification_csv)
            
            logger.info("\nğŸ”¬ æ ¸å¿ƒç§‘å­¦å‘ç°:")
            significant_vars = results_df[results_df['significant_5pct'] == True]
            if len(significant_vars) > 0:
                logger.info("ğŸ“Š ç»Ÿè®¡æ˜¾è‘—çš„æ”¿ç­–æ•ˆåº” (5%æ°´å¹³):")
                for _, row in significant_vars.iterrows():
                    var_name = row['variable']
                    coef = row['did_coefficient']
                    p_val = row['did_p_value']
                    logger.info(f"  â€¢ {var_name}: Î² = {coef:.4f} (p < 0.001)")
                    if coef > 0:
                        logger.info(f"    â†’ æ”¿ç­–æ˜¾è‘—å¢å¼ºäº†{var_name}é”å®šæ•ˆåº”")
                    else:
                        logger.info(f"    â†’ æ”¿ç­–æ˜¾è‘—å‡å¼±äº†{var_name}é”å®šæ•ˆåº”")
            else:
                logger.info("  æœªå‘ç°5%æ°´å¹³ç»Ÿè®¡æ˜¾è‘—çš„æ”¿ç­–æ•ˆåº”")
        
        logger.info(f"\nğŸ“ å®Œæ•´ç»“æœè¯·æŸ¥çœ‹: {Path(output_files['verification_report_md']).name}")
        
        return output_files
        
    except Exception as e:
        logger.error(f"âŒ å¿«é€Ÿæ¼”ç¤ºå¤±è´¥: {e}")
        raise

def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    
    parser = argparse.ArgumentParser(
        description="DLIåŠ¨æ€é”å®šæŒ‡æ•°åˆ†æç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py                    # å®Œæ•´åˆ†ææµç¨‹
  python main.py --mode dli         # ä»…DLIè®¡ç®—
  python main.py --mode verify      # ä»…ç»Ÿè®¡éªŒè¯  
  python main.py --mode demo        # å¿«é€Ÿæ¼”ç¤º
  
è¾“å‡ºæ–‡ä»¶:
  - dli_panel_data.csv             # DLIé¢æ¿æ•°æ®
  - dli_verification_report.md     # ç»Ÿè®¡éªŒè¯æŠ¥å‘Š
  - dli_weights_and_params.json    # PCAæƒé‡å‚æ•°
        """
    )
    
    parser.add_argument(
        '--mode', 
        choices=['full', 'dli', 'verify', 'demo'],
        default='full',
        help='æ‰§è¡Œæ¨¡å¼ (default: full)'
    )
    
    parser.add_argument(
        '--data-dir',
        type=str,
        help='åŸå§‹æ•°æ®ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        help='è¾“å‡ºç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--dli-file',
        type=str,
        help='DLIé¢æ¿æ•°æ®æ–‡ä»¶è·¯å¾„ (verifyæ¨¡å¼)'
    )
    
    parser.add_argument(
        '--data-file',
        type=str,
        help='å‡†å¤‡å¥½çš„æ•°æ®æ–‡ä»¶è·¯å¾„ (dliæ¨¡å¼)'
    )
    
    parser.add_argument(
        '--skip-prep',
        action='store_true',
        help='è·³è¿‡æ•°æ®å‡†å¤‡æ­¥éª¤'
    )
    
    parser.add_argument(
        '--skip-dli',
        action='store_true',
        help='è·³è¿‡DLIè®¡ç®—æ­¥éª¤'
    )

    parser.add_argument(
        '--skip-node-dli',
        action='store_true',
        help='è·³è¿‡Node-DLI_USæ„å»ºæ­¥éª¤'
    )
    
    parser.add_argument(
        '--skip-verify',
        action='store_true',
        help='è·³è¿‡ç»Ÿè®¡éªŒè¯æ­¥éª¤'
    )
    
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='é™é»˜æ¨¡å¼ï¼Œå‡å°‘æ—¥å¿—è¾“å‡º'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.quiet:
        logging.getLogger().setLevel(logging.WARNING)
    
    # æ˜¾ç¤ºbanner
    if not args.quiet:
        print_banner()
    
    try:
        # æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”åŠŸèƒ½
        if args.mode == 'full':
            output_files = run_full_dli_analysis(
                data_dir=args.data_dir,
                output_dir=args.output_dir,
                skip_data_prep=args.skip_prep,
                skip_dli_calculation=args.skip_dli,
                skip_node_dli=args.skip_node_dli,
                skip_verification=args.skip_verify
            )
            
        elif args.mode == 'dli':
            output_file = run_dli_calculation_only(
                data_file=args.data_file,
                output_dir=args.output_dir
            )
            output_files = {'dli_panel_data': output_file}
            
        elif args.mode == 'verify':
            output_files = run_verification_only(
                dli_data_file=args.dli_file,
                output_dir=args.output_dir
            )
            
        elif args.mode == 'demo':
            output_files = run_quick_demo()
        
        # æœ€ç»ˆæˆåŠŸæç¤º
        if not args.quiet:
            print(f"\n{'='*50}")
            print("ğŸ‰ DLIåˆ†ææˆåŠŸå®Œæˆ!")
            print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶æ•°: {len(output_files)}")
            print(f"ğŸ“ æŸ¥çœ‹ç»“æœ: {Path(list(output_files.values())[0]).parent}")
            print(f"{'='*50}")
        
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 1
        
    except Exception as e:
        logger.error(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        if not args.quiet:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())