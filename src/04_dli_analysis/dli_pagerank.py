#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆåŠ¨æ€é”å®šæŒ‡æ•°è®¡ç®—æ¨¡å— (Enhanced DLI with Personalized PageRank)
==================================================================

æœ¬æ¨¡å—å®ç°é›†æˆä¸ªæ€§åŒ–PageRankçš„äº”ç»´åº¦åŠ¨æ€é”å®šæŒ‡æ•°è®¡ç®—ï¼š

åŸæœ‰å››ä¸ªç»´åº¦ï¼š
1. è´¸æ˜“æŒç»­æ€§ (Continuity): è¡¡é‡å…³ç³»çš„é•¿æœŸæ€§
2. åŸºç¡€è®¾æ–½å¼ºåº¦ (Infrastructure): è¡¡é‡ä¸“ç”¨æ€§èµ„äº§å¯¼è‡´çš„é”å®š
3. è´¸æ˜“ç¨³å®šæ€§ (Stability): è¡¡é‡å…³ç³»çš„å¯é æ€§  
4. å¸‚åœºé”å®šåŠ› (Market Locking Power): è¡¡é‡å¸‚åœºç»“æ„å¯¼è‡´çš„é”å®šæ•ˆåº”

æ–°å¢ç½‘ç»œç»´åº¦ï¼š
5. ä¸ªæ€§åŒ–PageRankå½±å“åŠ›: è¡¡é‡æ–¹å‘æ€§ç½‘ç»œé”å®šèƒ½åŠ›

æ ¸å¿ƒåˆ›æ–°ï¼š
- ä¸¥æ ¼åŒºåˆ†æ–¹å‘æ€§ï¼šç¾å›½å‡ºå£é”å®šä»–å›½ vs ç¾å›½è¿›å£è¢«ä»–å›½é”å®š
- ä½¿ç”¨PCAå¤„ç†äº”ç»´åº¦å¤šé‡å…±çº¿æ€§é—®é¢˜
- å­¦æœ¯è§„èŒƒçš„è¯Šæ–­åˆ†æå’Œç›¸å…³æ€§æ£€éªŒ
- ç»Ÿä¸€æ ‡å°ºçš„ç»¼åˆæŒ‡æ•°è®¡ç®—

ç‰ˆæœ¬ï¼šv1.0 - Enhanced DLI with Network Centrality
ä½œè€…ï¼šEnergy Network Analysis Team
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
import argparse
import sys
import json
import warnings
warnings.filterwarnings('ignore')

from typing import Dict, List, Tuple, Optional, Union
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from datetime import datetime

# å¯è§†åŒ–åº“
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class EnhancedDLICalculator:
    """å¢å¼ºç‰ˆDLIè®¡ç®—å™¨ï¼Œé›†æˆä¸ªæ€§åŒ–PageRankç½‘ç»œç»´åº¦"""
    
    def __init__(self, dli_data_path: Path, pagerank_data_path: Path, output_dir: Path):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆDLIè®¡ç®—å™¨
        
        Args:
            dli_data_path: åŸæœ‰DLIå››ç»´åº¦æ•°æ®è·¯å¾„
            pagerank_data_path: ä¸ªæ€§åŒ–PageRankæ•°æ®è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.dli_data_path = Path(dli_data_path)
        self.pagerank_data_path = Path(pagerank_data_path)
        self.output_dir = Path(output_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºfiguresç›®å½•ï¼ˆåœ¨04_dli_analysisæ ¹ç›®å½•ä¸‹ï¼‰
        self.figures_dir = Path(__file__).parent / "figures"
        self.figures_dir.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ“ DLIæ•°æ®è·¯å¾„: {self.dli_data_path}")
        logger.info(f"ğŸ“ PageRankæ•°æ®è·¯å¾„: {self.pagerank_data_path}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # äº”ä¸ªç»´åº¦åˆ—åï¼ˆç”¨äºPCAåˆ†æï¼‰
        self.original_dimensions = ['continuity', 'infrastructure', 'stability', 'market_locking_power']
        self.pagerank_export_col = 'ppr_us_export_influence' 
        self.pagerank_import_col = 'ppr_influence_on_us'
        
        # å­˜å‚¨æƒé‡å’Œåˆ†æç»“æœ
        self.pca_weights = {}
        self.correlation_matrices = {}
        
    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        åŠ è½½åŸæœ‰DLIæ•°æ®å’Œä¸ªæ€§åŒ–PageRankæ•°æ®
        
        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: DLIæ•°æ®å’ŒPageRankæ•°æ®
        """
        logger.info("ğŸ“‚ å¼€å§‹åŠ è½½æ•°æ®...")
        
        # åŠ è½½åŸæœ‰DLIå››ç»´åº¦æ•°æ®
        if not self.dli_data_path.exists():
            raise FileNotFoundError(f"DLIæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.dli_data_path}")
        
        dli_data = pd.read_csv(self.dli_data_path)
        logger.info(f"âœ… DLIæ•°æ®åŠ è½½å®Œæˆ: {len(dli_data):,}æ¡è®°å½•")
        
        # åŠ è½½ä¸ªæ€§åŒ–PageRankæ•°æ®  
        if not self.pagerank_data_path.exists():
            raise FileNotFoundError(f"PageRankæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.pagerank_data_path}")
        
        pagerank_data = pd.read_csv(self.pagerank_data_path)
        logger.info(f"âœ… PageRankæ•°æ®åŠ è½½å®Œæˆ: {len(pagerank_data):,}æ¡è®°å½•")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        logger.info("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:")
        logger.info(f"  DLIæ•°æ®å¹´ä»½èŒƒå›´: {dli_data['year'].min()}-{dli_data['year'].max()}")
        logger.info(f"  PageRankæ•°æ®å¹´ä»½èŒƒå›´: {pagerank_data['year'].min()}-{pagerank_data['year'].max()}")
        logger.info(f"  DLIæ•°æ®åˆ—: {list(dli_data.columns)}")
        logger.info(f"  PageRankæ•°æ®åˆ—: {list(pagerank_data.columns)}")
        
        return dli_data, pagerank_data
        
    def integrate_pagerank_data(self, dli_data: pd.DataFrame, pagerank_data: pd.DataFrame) -> pd.DataFrame:
        """
        æ•´åˆä¸ªæ€§åŒ–PageRankæ•°æ®åˆ°DLIæ•°æ®ä¸­
        
        Args:
            dli_data: åŸæœ‰DLIå››ç»´åº¦æ•°æ®
            pagerank_data: ä¸ªæ€§åŒ–PageRankæ•°æ®
            
        Returns:
            pd.DataFrame: æ•´åˆåçš„äº”ç»´åº¦æ•°æ®
        """
        logger.info("ğŸ”— å¼€å§‹æ•´åˆPageRankæ•°æ®åˆ°DLIæ•°æ®...")
        
        # å‡†å¤‡åˆå¹¶çš„é”®
        # DLIæ•°æ®ä½¿ç”¨us_partnerå­—æ®µï¼ŒPageRankæ•°æ®ä½¿ç”¨country_nameå­—æ®µ
        pagerank_for_merge = pagerank_data[['year', 'country_name', 
                                          self.pagerank_export_col, 
                                          self.pagerank_import_col]].copy()
        
        pagerank_for_merge = pagerank_for_merge.rename(columns={
            'country_name': 'us_partner'
        })
        
        # æ‰§è¡Œå·¦è¿æ¥åˆå¹¶
        enhanced_data = dli_data.merge(
            pagerank_for_merge,
            on=['year', 'us_partner'],
            how='left'
        )
        
        logger.info(f"ğŸ”— æ•°æ®åˆå¹¶å®Œæˆ: {len(enhanced_data):,}æ¡è®°å½•")
        
        # æ£€æŸ¥åˆå¹¶æ•ˆæœ
        pagerank_missing = enhanced_data[self.pagerank_export_col].isna().sum()
        total_records = len(enhanced_data)
        missing_rate = pagerank_missing / total_records * 100
        
        logger.info(f"ğŸ“Š PageRankæ•°æ®è¦†ç›–ç‡: {(100-missing_rate):.1f}% ({total_records-pagerank_missing:,}/{total_records:,})")
        
        if missing_rate > 10:
            logger.warning(f"âš ï¸  PageRankæ•°æ®ç¼ºå¤±ç‡è¾ƒé«˜: {missing_rate:.1f}%")
        
        return enhanced_data
        
    def create_directional_datasets(self, enhanced_data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        åˆ›å»ºæ–¹å‘æ€§æ•°æ®é›†ï¼šåŒºåˆ†ç¾å›½å‡ºå£é”å®šå’Œè¿›å£è¢«é”å®š
        
        Args:
            enhanced_data: æ•´åˆåçš„äº”ç»´åº¦æ•°æ®
            
        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: å‡ºå£é”å®šæ•°æ®, è¿›å£è¢«é”å®šæ•°æ®
        """
        logger.info("â†”ï¸  åˆ›å»ºæ–¹å‘æ€§æ•°æ®é›†...")
        
        # 1. ç¾å›½å‡ºå£é”å®šä»–å›½æ•°æ® (ç¾å›½ä¸ºexporter)
        export_locking_data = enhanced_data[
            enhanced_data['us_role'] == 'exporter'
        ].copy()
        
        # ä¸ºå‡ºå£é”å®šæ•°æ®é€‰æ‹©ç›¸åº”çš„PageRankç»´åº¦
        export_locking_data['pagerank_dimension'] = export_locking_data[self.pagerank_export_col]
        
        # 2. ç¾å›½è¿›å£è¢«ä»–å›½é”å®šæ•°æ® (ç¾å›½ä¸ºimporter)
        import_locking_data = enhanced_data[
            enhanced_data['us_role'] == 'importer'  
        ].copy()
        
        # ä¸ºè¿›å£é”å®šæ•°æ®é€‰æ‹©ç›¸åº”çš„PageRankç»´åº¦
        import_locking_data['pagerank_dimension'] = import_locking_data[self.pagerank_import_col]
        
        logger.info(f"ğŸ“¤ ç¾å›½å‡ºå£é”å®šæ•°æ®: {len(export_locking_data):,}æ¡è®°å½•")
        logger.info(f"ğŸ“¥ ç¾å›½è¿›å£è¢«é”å®šæ•°æ®: {len(import_locking_data):,}æ¡è®°å½•")
        
        return export_locking_data, import_locking_data
    
    def diagnose_correlations(self, data: pd.DataFrame, direction: str) -> pd.DataFrame:
        """
        è¯Šæ–­äº”ä¸ªç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§
        
        Args:
            data: åŒ…å«äº”ä¸ªç»´åº¦çš„æ•°æ®
            direction: æ–¹å‘æ ‡è¯† ('export' æˆ– 'import')
            
        Returns:
            pd.DataFrame: ç›¸å…³ç³»æ•°çŸ©é˜µ
        """
        logger.info(f"ğŸ” å¼€å§‹{direction}é”å®šç»´åº¦ç›¸å…³æ€§è¯Šæ–­...")
        
        # å‡†å¤‡äº”ä¸ªç»´åº¦æ•°æ®
        five_dimensions = self.original_dimensions + ['pagerank_dimension']
        
        # ç­›é€‰æœ‰æ•ˆæ•°æ®ï¼ˆå»é™¤ç¼ºå¤±å€¼ï¼‰
        valid_data = data[five_dimensions].dropna()
        
        if len(valid_data) == 0:
            raise ValueError(f"{direction}é”å®šæ•°æ®ä¸­æ²¡æœ‰å®Œæ•´çš„äº”ç»´åº¦è§‚æµ‹å€¼")
        
        logger.info(f"  æœ‰æ•ˆè§‚æµ‹æ•°: {len(valid_data):,}")
        
        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        correlation_matrix = valid_data.corr()
        
        # è¾“å‡ºç›¸å…³ç³»æ•°ç»Ÿè®¡
        logger.info(f"  {direction}é”å®šç»´åº¦ç›¸å…³æ€§ç»Ÿè®¡:")
        for i, dim1 in enumerate(five_dimensions):
            for j, dim2 in enumerate(five_dimensions):
                if i < j:  # åªè¾“å‡ºä¸Šä¸‰è§’
                    corr = correlation_matrix.loc[dim1, dim2]
                    logger.info(f"    {dim1} vs {dim2}: {corr:.3f}")
        
        return correlation_matrix
    
    def create_correlation_heatmap(self, export_corr: pd.DataFrame, import_corr: pd.DataFrame):
        """
        åˆ›å»ºç›¸å…³ç³»æ•°çŸ©é˜µçƒ­åŠ›å›¾
        
        Args:
            export_corr: å‡ºå£é”å®šç›¸å…³çŸ©é˜µ
            import_corr: è¿›å£é”å®šç›¸å…³çŸ©é˜µ
        """
        logger.info("ğŸ¨ åˆ›å»ºç›¸å…³ç³»æ•°çŸ©é˜µçƒ­åŠ›å›¾...")
        
        # è®¾ç½®å›¾å½¢å‚æ•°
        plt.style.use('default')
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # ç»´åº¦æ ‡ç­¾ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        dimension_labels = [
            'Continuity', 'Infrastructure', 'Stability', 
            'Market Locking', 'PageRank Network'
        ]
        
        # 1. å‡ºå£é”å®šç›¸å…³çŸ©é˜µçƒ­åŠ›å›¾
        ax1 = axes[0]
        sns.heatmap(export_corr, 
                   annot=True, 
                   cmap='RdBu_r',
                   center=0,
                   square=True,
                   fmt='.3f',
                   cbar_kws={'label': 'Correlation Coefficient'},
                   xticklabels=dimension_labels,
                   yticklabels=dimension_labels,
                   ax=ax1)
        ax1.set_title('US Export Locking Dimensions\nCorrelation Matrix', fontsize=12, pad=20)
        ax1.tick_params(axis='x', rotation=45)
        ax1.tick_params(axis='y', rotation=0)
        
        # 2. è¿›å£é”å®šç›¸å…³çŸ©é˜µçƒ­åŠ›å›¾
        ax2 = axes[1]
        sns.heatmap(import_corr,
                   annot=True,
                   cmap='RdBu_r', 
                   center=0,
                   square=True,
                   fmt='.3f',
                   cbar_kws={'label': 'Correlation Coefficient'},
                   xticklabels=dimension_labels,
                   yticklabels=dimension_labels,
                   ax=ax2)
        ax2.set_title('US Import Locking Dimensions\nCorrelation Matrix', fontsize=12, pad=20)
        ax2.tick_params(axis='x', rotation=45)
        ax2.tick_params(axis='y', rotation=0)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾å½¢åˆ°æ­£ç¡®çš„figuresç›®å½•
        heatmap_path = self.figures_dir / "dli_dimensions_correlation.png"
        plt.savefig(heatmap_path, dpi=300, bbox_inches='tight', facecolor='white')
        logger.info(f"ğŸ“Š ç›¸å…³çŸ©é˜µçƒ­åŠ›å›¾å·²ä¿å­˜: {heatmap_path}")
        
        plt.close()
        
        # å­˜å‚¨ç›¸å…³çŸ©é˜µä¾›åç»­ä½¿ç”¨
        self.correlation_matrices['export'] = export_corr
        self.correlation_matrices['import'] = import_corr
    
    def calculate_enhanced_dli_with_pca(self, data: pd.DataFrame, direction: str) -> Tuple[pd.DataFrame, Dict]:
        """
        ä½¿ç”¨PCAè®¡ç®—å¢å¼ºç‰ˆäº”ç»´åº¦DLI
        
        Args:
            data: åŒ…å«äº”ä¸ªç»´åº¦çš„æ•°æ®
            direction: æ–¹å‘æ ‡è¯† ('export' æˆ– 'import')
            
        Returns:
            Tuple[pd.DataFrame, Dict]: å¸¦æœ‰DLIå¾—åˆ†çš„æ•°æ®, PCAæƒé‡ä¿¡æ¯
        """
        logger.info(f"ğŸ¯ è®¡ç®—{direction}é”å®šå¢å¼ºç‰ˆDLI...")
        
        enhanced_data = data.copy()
        
        # å‡†å¤‡äº”ä¸ªç»´åº¦æ•°æ®
        five_dimensions = self.original_dimensions + ['pagerank_dimension']
        
        # ç­›é€‰æœ‰æ•ˆæ•°æ®
        valid_mask = enhanced_data[five_dimensions].notna().all(axis=1)
        valid_data = enhanced_data[valid_mask].copy()
        
        if len(valid_data) == 0:
            raise ValueError(f"{direction}é”å®šæ•°æ®ä¸­æ²¡æœ‰å®Œæ•´çš„äº”ç»´åº¦è§‚æµ‹å€¼")
        
        logger.info(f"  æœ‰æ•ˆè§‚æµ‹æ•°: {len(valid_data):,}")
        
        # æå–äº”ç»´åº¦çŸ©é˜µè¿›è¡ŒPCA
        dimensions_matrix = valid_data[five_dimensions].values
        
        # æ ‡å‡†åŒ–ï¼ˆPCAå‰çš„å¿…è¦æ­¥éª¤ï¼‰
        scaler = StandardScaler()
        dimensions_standardized = scaler.fit_transform(dimensions_matrix)
        
        # æ‰§è¡ŒPCAåˆ†æ
        pca = PCA(n_components=5)
        pca_scores = pca.fit_transform(dimensions_standardized)
        
        # æå–ç¬¬ä¸€ä¸»æˆåˆ†ä½œä¸ºç»¼åˆDLI
        first_pc_scores = pca_scores[:, 0]
        
        # è·å–æƒé‡ï¼ˆç¬¬ä¸€ä¸»æˆåˆ†çš„è½½è·ï¼‰
        first_pc_loadings = pca.components_[0]
        
        # åˆ›å»ºæƒé‡å­—å…¸
        pca_weights = {
            'dimensions': five_dimensions,
            'loadings': first_pc_loadings.tolist(),
            'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),
            'cumulative_explained_variance': np.cumsum(pca.explained_variance_ratio_).tolist(),
            'first_pc_variance_explained': float(pca.explained_variance_ratio_[0])
        }
        
        # å°†æƒé‡ä¸ç»´åº¦åé…å¯¹
        dimension_weights = {
            dim: float(weight) for dim, weight in zip(five_dimensions, first_pc_loadings)
        }
        pca_weights['dimension_weights'] = dimension_weights
        
        logger.info(f"  {direction}é”å®šPCAåˆ†æç»“æœ:")
        logger.info(f"    ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šæ–¹å·®æ¯”: {pca_weights['first_pc_variance_explained']:.3f}")
        logger.info(f"    äº”ä¸ªç»´åº¦æƒé‡:")
        for dim, weight in dimension_weights.items():
            logger.info(f"      {dim}: {weight:.3f}")
        
        # å°†PCAå¾—åˆ†æ·»åŠ åˆ°æ•°æ®ä¸­
        valid_data[f'dli_enhanced_{direction}'] = first_pc_scores
        
        # ä¸ºæ‰€æœ‰æ•°æ®åˆ†é…DLIå¾—åˆ†ï¼ˆåŒ…æ‹¬ç¼ºå¤±å€¼çš„è§‚æµ‹ï¼‰
        enhanced_data[f'dli_enhanced_{direction}'] = np.nan
        enhanced_data.loc[valid_mask, f'dli_enhanced_{direction}'] = first_pc_scores
        
        # å­˜å‚¨æƒé‡ä¿¡æ¯
        self.pca_weights[direction] = pca_weights
        
        logger.info(f"âœ… {direction}é”å®šå¢å¼ºç‰ˆDLIè®¡ç®—å®Œæˆ")
        
        return enhanced_data, pca_weights
    
    def combine_directional_results(self, export_data: pd.DataFrame, import_data: pd.DataFrame) -> pd.DataFrame:
        """
        åˆå¹¶åŒå‘DLIç»“æœ
        
        Args:
            export_data: å‡ºå£é”å®šDLIæ•°æ®
            import_data: è¿›å£é”å®šDLIæ•°æ®
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„å®Œæ•´å¢å¼ºç‰ˆDLIæ•°æ®
        """
        logger.info("ğŸ”„ åˆå¹¶åŒå‘DLIç»“æœ...")
        
        # åˆå¹¶æ•°æ®
        combined_data = pd.concat([export_data, import_data], ignore_index=True)
        
        # åˆ›å»ºç»Ÿä¸€çš„å¢å¼ºç‰ˆDLIåˆ—
        combined_data['dli_enhanced'] = combined_data['dli_enhanced_export'].fillna(
            combined_data['dli_enhanced_import']
        )
        
        # æ·»åŠ å…ƒä¿¡æ¯åˆ—
        combined_data['calculation_method'] = 'Enhanced_5D_PCA'
        combined_data['pagerank_integrated'] = True
        combined_data['analysis_timestamp'] = datetime.now().isoformat()
        
        logger.info(f"ğŸ¯ åŒå‘DLIåˆå¹¶å®Œæˆ: {len(combined_data):,}æ¡è®°å½•")
        
        # è¾“å‡ºåŸºæœ¬ç»Ÿè®¡
        export_count = len(combined_data[combined_data['us_role'] == 'exporter'])
        import_count = len(combined_data[combined_data['us_role'] == 'importer'])
        
        logger.info(f"  ç¾å›½å‡ºå£é”å®šè®°å½•: {export_count:,}")
        logger.info(f"  ç¾å›½è¿›å£è¢«é”å®šè®°å½•: {import_count:,}")
        
        return combined_data
    
    def save_results(self, enhanced_data: pd.DataFrame) -> Tuple[Path, Path]:
        """
        ä¿å­˜å¢å¼ºç‰ˆDLIç»“æœ
        
        Args:
            enhanced_data: å¢å¼ºç‰ˆDLIæ•°æ®
            
        Returns:
            Tuple[Path, Path]: CSVæ–‡ä»¶è·¯å¾„, JSONæƒé‡æ–‡ä»¶è·¯å¾„
        """
        logger.info("ğŸ’¾ ä¿å­˜å¢å¼ºç‰ˆDLIç»“æœ...")
        
        # 1. ä¿å­˜å¢å¼ºç‰ˆDLIé¢æ¿æ•°æ®
        csv_path = self.output_dir / "dli_pagerank.csv"
        enhanced_data.to_csv(csv_path, index=False)
        logger.info(f"ğŸ“Š å¢å¼ºç‰ˆDLIæ•°æ®å·²ä¿å­˜: {csv_path}")
        
        # 2. ä¿å­˜æƒé‡å’Œåˆ†æå‚æ•°
        weights_and_params = {
            'analysis_metadata': {
                'calculation_method': 'Enhanced_5D_PCA',
                'dimensions_count': 5,
                'original_dli_dimensions': self.original_dimensions,
                'pagerank_dimensions': [self.pagerank_export_col, self.pagerank_import_col],
                'analysis_timestamp': datetime.now().isoformat(),
                'total_records': len(enhanced_data)
            },
            'export_dli_weights': self.pca_weights.get('export', {}),
            'import_dli_weights': self.pca_weights.get('import', {}),
            'correlation_analysis': {
                'export_correlation_summary': self._summarize_correlation(
                    self.correlation_matrices.get('export')
                ),
                'import_correlation_summary': self._summarize_correlation(
                    self.correlation_matrices.get('import')
                )
            }
        }
        
        json_path = self.output_dir / "dli_pagerank_weights.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(weights_and_params, f, indent=2, ensure_ascii=False, default=str)
        logger.info(f"ğŸ“ˆ æƒé‡å’Œå‚æ•°å·²ä¿å­˜: {json_path}")
        
        return csv_path, json_path
    
    def _summarize_correlation(self, corr_matrix: pd.DataFrame) -> Dict:
        """
        æ€»ç»“ç›¸å…³çŸ©é˜µçš„å…³é”®ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            corr_matrix: ç›¸å…³ç³»æ•°çŸ©é˜µ
            
        Returns:
            Dict: ç›¸å…³æ€§æ‘˜è¦ç»Ÿè®¡
        """
        if corr_matrix is None:
            return {'error': 'ç›¸å…³çŸ©é˜µä¸å­˜åœ¨'}
        
        # æå–ä¸Šä¸‰è§’ç›¸å…³ç³»æ•°ï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
        upper_triangle = []
        n = len(corr_matrix)
        for i in range(n):
            for j in range(i+1, n):
                upper_triangle.append(corr_matrix.iloc[i, j])
        
        upper_triangle = np.array(upper_triangle)
        
        return {
            'mean_correlation': float(np.mean(upper_triangle)),
            'max_correlation': float(np.max(upper_triangle)),
            'min_correlation': float(np.min(upper_triangle)),
            'std_correlation': float(np.std(upper_triangle)),
            'high_correlation_pairs': self._find_high_correlation_pairs(corr_matrix)
        }
    
    def _find_high_correlation_pairs(self, corr_matrix: pd.DataFrame, threshold: float = 0.7) -> List[Dict]:
        """
        æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„ç»´åº¦å¯¹
        
        Args:
            corr_matrix: ç›¸å…³çŸ©é˜µ
            threshold: é«˜ç›¸å…³æ€§é˜ˆå€¼
            
        Returns:
            List[Dict]: é«˜ç›¸å…³æ€§ç»´åº¦å¯¹åˆ—è¡¨
        """
        high_corr_pairs = []
        n = len(corr_matrix)
        
        for i in range(n):
            for j in range(i+1, n):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) >= threshold:
                    high_corr_pairs.append({
                        'dimension_1': corr_matrix.index[i],
                        'dimension_2': corr_matrix.columns[j],
                        'correlation': float(corr_val)
                    })
        
        return high_corr_pairs
    
    def generate_summary_report(self, enhanced_data: pd.DataFrame) -> str:
        """
        ç”Ÿæˆå¢å¼ºç‰ˆDLIåˆ†ææ‘˜è¦æŠ¥å‘Š
        
        Args:
            enhanced_data: å¢å¼ºç‰ˆDLIæ•°æ®
            
        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        report = []
        report.append("# å¢å¼ºç‰ˆåŠ¨æ€é”å®šæŒ‡æ•°(Enhanced DLI)åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        report.append("## 1. æ•°æ®æ¦‚è§ˆ")
        report.append(f"- æ€»è®°å½•æ•°: {len(enhanced_data):,}")
        report.append(f"- å¹´ä»½èŒƒå›´: {enhanced_data['year'].min()}-{enhanced_data['year'].max()}")
        report.append(f"- è¦†ç›–å›½å®¶: {enhanced_data['us_partner'].nunique()}")
        report.append(f"- èƒ½æºäº§å“: {enhanced_data['energy_product'].nunique()}")
        report.append("")
        
        report.append("## 2. æ–¹æ³•è®ºåˆ›æ–°")
        report.append("- äº”ç»´åº¦åˆ†ææ¡†æ¶: ä¼ ç»Ÿ4ç»´ + ä¸ªæ€§åŒ–PageRankç½‘ç»œç»´åº¦")
        report.append("- ä¸¥æ ¼æ–¹å‘æ€§åŒºåˆ†: ç¾å›½å‡ºå£é”å®š vs ç¾å›½è¿›å£è¢«é”å®š")  
        report.append("- PCAå¤„ç†å¤šé‡å…±çº¿æ€§: æ•°æ®é©±åŠ¨çš„æƒé‡ç¡®å®š")
        report.append("- å­¦æœ¯è§„èŒƒè¯Šæ–­: ç›¸å…³æ€§åˆ†æå’Œå¯è§†åŒ–")
        report.append("")
        
        # æ·»åŠ æƒé‡åˆ†æ
        if 'export' in self.pca_weights and 'import' in self.pca_weights:
            report.append("## 3. PCAæƒé‡åˆ†æ")
            
            export_weights = self.pca_weights['export']['dimension_weights']
            import_weights = self.pca_weights['import']['dimension_weights']
            
            report.append("### ç¾å›½å‡ºå£é”å®šæƒé‡:")
            for dim, weight in export_weights.items():
                report.append(f"- {dim}: {weight:.3f}")
            
            report.append("\n### ç¾å›½è¿›å£è¢«é”å®šæƒé‡:")
            for dim, weight in import_weights.items():
                report.append(f"- {dim}: {weight:.3f}")
            
            report.append(f"\n- å‡ºå£é”å®šç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {self.pca_weights['export']['first_pc_variance_explained']:.3f}")
            report.append(f"- è¿›å£é”å®šç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {self.pca_weights['import']['first_pc_variance_explained']:.3f}")
        
        report.append("\n## 4. æ ¸å¿ƒå‘ç°")
        
        # ç»Ÿè®¡åˆ†æ
        export_data = enhanced_data[enhanced_data['us_role'] == 'exporter']
        import_data = enhanced_data[enhanced_data['us_role'] == 'importer']
        
        if len(export_data) > 0:
            export_mean = export_data['dli_enhanced'].mean()
            report.append(f"- ç¾å›½å‡ºå£é”å®šå¹³å‡æ°´å¹³: {export_mean:.3f}")
        
        if len(import_data) > 0:
            import_mean = import_data['dli_enhanced'].mean()
            report.append(f"- ç¾å›½è¿›å£è¢«é”å®šå¹³å‡æ°´å¹³: {import_mean:.3f}")
        
        report.append("\n## 5. æ–‡ä»¶è¾“å‡º")
        report.append("- dli_pagerank.csv: å¢å¼ºç‰ˆDLIé¢æ¿æ•°æ®")
        report.append("- dli_pagerank_weights.json: PCAæƒé‡å’Œåˆ†æå‚æ•°")
        report.append("- dli_dimensions_correlation.png: ç»´åº¦ç›¸å…³æ€§çƒ­åŠ›å›¾")
        
        return "\n".join(report)
    
    def run_full_analysis(self) -> Tuple[pd.DataFrame, Path, Path]:
        """
        è¿è¡Œå®Œæ•´çš„å¢å¼ºç‰ˆDLIåˆ†ææµç¨‹
        
        Returns:
            Tuple[pd.DataFrame, Path, Path]: å¢å¼ºç‰ˆæ•°æ®, CSVè·¯å¾„, JSONè·¯å¾„
        """
        logger.info("=" * 60)
        logger.info("ğŸŒŸ å¢å¼ºç‰ˆDLIè®¡ç®—ç³»ç»Ÿå¯åŠ¨")
        logger.info("=" * 60)
        
        try:
            # 1. æ•°æ®åŠ è½½
            dli_data, pagerank_data = self.load_data()
            
            # 2. æ•°æ®æ•´åˆ
            enhanced_data = self.integrate_pagerank_data(dli_data, pagerank_data)
            
            # 3. åˆ›å»ºæ–¹å‘æ€§æ•°æ®é›†
            export_data, import_data = self.create_directional_datasets(enhanced_data)
            
            # 4. ç›¸å…³æ€§è¯Šæ–­åˆ†æ
            logger.info("ğŸ“Š æ‰§è¡Œå­¦æœ¯è§„èŒƒè¯Šæ–­åˆ†æ...")
            export_corr = self.diagnose_correlations(export_data, 'export')
            import_corr = self.diagnose_correlations(import_data, 'import')
            
            # 5. åˆ›å»ºç›¸å…³ç³»æ•°çŸ©é˜µçƒ­åŠ›å›¾
            self.create_correlation_heatmap(export_corr, import_corr)
            
            # 6. è®¡ç®—å¢å¼ºç‰ˆDLIï¼ˆåˆ†åˆ«ä¸ºä¸¤ä¸ªæ–¹å‘ï¼‰
            export_enhanced, export_weights = self.calculate_enhanced_dli_with_pca(export_data, 'export')
            import_enhanced, import_weights = self.calculate_enhanced_dli_with_pca(import_data, 'import')
            
            # 7. åˆå¹¶åŒå‘ç»“æœ
            final_enhanced_data = self.combine_directional_results(export_enhanced, import_enhanced)
            
            # 8. ä¿å­˜ç»“æœ
            csv_path, json_path = self.save_results(final_enhanced_data)
            
            # 9. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            logger.info("ğŸ“„ ç”Ÿæˆåˆ†ææ‘˜è¦æŠ¥å‘Š...")
            report_content = self.generate_summary_report(final_enhanced_data)
            report_path = self.output_dir / "enhanced_dli_analysis_report.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            logger.info(f"ğŸ“ æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
            # 10. è¾“å‡ºå®Œæˆä¿¡æ¯
            logger.info("=" * 60)
            logger.info("ğŸ‰ å¢å¼ºç‰ˆDLIåˆ†æå®Œæˆ!")
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {len(final_enhanced_data):,}")
            logger.info(f"ğŸ“… è¦†ç›–å¹´ä»½: {final_enhanced_data['year'].min()}-{final_enhanced_data['year'].max()}")
            logger.info(f"ğŸŒ è¦†ç›–å›½å®¶: {final_enhanced_data['us_partner'].nunique()}")
            logger.info(f"ğŸ“ ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
            logger.info(f"  â€¢ {csv_path.name}")
            logger.info(f"  â€¢ {json_path.name}")
            logger.info(f"  â€¢ dli_dimensions_correlation.png")
            logger.info(f"  â€¢ enhanced_dli_analysis_report.md")
            
            return final_enhanced_data, csv_path, json_path
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æè¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å¢å¼ºç‰ˆåŠ¨æ€é”å®šæŒ‡æ•°è®¡ç®—ç³»ç»Ÿ v1.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python dli_pagerank.py                                                        # ä½¿ç”¨é»˜è®¤è·¯å¾„
  python dli_pagerank.py --dli-data ./outputs/dli_panel_data.csv               # æŒ‡å®šDLIæ•°æ®
  python dli_pagerank.py --pagerank-data ../03_metrics/outputs/personalized_pagerank_panel.csv  # æŒ‡å®šPageRankæ•°æ®
  python dli_pagerank.py --output-dir ./enhanced_outputs                       # æŒ‡å®šè¾“å‡ºç›®å½•
        """
    )
    
    # è®¾ç½®é»˜è®¤è·¯å¾„
    current_dir = Path(__file__).parent
    default_dli_data = current_dir / "outputs" / "dli_panel_data.csv"
    default_pagerank_data = current_dir.parent / "03_metrics" / "outputs" / "personalized_pagerank_panel.csv" 
    default_output_dir = current_dir / "outputs"
    
    parser.add_argument(
        '--dli-data',
        type=str,
        default=str(default_dli_data),
        help=f'DLIå››ç»´åº¦æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_dli_data})'
    )
    
    parser.add_argument(
        '--pagerank-data', 
        type=str,
        default=str(default_pagerank_data),
        help=f'ä¸ªæ€§åŒ–PageRankæ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_pagerank_data})'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default=str(default_output_dir),
        help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {default_output_dir})'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # åˆ›å»ºå¢å¼ºç‰ˆDLIè®¡ç®—å™¨
        calculator = EnhancedDLICalculator(
            dli_data_path=Path(args.dli_data),
            pagerank_data_path=Path(args.pagerank_data),
            output_dir=Path(args.output_dir)
        )
        
        # è¿è¡Œå®Œæ•´åˆ†æ
        enhanced_data, csv_path, json_path = calculator.run_full_analysis()
        
        print(f"\nâœ… å¢å¼ºç‰ˆDLIè®¡ç®—æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“Š å¢å¼ºç‰ˆæ•°æ®æ–‡ä»¶: {csv_path}")
        print(f"âš–ï¸  æƒé‡å‚æ•°æ–‡ä»¶: {json_path}")
        print(f"ğŸ“ˆ ç›¸å…³æ€§çƒ­åŠ›å›¾: dli_dimensions_correlation.png")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ è®¡ç®—å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)