#!/usr/bin/env python3
"""
æ•°æ®åŠ è½½ä¸é¢„å¤„ç†æ¨¡å— (Data Loader & Preprocessing)
================================================

æœ¬æ¨¡å—è´Ÿè´£æ•´åˆæ¥è‡ªé¡¹ç›®å…¶ä»–æ¨¡å—çš„æ•°æ®ï¼Œä¸ºå¼‚è´¨æ€§åˆ†ææä¾›ç»Ÿä¸€çš„æ•°æ®æ¥å£ã€‚
æ•´åˆæ•°æ®åŒ…æ‹¬ï¼šDLIæ•ˆåº”æŒ‡æ ‡ã€å…¨å±€/å±€éƒ¨ç½‘ç»œæŒ‡æ ‡ã€å› æœåˆ†æåŸºå‡†æ•°æ®ã€‚

ä½œè€…ï¼šEnergy Network Analysis Team
ç‰ˆæœ¬ï¼šv1.0
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
import sys
import pickle
import json
from typing import Dict, Tuple, Optional, List

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é¡¹ç›®æ ¹è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
SRC_ROOT = PROJECT_ROOT / "src"


class HeterogeneityDataLoader:
    """ç½‘ç»œç»“æ„å¼‚è´¨æ€§åˆ†ææ•°æ®åŠ è½½å™¨"""
    
    def __init__(self):
        self.dli_data = None
        self.global_metrics = None
        self.local_metrics = None
        self.causal_data = None
        logger.info("ğŸš€ åˆå§‹åŒ–å¼‚è´¨æ€§åˆ†ææ•°æ®åŠ è½½å™¨")
    
    def load_dli_data(self) -> pd.DataFrame:
        """
        åŠ è½½DLIæ•ˆåº”æŒ‡æ ‡æ•°æ®
        
        Returns:
            åŒ…å«DLIæŒ‡æ ‡çš„DataFrame
        """
        logger.info("ğŸ“Š åŠ è½½DLIæ•ˆåº”æŒ‡æ ‡æ•°æ®...")
        
        dli_paths = [
            SRC_ROOT / "04_dli_analysis" / "dli_panel_data.csv",
            SRC_ROOT / "04_dli_analysis" / "outputs" / "dli_panel_data.csv"
        ]
        
        for path in dli_paths:
            if path.exists():
                self.dli_data = pd.read_csv(path)
                logger.info(f"âœ… æˆåŠŸåŠ è½½DLIæ•°æ®: {len(self.dli_data)} è¡Œ")
                return self.dli_data
        
        # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®
        logger.warning("âš ï¸ æœªæ‰¾åˆ°DLIæ•°æ®æ–‡ä»¶ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®")
        self.dli_data = self._generate_demo_dli_data()
        return self.dli_data
    
    def load_global_metrics(self) -> pd.DataFrame:
        """
        åŠ è½½å…¨å±€ç½‘ç»œæŒ‡æ ‡æ•°æ®
        
        Returns:
            åŒ…å«å…¨å±€ç½‘ç»œæŒ‡æ ‡çš„DataFrame
        """
        logger.info("ğŸŒ åŠ è½½å…¨å±€ç½‘ç»œæŒ‡æ ‡æ•°æ®...")
        
        metrics_paths = [
            SRC_ROOT / "03_metrics" / "global_network_metrics.csv",
            SRC_ROOT / "03_metrics" / "all_metrics.csv"
        ]
        
        for path in metrics_paths:
            if path.exists():
                df = pd.read_csv(path)
                # ç­›é€‰å…¨å±€æŒ‡æ ‡
                global_cols = [col for col in df.columns if 'global_' in col or 'network_' in col]
                if 'year' in df.columns:
                    global_cols.append('year')
                
                if global_cols:
                    self.global_metrics = df[global_cols].drop_duplicates(subset=['year'] if 'year' in global_cols else None)
                    logger.info(f"âœ… æˆåŠŸåŠ è½½å…¨å±€æŒ‡æ ‡æ•°æ®: {len(self.global_metrics)} è¡Œ, {len(global_cols)} åˆ—")
                    return self.global_metrics
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        logger.warning("âš ï¸ æœªæ‰¾åˆ°å…¨å±€æŒ‡æ ‡æ•°æ®ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®")
        self.global_metrics = self._generate_demo_global_metrics()
        return self.global_metrics
    
    def load_local_metrics(self) -> pd.DataFrame:
        """
        åŠ è½½å±€éƒ¨èŠ‚ç‚¹æŒ‡æ ‡æ•°æ®
        
        Returns:
            åŒ…å«èŠ‚ç‚¹ä¸­å¿ƒæ€§æŒ‡æ ‡çš„DataFrame
        """
        logger.info("ğŸ  åŠ è½½å±€éƒ¨èŠ‚ç‚¹æŒ‡æ ‡æ•°æ®...")
        
        metrics_paths = [
            SRC_ROOT / "03_metrics" / "node_centrality_metrics.csv",
            SRC_ROOT / "03_metrics" / "all_metrics.csv"
        ]
        
        for path in metrics_paths:
            if path.exists():
                df = pd.read_csv(path)
                # ç­›é€‰èŠ‚ç‚¹æŒ‡æ ‡
                if 'country_code' in df.columns:
                    required_cols = ['year', 'country_code']
                    centrality_cols = [col for col in df.columns if any(x in col for x in 
                                     ['degree', 'centrality', 'strength', 'pagerank'])]
                    
                    node_cols = required_cols + centrality_cols
                    available_cols = [col for col in node_cols if col in df.columns]
                    
                    if len(available_cols) > 2:  # è‡³å°‘æœ‰year, country_codeå’Œä¸€ä¸ªæŒ‡æ ‡
                        self.local_metrics = df[available_cols]
                        logger.info(f"âœ… æˆåŠŸåŠ è½½å±€éƒ¨æŒ‡æ ‡æ•°æ®: {len(self.local_metrics)} è¡Œ")
                        return self.local_metrics
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        logger.warning("âš ï¸ æœªæ‰¾åˆ°å±€éƒ¨æŒ‡æ ‡æ•°æ®ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®")
        self.local_metrics = self._generate_demo_local_metrics()
        return self.local_metrics
    
    def load_causal_data(self) -> pd.DataFrame:
        """
        åŠ è½½å› æœåˆ†æåŸºå‡†æ•°æ®
        
        Returns:
            åŒ…å«å› æœåˆ†æå˜é‡çš„DataFrame
        """
        logger.info("ğŸ”— åŠ è½½å› æœåˆ†æåŸºå‡†æ•°æ®...")
        
        causal_paths = [
            SRC_ROOT / "05_causal_validation" / "outputs" / "network_resilience.csv",
            SRC_ROOT / "05_causal_validation" / "network_resilience.csv"
        ]
        
        for path in causal_paths:
            if path.exists():
                self.causal_data = pd.read_csv(path)
                logger.info(f"âœ… æˆåŠŸåŠ è½½å› æœåˆ†ææ•°æ®: {len(self.causal_data)} è¡Œ")
                return self.causal_data
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®
        logger.warning("âš ï¸ æœªæ‰¾åˆ°å› æœåˆ†ææ•°æ®ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®")
        self.causal_data = self._generate_demo_causal_data()
        return self.causal_data
    
    def load_all_data(self) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½æ‰€æœ‰å¿…è¦æ•°æ®
        
        Returns:
            åŒ…å«æ‰€æœ‰æ•°æ®é›†çš„å­—å…¸
        """
        logger.info("ğŸ“¦ å¼€å§‹åŠ è½½æ‰€æœ‰æ•°æ®...")
        
        data = {
            'dli': self.load_dli_data(),
            'global_metrics': self.load_global_metrics(),
            'local_metrics': self.load_local_metrics(),
            'causal': self.load_causal_data()
        }
        
        logger.info("âœ… æ‰€æœ‰æ•°æ®åŠ è½½å®Œæˆ")
        return data
    
    def create_analysis_dataset(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        åˆ›å»ºåˆ†ææ•°æ®é›†ï¼Œæ•´åˆæ‰€æœ‰æ•°æ®æº
        
        Returns:
            (global_dataset, local_dataset): å…¨å±€åˆ†æå’Œå±€éƒ¨åˆ†ææ•°æ®é›†
        """
        logger.info("ğŸ”§ æ„å»ºåˆ†ææ•°æ®é›†...")
        
        # åŠ è½½æ‰€æœ‰æ•°æ®
        data = self.load_all_data()
        
        # æ„å»ºå…¨å±€åˆ†ææ•°æ®é›†
        global_dataset = self._build_global_dataset(data)
        
        # æ„å»ºå±€éƒ¨åˆ†ææ•°æ®é›†  
        local_dataset = self._build_local_dataset(data)
        
        logger.info(f"âœ… åˆ†ææ•°æ®é›†æ„å»ºå®Œæˆ:")
        logger.info(f"   - å…¨å±€åˆ†ææ•°æ®é›†: {len(global_dataset)} è¡Œ")
        logger.info(f"   - å±€éƒ¨åˆ†ææ•°æ®é›†: {len(local_dataset)} è¡Œ")
        
        return global_dataset, local_dataset
    
    def _build_global_dataset(self, data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """æ„å»ºå…¨å±€åˆ†ææ•°æ®é›† - åˆ†æå•ä½ï¼šå›½å®¶-å¹´ä»½"""
        # ä»å› æœåˆ†ææ•°æ®å¼€å§‹ï¼ˆå·²ç»æ˜¯å›½å®¶-å¹´ä»½æ ¼å¼ï¼‰
        global_df = data['causal'].copy()
        
        # å¤„ç†DLIæ•°æ®ï¼šä»åŒè¾¹å…³ç³»èšåˆåˆ°å›½å®¶å±‚é¢
        if 'year' in data['dli'].columns:
            # ä¸ºç¾å›½æ„å»ºå›½å®¶å±‚é¢çš„DLIæŒ‡æ ‡
            if 'us_partner' in data['dli'].columns:
                # æŒ‰å¹´ä»½å’Œç¾å›½è§’è‰²èšåˆDLIï¼ˆè¿›å£é”å®š+å‡ºå£é”å®šï¼‰
                dli_country_level = data['dli'].groupby(['year', 'us_role']).agg({
                    'dli_score': ['mean', 'sum', 'std'],
                    'trade_value_usd': 'sum'
                }).reset_index()
                
                # é‡å‘½ååˆ—
                dli_country_level.columns = ['year', 'us_role', 'dli_mean', 'dli_total', 'dli_volatility', 'total_trade_value']
                
                # é€è§†è½¬æ¢ï¼šè¿›å£é”å®šå’Œå‡ºå£é”å®šåˆ†å¼€
                dli_pivot = dli_country_level.pivot(index='year', columns='us_role', 
                                                   values=['dli_mean', 'dli_total', 'dli_volatility'])
                dli_pivot.columns = [f'{metric}_{role}' for metric, role in dli_pivot.columns]
                dli_pivot = dli_pivot.reset_index()
                
                # è®¡ç®—ç»¼åˆé”å®šæŒ‡æ ‡
                if 'dli_mean_importer' in dli_pivot.columns and 'dli_mean_exporter' in dli_pivot.columns:
                    dli_pivot['dli_composite'] = (dli_pivot['dli_mean_importer'] + dli_pivot['dli_mean_exporter']) / 2
                
                # åˆå¹¶åˆ°å…¨å±€æ•°æ®é›†
                if 'year' in global_df.columns:
                    global_df = pd.merge(global_df, dli_pivot, on='year', how='left')
        
        # åˆå¹¶å…¨å±€ç½‘ç»œæŒ‡æ ‡
        if 'year' in global_df.columns and 'year' in data['global_metrics'].columns:
            global_df = pd.merge(global_df, data['global_metrics'], on='year', how='left')
        
        return global_df
    
    def _build_local_dataset(self, data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """æ„å»ºå±€éƒ¨åˆ†ææ•°æ®é›† - åˆ†æå•ä½ï¼šå›½å®¶-å¹´ä»½"""
        # ä»å› æœåˆ†ææ•°æ®å¼€å§‹ï¼ˆå·²ç»æ˜¯å›½å®¶-å¹´ä»½æ ¼å¼ï¼‰
        local_df = data['causal'].copy()
        
        # å¤„ç†DLIæ•°æ®ï¼šä»åŒè¾¹å…³ç³»èšåˆåˆ°å›½å®¶å±‚é¢
        if 'year' in data['dli'].columns and 'us_partner' in data['dli'].columns:
            # è®¡ç®—æ¯ä¸ªå›½å®¶ä½œä¸ºç¾å›½ä¼™ä¼´æ—¶çš„DLIæš´éœ²åº¦
            dli_partner_level = data['dli'].groupby(['year', 'us_partner', 'us_role']).agg({
                'dli_score': 'mean',
                'trade_value_usd': 'sum'
            }).reset_index()
            
            # é€è§†ï¼šåˆ†åˆ«è®¡ç®—è¯¥å›½ä»ç¾å›½è¿›å£å’Œå‘ç¾å›½å‡ºå£çš„é”å®šåº¦
            dli_partner_pivot = dli_partner_level.pivot_table(
                index=['year', 'us_partner'], 
                columns='us_role',
                values='dli_score',
                aggfunc='mean'
            ).reset_index()
            dli_partner_pivot.columns.name = None
            
            # é‡å‘½ååˆ—
            col_mapping = {}
            for col in dli_partner_pivot.columns:
                if col == 'exporter':  # ç¾å›½ä½œä¸ºå‡ºå£å•†ï¼Œä¼™ä¼´å›½è¢«é”å®šä¸ºè¿›å£å•†
                    col_mapping[col] = 'partner_import_locking'
                elif col == 'importer':  # ç¾å›½ä½œä¸ºè¿›å£å•†ï¼Œä¼™ä¼´å›½è¢«é”å®šä¸ºå‡ºå£å•†  
                    col_mapping[col] = 'partner_export_locking'
            dli_partner_pivot.rename(columns=col_mapping, inplace=True)
            
            # è®¡ç®—ç»¼åˆé”å®šæŒ‡æ•°
            if 'partner_import_locking' in dli_partner_pivot.columns and 'partner_export_locking' in dli_partner_pivot.columns:
                dli_partner_pivot['partner_total_locking'] = (
                    dli_partner_pivot['partner_import_locking'].fillna(0) + 
                    dli_partner_pivot['partner_export_locking'].fillna(0)
                ) / 2
            
            # åˆå¹¶åˆ°å±€éƒ¨æ•°æ®é›†ï¼ˆåŒ¹é…countryå­—æ®µä¸us_partnerï¼‰
            if 'country' in local_df.columns:
                local_df = pd.merge(local_df, dli_partner_pivot, 
                                  left_on=['year', 'country'], 
                                  right_on=['year', 'us_partner'], 
                                  how='left')
        
        # åˆå¹¶å±€éƒ¨èŠ‚ç‚¹æŒ‡æ ‡ï¼ˆæŒ‰å¹´ä»½å’Œå›½å®¶ç²¾ç¡®åŒ¹é…ï¼‰
        if 'year' in local_df.columns and 'country' in local_df.columns:
            if 'year' in data['local_metrics'].columns and 'country_code' in data['local_metrics'].columns:
                local_df = pd.merge(local_df, data['local_metrics'], 
                                  left_on=['year', 'country'], 
                                  right_on=['year', 'country_code'], 
                                  how='left')
        
        return local_df
    
    def _generate_demo_dli_data(self) -> pd.DataFrame:
        """ç”ŸæˆDLIç¤ºä¾‹æ•°æ®"""
        years = range(2010, 2025)
        countries = ['USA', 'CHN', 'DEU', 'JPN', 'GBR']
        
        data = []
        for year in years:
            for country in countries:
                data.append({
                    'year': year,
                    'country_code': country,
                    'dli_import': np.random.normal(0.5, 0.2),
                    'dli_export': np.random.normal(0.3, 0.15),
                    'dli_composite': np.random.normal(0.4, 0.18)
                })
        
        return pd.DataFrame(data)
    
    def _generate_demo_global_metrics(self) -> pd.DataFrame:
        """ç”Ÿæˆå…¨å±€æŒ‡æ ‡ç¤ºä¾‹æ•°æ®"""
        years = range(2010, 2025)
        
        data = []
        for year in years:
            data.append({
                'year': year,
                'global_density': np.random.normal(0.3, 0.1),
                'global_transitivity': np.random.normal(0.6, 0.15),
                'global_avg_clustering': np.random.normal(0.7, 0.12),
                'global_efficiency': np.random.normal(0.8, 0.1),
                'network_size': np.random.randint(50, 100)
            })
        
        return pd.DataFrame(data)
    
    def _generate_demo_local_metrics(self) -> pd.DataFrame:
        """ç”Ÿæˆå±€éƒ¨æŒ‡æ ‡ç¤ºä¾‹æ•°æ®"""
        years = range(2010, 2025)
        countries = ['USA', 'CHN', 'DEU', 'JPN', 'GBR', 'FRA', 'ITA', 'RUS', 'SAU', 'CAN']
        
        data = []
        for year in years:
            for country in countries:
                data.append({
                    'year': year,
                    'country_code': country,
                    'betweenness_centrality': np.random.exponential(0.1),
                    'degree_centrality': np.random.beta(2, 5),
                    'pagerank_centrality': np.random.gamma(2, 0.1),
                    'in_strength': np.random.lognormal(5, 1),
                    'out_strength': np.random.lognormal(5, 1)
                })
        
        return pd.DataFrame(data)
    
    def _generate_demo_causal_data(self) -> pd.DataFrame:
        """ç”Ÿæˆå› æœåˆ†æç¤ºä¾‹æ•°æ®"""
        years = range(2010, 2025)
        countries = ['USA', 'CHN', 'DEU', 'JPN', 'GBR']
        
        data = []
        for year in years:
            for country in countries:
                data.append({
                    'year': year,
                    'country': country,  # ä¿®æ”¹ä¸ºcountryä»¥åŒ¹é…05æ¨¡å—çš„æ ¼å¼
                    'comprehensive_resilience': np.random.normal(0.7, 0.2),
                    'dli_composite': np.random.normal(0.1, 0.05),  # æ·»åŠ dli_composite
                    'control_var1': np.random.normal(0, 1),
                    'control_var2': np.random.exponential(1)
                })
        
        return pd.DataFrame(data)


def main():
    """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
    loader = HeterogeneityDataLoader()
    
    # æµ‹è¯•åŠ è½½æ‰€æœ‰æ•°æ®
    global_data, local_data = loader.create_analysis_dataset()
    
    print("ğŸ¯ å…¨å±€åˆ†ææ•°æ®é›†é¢„è§ˆ:")
    print(global_data.head())
    print(f"\nåˆ—å: {list(global_data.columns)}")
    
    print("\nğŸ¯ å±€éƒ¨åˆ†ææ•°æ®é›†é¢„è§ˆ:")
    print(local_data.head())
    print(f"\nåˆ—å: {list(local_data.columns)}")


if __name__ == "__main__":
    main()