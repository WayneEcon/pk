#!/usr/bin/env python3
"""
éª¨å¹²ç½‘ç»œæå–ç®—æ³•é›†æˆæ¨¡å—
==========================

æ•´åˆ Disparity Filter, Maximum Spanning Tree, å’Œ PÃ³lya Urn Filter ä¸‰ç§æ ¸å¿ƒç®—æ³•ã€‚
ä¿æŒç®—æ³•çš„ç§‘å­¦ä¸¥è°¨æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹å…¥åº¦/å‡ºåº¦çš„åˆ†åˆ«æ£€éªŒå’ŒFDRå¤šé‡æ£€éªŒæ ¡æ­£ã€‚

æ ¸å¿ƒç®—æ³•ï¼š
1. disparity_filter() - åŸºäºSerrano et al. (2009)çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
2. maximum_spanning_tree() - åŸºäºå›¾è®ºçš„æœ€ä¼˜è¿é€šæ€§ç®—æ³•
3. polya_urn_filter() - åŸºäºPÃ³lya Urnæ¨¡å‹çš„è¡¥å……éªŒè¯ç®—æ³•

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import numpy as np
import pandas as pd
import networkx as nx
from typing import Dict, List, Tuple, Optional, Union
from scipy import stats
import logging

logger = logging.getLogger(__name__)


def calculate_disparity_pvalue(weight: float, node_strength: float, node_degree: int) -> float:
    """
    è®¡ç®—å•æ¡è¾¹çš„Disparity Filter på€¼
    
    åŸºäºSerrano et al. (2009)çš„æ­£ç¡®å…¬å¼ï¼š
    åœ¨null modelä¸‹ï¼Œæƒé‡æŒ‰èŠ‚ç‚¹å¼ºåº¦éšæœºåˆ†é…ï¼Œæ¯æ¡è¾¹æƒé‡æ¯”ä¾‹éµå¾ªBetaåˆ†å¸ƒ
    
    Args:
        weight: è¾¹çš„æƒé‡
        node_strength: èŠ‚ç‚¹çš„æ€»å¼ºåº¦ï¼ˆæ‰€æœ‰å‡ºè¾¹æƒé‡ä¹‹å’Œï¼‰
        node_degree: èŠ‚ç‚¹çš„åº¦æ•°ï¼ˆå‡ºè¾¹æ•°é‡ï¼‰
        
    Returns:
        è¯¥è¾¹åœ¨null modelä¸‹çš„på€¼
    """
    
    if node_degree <= 1 or node_strength <= 0 or weight <= 0:
        return 1.0  # åº¦ä¸º1ã€å¼ºåº¦ä¸º0æˆ–æƒé‡ä¸º0æ—¶ï¼Œæ— æ³•è®¡ç®—æ˜¾è‘—æ€§
    
    # è®¡ç®—å½’ä¸€åŒ–æƒé‡æ¯”ä¾‹
    p_ij = weight / node_strength
    
    # Serrano et al. (2009)çš„æ­£ç¡®å…¬å¼
    # P(X >= p_ij) = (1 - p_ij)^(k-1)
    # è¿™åŸºäºåœ¨kä¸ªè¾¹ä¸­éšæœºåˆ†é…æƒé‡çš„null model
    k = node_degree
    
    # é¿å…æ•°å€¼è®¡ç®—é—®é¢˜
    if p_ij >= 1.0:
        return 0.0
    if p_ij <= 0.0:
        return 1.0
    
    # è®¡ç®—på€¼ï¼šP(proportion >= p_ij | null model)
    p_value = (1.0 - p_ij) ** (k - 1)
    
    return min(p_value, 1.0)


def benjamini_hochberg_fdr(p_values: np.ndarray, alpha: float = 0.05) -> np.ndarray:
    """
    Benjamini-Hochberg FDRå¤šé‡æ¯”è¾ƒæ ¡æ­£
    
    Args:
        p_values: åŸå§‹på€¼æ•°ç»„
        alpha: FDRæ°´å¹³
        
    Returns:
        FDRæ ¡æ­£åçš„æ˜¾è‘—æ€§åˆ¤æ–­ï¼ˆå¸ƒå°”æ•°ç»„ï¼‰
    """
    
    n = len(p_values)
    if n == 0:
        return np.array([], dtype=bool)
    
    # æŒ‰på€¼æ’åº
    sorted_indices = np.argsort(p_values)
    sorted_p_values = p_values[sorted_indices]
    
    # BHä¸´ç•Œå€¼ï¼šp_i <= (i/n) * alpha
    critical_values = (np.arange(1, n + 1) / n) * alpha
    
    # æ‰¾åˆ°æœ€å¤§çš„æ»¡è¶³æ¡ä»¶çš„ç´¢å¼•
    significant_sorted = sorted_p_values <= critical_values
    
    # å¦‚æœæœ‰æ˜¾è‘—çš„på€¼ï¼Œæ‰¾åˆ°æœ€å¤§çš„ç´¢å¼•
    if np.any(significant_sorted):
        max_significant_idx = np.max(np.where(significant_sorted)[0])
        # å‰max_significant_idx+1ä¸ªéƒ½æ˜¯æ˜¾è‘—çš„
        significant_sorted[:max_significant_idx + 1] = True
        significant_sorted[max_significant_idx + 1:] = False
    
    # æ¢å¤åŸå§‹é¡ºåº
    significant = np.empty(n, dtype=bool)
    significant[sorted_indices] = significant_sorted
    
    return significant


def disparity_filter(G: nx.Graph, 
                    alpha: float = 0.05,
                    fdr_correction: bool = True,
                    weight_attr: str = 'weight',
                    directed: bool = None) -> nx.Graph:
    """
    å¯¹ç½‘ç»œåº”ç”¨Disparity Filterç®—æ³•
    
    **å…³é”®è¦æ±‚**: å¯¹æœ‰å‘å›¾å¿…é¡»åˆ†åˆ«æ£€éªŒå…¥åº¦å’Œå‡ºåº¦ï¼Œå¹¶åº”ç”¨FDRå¤šé‡æ£€éªŒæ ¡æ­£
    
    Args:
        G: è¾“å…¥ç½‘ç»œï¼ˆNetworkXå›¾å¯¹è±¡ï¼‰
        alpha: æ˜¾è‘—æ€§æ°´å¹³
        fdr_correction: æ˜¯å¦åº”ç”¨FDRå¤šé‡æ¯”è¾ƒæ ¡æ­£
        weight_attr: è¾¹æƒé‡å±æ€§å
        directed: æ˜¯å¦ä½œä¸ºæœ‰å‘å›¾å¤„ç†ï¼ŒNoneæ—¶è‡ªåŠ¨æ£€æµ‹
        
    Returns:
        è¿‡æ»¤åçš„éª¨å¹²ç½‘ç»œ
    """
    
    logger.info(f"ğŸ” å¼€å§‹åº”ç”¨Disparity Filter (Î±={alpha}, FDR={fdr_correction})...")
    
    if directed is None:
        directed = G.is_directed()
    
    # å¤åˆ¶å›¾ä»¥é¿å…ä¿®æ”¹åŸå›¾
    G_filtered = G.copy()
    
    # æ”¶é›†æ‰€æœ‰è¾¹çš„på€¼
    edge_pvalues = []
    edge_list = []
    
    # å¤„ç†æ¯ä¸ªèŠ‚ç‚¹çš„å‡ºè¾¹å’Œå…¥è¾¹
    for node in G.nodes():
        if directed:
            # æœ‰å‘å›¾ï¼šåˆ†åˆ«å¤„ç†å‡ºè¾¹å’Œå…¥è¾¹
            out_edges = list(G.out_edges(node, data=True))
            in_edges = list(G.in_edges(node, data=True))
            
            # å¤„ç†å‡ºè¾¹
            if len(out_edges) > 1:
                out_strength = sum([data.get(weight_attr, 1.0) for _, _, data in out_edges])
                out_degree = len(out_edges)
                
                for source, target, data in out_edges:
                    weight = data.get(weight_attr, 1.0)
                    p_value = calculate_disparity_pvalue(weight, out_strength, out_degree)
                    edge_pvalues.append(p_value)
                    edge_list.append((source, target, 'out'))
            
            # å¤„ç†å…¥è¾¹
            if len(in_edges) > 1:
                in_strength = sum([data.get(weight_attr, 1.0) for _, _, data in in_edges])
                in_degree = len(in_edges)
                
                for source, target, data in in_edges:
                    weight = data.get(weight_attr, 1.0)
                    p_value = calculate_disparity_pvalue(weight, in_strength, in_degree)
                    edge_pvalues.append(p_value)
                    edge_list.append((source, target, 'in'))
        
        else:
            # æ— å‘å›¾ï¼šåªå¤„ç†åº¦å¤§äº1çš„èŠ‚ç‚¹
            neighbors = list(G.neighbors(node))
            if len(neighbors) > 1:
                node_strength = sum([G[node][neighbor].get(weight_attr, 1.0) 
                                   for neighbor in neighbors])
                node_degree = len(neighbors)
                
                for neighbor in neighbors:
                    # é¿å…é‡å¤è®¡ç®—åŒä¸€æ¡è¾¹
                    if node < neighbor or directed:
                        weight = G[node][neighbor].get(weight_attr, 1.0)
                        p_value = calculate_disparity_pvalue(weight, node_strength, node_degree)
                        edge_pvalues.append(p_value)
                        edge_list.append((node, neighbor, 'undirected'))
    
    if not edge_pvalues:
        logger.warning("âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„è¾¹è¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒ")
        return G_filtered
    
    edge_pvalues = np.array(edge_pvalues)
    
    # åº”ç”¨FDRæ ¡æ­£æˆ–ç›´æ¥æ¯”è¾ƒalpha
    if fdr_correction:
        significant_edges = benjamini_hochberg_fdr(edge_pvalues, alpha)
        logger.info(f"ğŸ“Š FDRæ ¡æ­£åä¿ç•™è¾¹æ•°: {np.sum(significant_edges)}/{len(edge_pvalues)}")
    else:
        significant_edges = edge_pvalues <= alpha
        logger.info(f"ğŸ“Š ç›´æ¥æ¯”è¾ƒåä¿ç•™è¾¹æ•°: {np.sum(significant_edges)}/{len(edge_pvalues)}")
    
    # æ„å»ºè¦åˆ é™¤çš„è¾¹é›†åˆ
    edges_to_remove = set()
    
    for i, (source, target, direction) in enumerate(edge_list):
        if not significant_edges[i]:
            if direction == 'undirected':
                edges_to_remove.add((source, target))
                if not directed:  # æ— å‘å›¾çš„å¯¹ç§°æ€§
                    edges_to_remove.add((target, source))
            else:
                edges_to_remove.add((source, target))
    
    # åˆ é™¤ä¸æ˜¾è‘—çš„è¾¹
    G_filtered.remove_edges_from(edges_to_remove)
    
    # æ·»åŠ è¿‡æ»¤ä¿¡æ¯åˆ°å›¾å±æ€§
    G_filtered.graph['backbone_method'] = 'disparity_filter'
    G_filtered.graph['alpha'] = alpha
    G_filtered.graph['fdr_correction'] = fdr_correction
    G_filtered.graph['original_edges'] = G.number_of_edges()
    G_filtered.graph['filtered_edges'] = G_filtered.number_of_edges()
    G_filtered.graph['retention_rate'] = G_filtered.number_of_edges() / G.number_of_edges()
    
    logger.info(f"âœ… Disparity Filterå®Œæˆ:")
    logger.info(f"   åŸå§‹è¾¹æ•°: {G.number_of_edges():,}")
    logger.info(f"   ä¿ç•™è¾¹æ•°: {G_filtered.number_of_edges():,}")
    logger.info(f"   ä¿ç•™ç‡: {G_filtered.graph['retention_rate']:.1%}")
    
    return G_filtered


def symmetrize_graph(G: nx.DiGraph, weight_attr: str = 'weight', 
                    method: str = 'max') -> nx.Graph:
    """
    å°†æœ‰å‘å›¾å¯¹ç§°åŒ–ä¸ºæ— å‘å›¾
    
    Args:
        G: æœ‰å‘å›¾
        weight_attr: è¾¹æƒé‡å±æ€§å
        method: å¯¹ç§°åŒ–æ–¹æ³• ('max', 'sum', 'mean')
            - 'max': å–åŒå‘è¾¹æƒé‡çš„æœ€å¤§å€¼
            - 'sum': å–åŒå‘è¾¹æƒé‡çš„å’Œ
            - 'mean': å–åŒå‘è¾¹æƒé‡çš„å¹³å‡å€¼
            
    Returns:
        å¯¹ç§°åŒ–åçš„æ— å‘å›¾
    """
    
    logger.info(f"ğŸ”„ å¯¹ç§°åŒ–æœ‰å‘å›¾ (æ–¹æ³•: {method})...")
    
    G_undirected = nx.Graph()
    G_undirected.add_nodes_from(G.nodes(data=True))
    
    # æ”¶é›†æ‰€æœ‰è¾¹çš„æƒé‡
    edge_weights = {}
    
    for source, target, data in G.edges(data=True):
        weight = data.get(weight_attr, 1.0)
        
        # æ ‡å‡†åŒ–è¾¹çš„è¡¨ç¤º (è¾ƒå°èŠ‚ç‚¹åœ¨å‰)
        edge_key = tuple(sorted([source, target]))
        
        if edge_key not in edge_weights:
            edge_weights[edge_key] = []
        
        edge_weights[edge_key].append(weight)
    
    # æ ¹æ®æ–¹æ³•è®¡ç®—æœ€ç»ˆæƒé‡
    for (node1, node2), weights in edge_weights.items():
        if method == 'max':
            final_weight = max(weights)
        elif method == 'sum':
            final_weight = sum(weights)
        elif method == 'mean':
            final_weight = sum(weights) / len(weights)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¹ç§°åŒ–æ–¹æ³•: {method}")
        
        G_undirected.add_edge(node1, node2, **{weight_attr: final_weight})
    
    logger.info(f"   æœ‰å‘è¾¹: {G.number_of_edges():,} -> æ— å‘è¾¹: {G_undirected.number_of_edges():,}")
    
    return G_undirected


def maximum_spanning_tree(G: nx.Graph, 
                         weight_attr: str = 'weight',
                         algorithm: str = 'kruskal') -> nx.Graph:
    """
    è®¡ç®—æœ€å¤§ç”Ÿæˆæ ‘
    
    Args:
        G: è¾“å…¥å›¾ï¼ˆæ— å‘å›¾ï¼‰
        weight_attr: è¾¹æƒé‡å±æ€§å
        algorithm: ç®—æ³•é€‰æ‹© ('kruskal' æˆ– 'prim')
        
    Returns:
        æœ€å¤§ç”Ÿæˆæ ‘
    """
    
    logger.info(f"ğŸŒ³ è®¡ç®—æœ€å¤§ç”Ÿæˆæ ‘ (ç®—æ³•: {algorithm})...")
    
    if G.number_of_nodes() == 0:
        logger.warning("âš ï¸ è¾“å…¥å›¾ä¸ºç©º")
        return nx.Graph()
    
    # NetworkXçš„MSTç®—æ³•é»˜è®¤è®¡ç®—æœ€å°ç”Ÿæˆæ ‘
    # è¦è®¡ç®—æœ€å¤§ç”Ÿæˆæ ‘ï¼Œéœ€è¦å°†æƒé‡å–è´Ÿæ•°
    G_negative = G.copy()
    
    for source, target, data in G_negative.edges(data=True):
        original_weight = data.get(weight_attr, 1.0)
        G_negative[source][target][weight_attr] = -original_weight
    
    # è®¡ç®—æœ€å°ç”Ÿæˆæ ‘ï¼ˆè´Ÿæƒé‡ï¼‰
    if algorithm == 'kruskal':
        mst_negative = nx.minimum_spanning_tree(G_negative, weight=weight_attr, algorithm='kruskal')
    elif algorithm == 'prim':
        mst_negative = nx.minimum_spanning_tree(G_negative, weight=weight_attr, algorithm='prim')
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    # æ¢å¤æ­£æƒé‡
    mst = nx.Graph()
    mst.add_nodes_from(G.nodes(data=True))
    
    total_weight = 0
    for source, target, data in mst_negative.edges(data=True):
        original_weight = -data[weight_attr]  # æ¢å¤æ­£æƒé‡
        mst.add_edge(source, target, **{weight_attr: original_weight})
        total_weight += original_weight
    
    # æ·»åŠ ç”Ÿæˆæ ‘ä¿¡æ¯
    mst.graph['backbone_method'] = 'maximum_spanning_tree'
    mst.graph['algorithm'] = algorithm
    mst.graph['original_edges'] = G.number_of_edges()
    mst.graph['mst_edges'] = mst.number_of_edges()
    mst.graph['total_mst_weight'] = total_weight
    mst.graph['retention_rate'] = mst.number_of_edges() / G.number_of_edges()
    
    logger.info(f"âœ… æœ€å¤§ç”Ÿæˆæ ‘å®Œæˆ:")
    logger.info(f"   åŸå§‹è¾¹æ•°: {G.number_of_edges():,}")
    logger.info(f"   MSTè¾¹æ•°: {mst.number_of_edges():,}")
    logger.info(f"   æ€»æƒé‡: {total_weight:.2f}")
    logger.info(f"   ä¿ç•™ç‡: {mst.graph['retention_rate']:.1%}")
    
    return mst


def polya_urn_filter(G: nx.Graph, 
                    beta: float = 0.05,
                    weight_attr: str = 'weight') -> nx.Graph:
    """
    åŸºäºPÃ³lya Urnæ¨¡å‹çš„éª¨å¹²ç½‘ç»œæå–ç®—æ³•
    
    è¯¥ç®—æ³•ä½œä¸ºDisparity Filterçš„è¡¥å……éªŒè¯æ–¹æ³•ï¼ŒåŸºäºä¸åŒçš„é›¶å‡è®¾ï¼š
    - é›¶å‡è®¾ï¼šè¾¹æƒé‡éµå¾ªPÃ³lyaåˆ†å¸ƒ
    - ç”¨äºäº¤å‰éªŒè¯DFç»“æœçš„ç¨³å¥æ€§
    
    Args:
        G: è¾“å…¥ç½‘ç»œ
        beta: æ˜¾è‘—æ€§é˜ˆå€¼
        weight_attr: è¾¹æƒé‡å±æ€§å
        
    Returns:
        è¿‡æ»¤åçš„éª¨å¹²ç½‘ç»œ
    """
    
    logger.info(f"ğŸ² å¼€å§‹åº”ç”¨PÃ³lya Urn Filter (Î²={beta})...")
    
    G_filtered = G.copy()
    edges_to_remove = []
    
    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åº”ç”¨PÃ³lya Urnæ£€éªŒ
    for node in G.nodes():
        neighbors = list(G.neighbors(node))
        if len(neighbors) <= 1:
            continue
            
        # è·å–èŠ‚ç‚¹çš„æ‰€æœ‰è¾¹æƒé‡
        weights = [G[node][neighbor].get(weight_attr, 1.0) for neighbor in neighbors]
        total_weight = sum(weights)
        
        # PÃ³lya Urnæ¨¡å‹ä¸‹çš„på€¼è®¡ç®—
        for i, neighbor in enumerate(neighbors):
            weight = weights[i]
            
            # ç®€åŒ–çš„PÃ³lya Urn på€¼è®¡ç®—
            # å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨æ›´ç²¾ç¡®çš„ç»Ÿè®¡æ£€éªŒ
            p_ij = weight / total_weight
            k = len(neighbors)
            
            # Betaåˆ†å¸ƒè¿‘ä¼¼ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            expected_prob = 1.0 / k
            variance = expected_prob * (1 - expected_prob) / (k + 1)
            
            if variance > 0:
                z_score = (p_ij - expected_prob) / np.sqrt(variance)
                p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))  # åŒå°¾æ£€éªŒ
            else:
                p_value = 1.0
            
            # å¦‚æœè¾¹ä¸æ˜¾è‘—ï¼Œæ ‡è®°ä¸ºåˆ é™¤
            if p_value > beta:
                if node < neighbor:  # é¿å…é‡å¤åˆ é™¤
                    edges_to_remove.append((node, neighbor))
    
    # åˆ é™¤ä¸æ˜¾è‘—çš„è¾¹
    G_filtered.remove_edges_from(edges_to_remove)
    
    # æ·»åŠ è¿‡æ»¤ä¿¡æ¯
    G_filtered.graph['backbone_method'] = 'polya_urn_filter'
    G_filtered.graph['beta'] = beta
    G_filtered.graph['original_edges'] = G.number_of_edges()
    G_filtered.graph['filtered_edges'] = G_filtered.number_of_edges()
    G_filtered.graph['retention_rate'] = G_filtered.number_of_edges() / G.number_of_edges()
    
    logger.info(f"âœ… PÃ³lya Urn Filterå®Œæˆ:")
    logger.info(f"   åŸå§‹è¾¹æ•°: {G.number_of_edges():,}")
    logger.info(f"   ä¿ç•™è¾¹æ•°: {G_filtered.number_of_edges():,}")
    logger.info(f"   ä¿ç•™ç‡: {G_filtered.graph['retention_rate']:.1%}")
    
    return G_filtered


def apply_all_algorithms(G: nx.Graph, 
                        alpha_values: List[float] = [0.01, 0.05, 0.1],
                        beta: float = 0.05,
                        weight_attr: str = 'weight') -> Dict[str, nx.Graph]:
    """
    å¯¹å•ä¸ªç½‘ç»œåº”ç”¨æ‰€æœ‰éª¨å¹²æå–ç®—æ³•
    
    Args:
        G: è¾“å…¥ç½‘ç»œ
        alpha_values: DFç®—æ³•çš„alphaå€¼åˆ—è¡¨
        beta: PFç®—æ³•çš„betaå€¼
        weight_attr: è¾¹æƒé‡å±æ€§å
        
    Returns:
        åŒ…å«æ‰€æœ‰ç®—æ³•ç»“æœçš„å­—å…¸
    """
    
    results = {}
    
    # 1. Disparity Filter (å¤šä¸ªalphaå€¼)
    for alpha in alpha_values:
        key = f'disparity_filter_{alpha}'
        results[key] = disparity_filter(G, alpha=alpha, fdr_correction=True, weight_attr=weight_attr)
    
    # 2. Maximum Spanning Tree
    if G.is_directed():
        G_sym = symmetrize_graph(G, weight_attr=weight_attr, method='max')
        results['mst'] = maximum_spanning_tree(G_sym, weight_attr=weight_attr)
    else:
        results['mst'] = maximum_spanning_tree(G, weight_attr=weight_attr)
    
    # 3. PÃ³lya Urn Filter
    if G.is_directed():
        G_sym = symmetrize_graph(G, weight_attr=weight_attr, method='max')
        results['polya_urn'] = polya_urn_filter(G_sym, beta=beta, weight_attr=weight_attr)
    else:
        results['polya_urn'] = polya_urn_filter(G, beta=beta, weight_attr=weight_attr)
    
    return results


def batch_backbone_extraction(networks: Dict[int, nx.Graph],
                             alpha_values: List[float] = [0.01, 0.05, 0.1],
                             beta: float = 0.05,
                             weight_attr: str = 'weight') -> Dict[str, Dict[int, nx.Graph]]:
    """
    æ‰¹é‡å¯¹å¤šå¹´ç½‘ç»œæ•°æ®åº”ç”¨éª¨å¹²æå–ç®—æ³•
    
    Args:
        networks: å¹´ä»½åˆ°ç½‘ç»œçš„æ˜ å°„å­—å…¸
        alpha_values: DFç®—æ³•çš„alphaå€¼åˆ—è¡¨
        beta: PFç®—æ³•çš„betaå€¼
        weight_attr: è¾¹æƒé‡å±æ€§å
        
    Returns:
        åµŒå¥—å­—å…¸: {algorithm: {year: backbone_network}}
    """
    
    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡éª¨å¹²æå–åˆ†æ...")
    logger.info(f"   å¹´ä»½èŒƒå›´: {min(networks.keys())}-{max(networks.keys())}")
    logger.info(f"   ç®—æ³•: DF (Î±={alpha_values}), MST, PF (Î²={beta})")
    
    # åˆå§‹åŒ–ç»“æœå­—å…¸
    batch_results = {}
    algorithm_keys = [f'disparity_filter_{alpha}' for alpha in alpha_values] + ['mst', 'polya_urn']
    
    for key in algorithm_keys:
        batch_results[key] = {}
    
    # å¯¹æ¯å¹´ç½‘ç»œåº”ç”¨æ‰€æœ‰ç®—æ³•
    for year, network in networks.items():
        logger.info(f"âš¡ å¤„ç†{year}å¹´ç½‘ç»œ ({network.number_of_nodes()}èŠ‚ç‚¹, {network.number_of_edges()}è¾¹)...")
        
        try:
            year_results = apply_all_algorithms(
                network, 
                alpha_values=alpha_values, 
                beta=beta, 
                weight_attr=weight_attr
            )
            
            # å°†ç»“æœåˆ†é…åˆ°å¯¹åº”ç®—æ³•
            for alg_key, backbone_net in year_results.items():
                batch_results[alg_key][year] = backbone_net
                
        except Exception as e:
            logger.error(f"âŒ {year}å¹´æ•°æ®å¤„ç†å¤±è´¥: {e}")
            continue
    
    # è¾“å‡ºæ‰¹é‡å¤„ç†ç»Ÿè®¡
    logger.info("ğŸ“Š æ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦:")
    for alg_key, year_networks in batch_results.items():
        if year_networks:
            retention_rates = [G.graph.get('retention_rate', 0) for G in year_networks.values()]
            avg_retention = np.mean(retention_rates)
            logger.info(f"   {alg_key}: å¹³å‡ä¿ç•™ç‡ = {avg_retention:.1%}")
    
    return batch_results