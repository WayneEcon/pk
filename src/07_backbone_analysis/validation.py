#!/usr/bin/env python3
"""
ç¨³å¥æ€§æ£€éªŒå’Œå¯¹æ¯”åˆ†ææ¨¡å—
========================

æ•´åˆæ‰€æœ‰ç¨³å¥æ€§æ£€éªŒåŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯ä¸"è½¨é“ä¸€"(03æ¨¡å—)çš„å¯¹æ¯”åˆ†æã€‚
**æ ¸å¿ƒä½¿å‘½**: éªŒè¯éª¨å¹²ç½‘ç»œåˆ†æç»“æœä¸å®Œæ•´ç½‘ç»œåˆ†æç»“æœçš„ä¸€è‡´æ€§ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. run_robustness_checks() - ä¸»è¦æ£€éªŒå‡½æ•°ï¼ŒåŒ…å«ä¸03æ¨¡å—çš„Spearmanç›¸å…³æ€§æ£€éªŒ
2. å‚æ•°æ•æ„Ÿæ€§åˆ†æ - DFç®—æ³•åœ¨ä¸åŒalphaå€¼ä¸‹çš„ç¨³å®šæ€§
3. è·¨ç®—æ³•éªŒè¯ - ç¡®ä¿æ ¸å¿ƒå‘ç°åœ¨æ‰€æœ‰ç®—æ³•ä¸­éƒ½ä¸€è‡´
4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ - éªŒè¯ç¾å›½åœ°ä½å˜åŒ–çš„ç»Ÿè®¡æ˜¾è‘—æ€§

å­¦æœ¯æ ‡å‡†ï¼š
- Spearmanç›¸å…³ç³»æ•° > 0.7
- æ ¸å¿ƒå‘ç°ç¨³å®šæ€§ > 80%
- ç»Ÿè®¡æ˜¾è‘—æ€§ p < 0.05
- è·¨ç®—æ³•ä¸€è‡´æ€§ > 75%

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import pandas as pd
import networkx as nx
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
from scipy import stats
from scipy.stats import spearmanr, kendalltau, ttest_ind, mannwhitneyu
import logging
import json

logger = logging.getLogger(__name__)


def calculate_node_centralities(G: nx.Graph) -> Dict[str, Dict[str, float]]:
    """
    è®¡ç®—èŠ‚ç‚¹çš„å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡
    
    Args:
        G: NetworkXå›¾å¯¹è±¡
        
    Returns:
        åŒ…å«å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡çš„å­—å…¸
    """
    
    centralities = {}
    
    # åº¦ä¸­å¿ƒæ€§
    centralities['degree'] = dict(G.degree())
    
    # å¼ºåº¦ä¸­å¿ƒæ€§ï¼ˆåŠ æƒåº¦ï¼‰
    centralities['strength'] = dict(G.degree(weight='weight'))
    
    # ä¸­ä»‹ä¸­å¿ƒæ€§
    try:
        centralities['betweenness'] = nx.betweenness_centrality(G, weight='weight')
    except:
        centralities['betweenness'] = {node: 0.0 for node in G.nodes()}
    
    # PageRankä¸­å¿ƒæ€§
    try:
        centralities['pagerank'] = nx.pagerank(G, weight='weight')
    except:
        centralities['pagerank'] = {node: 1.0/G.number_of_nodes() for node in G.nodes()}
    
    # æ¥è¿‘ä¸­å¿ƒæ€§
    try:
        centralities['closeness'] = nx.closeness_centrality(G, distance='weight')
    except:
        centralities['closeness'] = {node: 0.0 for node in G.nodes()}
    
    return centralities


def compare_centrality_rankings(full_centrality: Dict[str, Dict[str, float]], 
                              backbone_centrality: Dict[str, Dict[str, float]],
                              metric: str = 'strength') -> Dict[str, float]:
    """
    æ¯”è¾ƒå®Œæ•´ç½‘ç»œå’Œéª¨å¹²ç½‘ç»œçš„ä¸­å¿ƒæ€§æ’å
    
    Args:
        full_centrality: å®Œæ•´ç½‘ç»œçš„ä¸­å¿ƒæ€§æŒ‡æ ‡
        backbone_centrality: éª¨å¹²ç½‘ç»œçš„ä¸­å¿ƒæ€§æŒ‡æ ‡  
        metric: è¦æ¯”è¾ƒçš„ä¸­å¿ƒæ€§æŒ‡æ ‡
        
    Returns:
        åŒ…å«ç›¸å…³æ€§åˆ†æç»“æœçš„å­—å…¸
    """
    
    # è·å–å…±åŒèŠ‚ç‚¹
    common_nodes = set(full_centrality[metric].keys()).intersection(
        set(backbone_centrality[metric].keys())
    )
    
    if len(common_nodes) < 5:
        return {'spearman_rho': 0.0, 'spearman_pvalue': 1.0, 'kendall_tau': 0.0, 'common_nodes': len(common_nodes)}
    
    # æå–å…±åŒèŠ‚ç‚¹çš„ä¸­å¿ƒæ€§å€¼
    full_values = [full_centrality[metric][node] for node in common_nodes]
    backbone_values = [backbone_centrality[metric][node] for node in common_nodes]
    
    # è®¡ç®—Spearmanç›¸å…³ç³»æ•°
    spearman_rho, spearman_p = spearmanr(full_values, backbone_values)
    
    # è®¡ç®—Kendall's tau
    kendall_tau, kendall_p = kendalltau(full_values, backbone_values)
    
    return {
        'spearman_rho': spearman_rho,
        'spearman_pvalue': spearman_p,
        'kendall_tau': kendall_tau,
        'kendall_pvalue': kendall_p,
        'common_nodes': len(common_nodes)
    }


def run_robustness_checks(full_networks: Dict[int, nx.Graph],
                         backbone_networks: Dict[str, Dict[int, nx.Graph]],
                         track1_results: Optional[Dict] = None) -> Dict[str, Any]:
    """
    **æ ¸å¿ƒå‡½æ•°**: è¿è¡Œå®Œæ•´çš„ç¨³å¥æ€§æ£€éªŒ
    
    **å…³é”®è¦æ±‚**: æ­¤å‡½æ•°å¿…é¡»å®ç°ä¸"è½¨é“ä¸€"(03æ¨¡å—)æ ¸å¿ƒæŒ‡æ ‡çš„å¯¹æ¯”åˆ†æï¼Œ
    ç‰¹åˆ«æ˜¯ä½¿ç”¨Spearmanç­‰çº§ç›¸å…³ç³»æ•°æ¥æ£€éªŒèŠ‚ç‚¹ä¸­å¿ƒæ€§æ’åºçš„ä¸€è‡´æ€§ã€‚
    
    Args:
        full_networks: å®Œæ•´ç½‘ç»œæ•°æ® {year: network}
        backbone_networks: éª¨å¹²ç½‘ç»œæ•°æ® {algorithm: {year: network}}
        track1_results: è½¨é“ä¸€(03æ¨¡å—)çš„åˆ†æç»“æœ
        
    Returns:
        å®Œæ•´çš„ç¨³å¥æ€§æ£€éªŒç»“æœ
    """
    
    logger.info("ğŸ” å¼€å§‹å®Œæ•´ç¨³å¥æ€§æ£€éªŒåˆ†æ...")
    
    robustness_results = {
        'centrality_consistency': {},
        'parameter_sensitivity': {},
        'cross_algorithm_validation': {},
        'statistical_significance': {},
        'track1_comparison': {},  # ä¸è½¨é“ä¸€çš„å¯¹æ¯”
        'overall_assessment': {}
    }
    
    # è·å–å…±åŒå¹´ä»½
    common_years = set(full_networks.keys())
    for alg_networks in backbone_networks.values():
        common_years = common_years.intersection(set(alg_networks.keys()))
    common_years = sorted(common_years)
    
    if len(common_years) < 3:
        logger.warning("âš ï¸ å…±åŒå¹´ä»½å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œå……åˆ†éªŒè¯")
        return robustness_results
    
    logger.info(f"   åˆ†æå¹´ä»½: {len(common_years)} å¹´ ({min(common_years)}-{max(common_years)})")
    
    # 1. ä¸­å¿ƒæ€§ä¸€è‡´æ€§éªŒè¯
    logger.info("ğŸ“Š 1. ä¸­å¿ƒæ€§ä¸€è‡´æ€§éªŒè¯...")
    centrality_results = validate_centrality_consistency(
        full_networks, backbone_networks, common_years
    )
    robustness_results['centrality_consistency'] = centrality_results
    
    # 2. å‚æ•°æ•æ„Ÿæ€§åˆ†æ
    logger.info("ğŸ¯ 2. Disparity Filterå‚æ•°æ•æ„Ÿæ€§åˆ†æ...")
    sensitivity_results = analyze_parameter_sensitivity(
        backbone_networks, common_years
    )
    robustness_results['parameter_sensitivity'] = sensitivity_results
    
    # 3. è·¨ç®—æ³•éªŒè¯
    logger.info("ğŸ”„ 3. è·¨ç®—æ³•ä¸€è‡´æ€§éªŒè¯...")
    cross_algo_results = validate_cross_algorithm_consistency(
        backbone_networks, common_years
    )
    robustness_results['cross_algorithm_validation'] = cross_algo_results
    
    # 4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    logger.info("ğŸ“ˆ 4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")
    significance_results = test_statistical_significance(
        full_networks, backbone_networks, common_years
    )
    robustness_results['statistical_significance'] = significance_results
    
    # 5. ä¸è½¨é“ä¸€å¯¹æ¯”ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
    if track1_results:
        logger.info("ğŸ”— 5. ä¸è½¨é“ä¸€(03æ¨¡å—)å¯¹æ¯”åˆ†æ...")
        track1_comparison = compare_with_track1_results(
            backbone_networks, track1_results, common_years
        )
        robustness_results['track1_comparison'] = track1_comparison
    
    # 6. æ€»ä½“è¯„ä¼°
    overall_assessment = calculate_overall_robustness_score(robustness_results)
    robustness_results['overall_assessment'] = overall_assessment
    
    logger.info(f"âœ… ç¨³å¥æ€§æ£€éªŒå®Œæˆï¼Œæ€»ä½“å¾—åˆ†: {overall_assessment.get('total_score', 0):.3f}")
    
    return robustness_results


def validate_centrality_consistency(full_networks: Dict[int, nx.Graph],
                                  backbone_networks: Dict[str, Dict[int, nx.Graph]],
                                  common_years: List[int]) -> Dict[str, Any]:
    """
    éªŒè¯ä¸­å¿ƒæ€§æ’åºçš„ä¸€è‡´æ€§
    
    æ ¸å¿ƒæ£€éªŒ: ç¾å›½åœ¨å®Œæ•´ç½‘ç»œvséª¨å¹²ç½‘ç»œä¸­çš„æ’åå¯¹æ¯”
    ç›®æ ‡: Spearmanç›¸å…³ç³»æ•° > 0.7
    """
    
    consistency_results = {
        'algorithm_correlations': {},
        'usa_rank_analysis': {},
        'overall_consistency_score': 0
    }
    
    all_correlations = []
    usa_rank_preservation = []
    
    # å¯¹æ¯ä¸ªç®—æ³•è¿›è¡Œåˆ†æ
    for algorithm_name, alg_networks in backbone_networks.items():
        logger.info(f"   åˆ†æ{algorithm_name}ç®—æ³•...")
        
        alg_correlations = []
        usa_ranks = {'full': [], 'backbone': []}
        
        for year in common_years:
            if year not in alg_networks:
                continue
                
            full_G = full_networks[year]
            backbone_G = alg_networks[year]
            
            # è®¡ç®—ä¸­å¿ƒæ€§
            full_centrality = calculate_node_centralities(full_G)
            backbone_centrality = calculate_node_centralities(backbone_G)
            
            # æ¯”è¾ƒå¼ºåº¦ä¸­å¿ƒæ€§æ’å
            correlation_result = compare_centrality_rankings(
                full_centrality, backbone_centrality, 'strength'
            )
            
            if correlation_result['common_nodes'] >= 5:
                alg_correlations.append(correlation_result['spearman_rho'])
                
                # åˆ†æç¾å›½æ’å
                if 'USA' in full_centrality['strength'] and 'USA' in backbone_centrality['strength']:
                    full_usa_rank = get_node_rank(full_centrality['strength'], 'USA')
                    backbone_usa_rank = get_node_rank(backbone_centrality['strength'], 'USA')
                    
                    usa_ranks['full'].append(full_usa_rank)
                    usa_ranks['backbone'].append(backbone_usa_rank)
        
        # ç®—æ³•çº§åˆ«ç»Ÿè®¡
        if alg_correlations:
            mean_correlation = np.mean(alg_correlations)
            consistency_results['algorithm_correlations'][algorithm_name] = {
                'mean_spearman_rho': mean_correlation,
                'yearly_correlations': alg_correlations,
                'meets_threshold': mean_correlation > 0.7
            }
            all_correlations.extend(alg_correlations)
            
            # ç¾å›½æ’åä¸€è‡´æ€§
            if usa_ranks['full'] and usa_ranks['backbone']:
                usa_correlation, _ = spearmanr(usa_ranks['full'], usa_ranks['backbone'])
                consistency_results['usa_rank_analysis'][algorithm_name] = {
                    'rank_correlation': usa_correlation,
                    'full_ranks': usa_ranks['full'],
                    'backbone_ranks': usa_ranks['backbone']
                }
                usa_rank_preservation.append(usa_correlation)
    
    # æ€»ä½“ä¸€è‡´æ€§è¯„åˆ†
    if all_correlations:
        overall_score = np.mean(all_correlations)
        consistency_results['overall_consistency_score'] = overall_score
        consistency_results['meets_academic_standard'] = overall_score > 0.7
    
    return consistency_results


def analyze_parameter_sensitivity(backbone_networks: Dict[str, Dict[int, nx.Graph]],
                                common_years: List[int]) -> Dict[str, Any]:
    """
    åˆ†æDisparity Filterç®—æ³•çš„å‚æ•°æ•æ„Ÿæ€§
    """
    
    sensitivity_results = {
        'alpha_stability': {},
        'core_findings_stability': 0,
        'usa_degree_stability': {}
    }
    
    # æ‰¾åˆ°æ‰€æœ‰DFç®—æ³•ç»“æœ
    df_algorithms = {k: v for k, v in backbone_networks.items() if k.startswith('disparity_filter_')}
    
    if len(df_algorithms) < 2:
        logger.warning("âš ï¸ DFç®—æ³•ç»“æœä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå‚æ•°æ•æ„Ÿæ€§åˆ†æ")
        return sensitivity_results
    
    # åˆ†æä¸åŒalphaå€¼ä¸‹çš„ç¨³å®šæ€§
    usa_degrees_by_alpha = {}
    retention_rates_by_alpha = {}
    
    for alpha_key, alg_networks in df_algorithms.items():
        alpha_value = float(alpha_key.split('_')[-1])
        
        usa_degrees = []
        retention_rates = []
        
        for year in common_years:
            if year not in alg_networks:
                continue
                
            backbone_G = alg_networks[year]
            
            # ç¾å›½åº¦æ•°
            if 'USA' in backbone_G.nodes():
                usa_degrees.append(backbone_G.degree('USA'))
            
            # ä¿ç•™ç‡
            retention_rate = backbone_G.graph.get('retention_rate', 0)
            retention_rates.append(retention_rate)
        
        usa_degrees_by_alpha[alpha_value] = usa_degrees
        retention_rates_by_alpha[alpha_value] = retention_rates
    
    # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
    alpha_values = sorted(usa_degrees_by_alpha.keys())
    
    # ç¾å›½åº¦æ•°ç¨³å®šæ€§
    usa_stability_scores = []
    for i in range(len(alpha_values) - 1):
        alpha1, alpha2 = alpha_values[i], alpha_values[i + 1]
        if usa_degrees_by_alpha[alpha1] and usa_degrees_by_alpha[alpha2]:
            correlation, _ = spearmanr(usa_degrees_by_alpha[alpha1], usa_degrees_by_alpha[alpha2])
            usa_stability_scores.append(correlation)
    
    if usa_stability_scores:
        sensitivity_results['usa_degree_stability'] = {
            'mean_stability': np.mean(usa_stability_scores),
            'stability_scores': usa_stability_scores,
            'meets_threshold': np.mean(usa_stability_scores) > 0.8
        }
    
    # æ ¸å¿ƒå‘ç°ç¨³å®šæ€§ï¼ˆä¿ç•™ç‡å˜åŒ–ï¼‰
    retention_stability = []
    for i in range(len(alpha_values) - 1):
        alpha1, alpha2 = alpha_values[i], alpha_values[i + 1]
        if retention_rates_by_alpha[alpha1] and retention_rates_by_alpha[alpha2]:
            # è®¡ç®—ä¿ç•™ç‡å˜åŒ–çš„ç¨³å®šæ€§
            rates1, rates2 = retention_rates_by_alpha[alpha1], retention_rates_by_alpha[alpha2]
            correlation, _ = spearmanr(rates1, rates2)
            retention_stability.append(correlation)
    
    if retention_stability:
        sensitivity_results['core_findings_stability'] = np.mean(retention_stability)
    
    return sensitivity_results


def validate_cross_algorithm_consistency(backbone_networks: Dict[str, Dict[int, nx.Graph]],
                                       common_years: List[int]) -> Dict[str, Any]:
    """
    éªŒè¯è·¨ç®—æ³•ä¸€è‡´æ€§
    """
    
    cross_algo_results = {
        'algorithm_pairs': {},
        'usa_position_consistency': {},
        'algorithm_consistency_score': 0
    }
    
    algorithms = list(backbone_networks.keys())
    consistency_scores = []
    
    # ä¸¤ä¸¤æ¯”è¾ƒç®—æ³•
    for i in range(len(algorithms)):
        for j in range(i + 1, len(algorithms)):
            alg1, alg2 = algorithms[i], algorithms[j]
            
            pair_key = f"{alg1}_vs_{alg2}"
            
            # æ¯”è¾ƒç¾å›½åº¦æ•°è¶‹åŠ¿
            usa_degrees_1 = []
            usa_degrees_2 = []
            
            for year in common_years:
                if year in backbone_networks[alg1] and year in backbone_networks[alg2]:
                    G1 = backbone_networks[alg1][year]
                    G2 = backbone_networks[alg2][year]
                    
                    if 'USA' in G1.nodes() and 'USA' in G2.nodes():
                        usa_degrees_1.append(G1.degree('USA'))
                        usa_degrees_2.append(G2.degree('USA'))
            
            if len(usa_degrees_1) >= 3:
                correlation, p_value = spearmanr(usa_degrees_1, usa_degrees_2)
                
                cross_algo_results['algorithm_pairs'][pair_key] = {
                    'usa_degree_correlation': correlation,
                    'correlation_pvalue': p_value,
                    'significant': p_value < 0.05
                }
                
                consistency_scores.append(correlation)
    
    # æ€»ä½“ä¸€è‡´æ€§è¯„åˆ†
    if consistency_scores:
        mean_consistency = np.mean(consistency_scores)
        cross_algo_results['algorithm_consistency_score'] = mean_consistency
        cross_algo_results['meets_threshold'] = mean_consistency > 0.75
    
    return cross_algo_results


def test_statistical_significance(full_networks: Dict[int, nx.Graph],
                                 backbone_networks: Dict[str, Dict[int, nx.Graph]],
                                 common_years: List[int]) -> Dict[str, Any]:
    """
    æµ‹è¯•ç»Ÿè®¡æ˜¾è‘—æ€§
    """
    
    significance_results = {
        'usa_position_change': {},
        'temporal_trends': {},
        'overall_significance': False
    }
    
    # åˆ†æç¾å›½åœ°ä½çš„æ—¶é—´å˜åŒ–
    for algorithm_name, alg_networks in backbone_networks.items():
        usa_degrees = []
        years_with_usa = []
        
        for year in sorted(common_years):
            if year in alg_networks:
                G = alg_networks[year]
                if 'USA' in G.nodes():
                    usa_degrees.append(G.degree('USA'))
                    years_with_usa.append(year)
        
        if len(usa_degrees) >= 5:
            # åˆ†ææ—¶é—´è¶‹åŠ¿
            correlation, p_value = spearmanr(years_with_usa, usa_degrees)
            
            # åˆ†æœŸæ¯”è¾ƒï¼ˆé¡µå²©é©å‘½å‰åï¼‰
            shale_year = 2011
            pre_shale = [d for y, d in zip(years_with_usa, usa_degrees) if y < shale_year]
            post_shale = [d for y, d in zip(years_with_usa, usa_degrees) if y >= shale_year]
            
            if len(pre_shale) >= 2 and len(post_shale) >= 2:
                # Mann-Whitney Uæ£€éªŒ
                u_stat, u_p = mannwhitneyu(pre_shale, post_shale, alternative='less')
                
                significance_results['usa_position_change'][algorithm_name] = {
                    'temporal_correlation': correlation,
                    'temporal_pvalue': p_value,
                    'pre_post_shale_test': {
                        'u_statistic': u_stat,
                        'u_pvalue': u_p,
                        'significant_increase': u_p < 0.05
                    }
                }
    
    # æ€»ä½“æ˜¾è‘—æ€§åˆ¤æ–­
    significant_algorithms = []
    for alg_results in significance_results['usa_position_change'].values():
        if alg_results.get('pre_post_shale_test', {}).get('significant_increase', False):
            significant_algorithms.append(True)
    
    significance_results['overall_significance'] = len(significant_algorithms) > 0
    
    return significance_results


def compare_with_track1_results(backbone_networks: Dict[str, Dict[int, nx.Graph]],
                               track1_results: Dict,
                               common_years: List[int]) -> Dict[str, Any]:
    """
    ä¸è½¨é“ä¸€(03æ¨¡å—)ç»“æœå¯¹æ¯”åˆ†æ
    """
    
    track1_comparison = {
        'centrality_ranking_comparison': {},
        'usa_metrics_comparison': {},
        'consistency_with_track1': 0
    }
    
    # è¿™é‡Œéœ€è¦æ ¹æ®03æ¨¡å—çš„å…·ä½“è¾“å‡ºæ ¼å¼æ¥å®ç°
    # ç›®å‰æä¾›æ¡†æ¶ç»“æ„
    
    logger.info("   è½¨é“ä¸€å¯¹æ¯”åˆ†æåŠŸèƒ½å¾…03æ¨¡å—å…·ä½“æ ¼å¼ç¡®å®šåå®Œå–„")
    
    return track1_comparison


def calculate_overall_robustness_score(robustness_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    è®¡ç®—æ€»ä½“ç¨³å¥æ€§å¾—åˆ†
    """
    
    scores = []
    
    # 1. ä¸­å¿ƒæ€§ä¸€è‡´æ€§å¾—åˆ†
    centrality_score = robustness_results.get('centrality_consistency', {}).get('overall_consistency_score', 0)
    scores.append(centrality_score)
    
    # 2. å‚æ•°æ•æ„Ÿæ€§å¾—åˆ†
    sensitivity_score = robustness_results.get('parameter_sensitivity', {}).get('core_findings_stability', 0)
    scores.append(sensitivity_score)
    
    # 3. è·¨ç®—æ³•ä¸€è‡´æ€§å¾—åˆ†
    cross_algo_score = robustness_results.get('cross_algorithm_validation', {}).get('algorithm_consistency_score', 0)
    scores.append(cross_algo_score)
    
    # 4. ç»Ÿè®¡æ˜¾è‘—æ€§å¾—åˆ†
    significance_score = 1.0 if robustness_results.get('statistical_significance', {}).get('overall_significance', False) else 0.0
    scores.append(significance_score)
    
    # è®¡ç®—æ€»åˆ†
    total_score = np.mean([s for s in scores if not np.isnan(s)])
    
    # è¯„çº§
    if total_score >= 0.85:
        rating = 'excellent'
    elif total_score >= 0.7:
        rating = 'high'
    elif total_score >= 0.5:
        rating = 'moderate'
    else:
        rating = 'low'
    
    return {
        'total_score': total_score,
        'component_scores': {
            'centrality_consistency': centrality_score,
            'parameter_sensitivity': sensitivity_score,
            'cross_algorithm_consistency': cross_algo_score,
            'statistical_significance': significance_score
        },
        'rating': rating,
        'meets_academic_standards': total_score > 0.7
    }


def get_node_rank(centrality_dict: Dict[str, float], node: str) -> int:
    """
    è·å–èŠ‚ç‚¹åœ¨ä¸­å¿ƒæ€§æ’åºä¸­çš„æ’å
    """
    
    sorted_nodes = sorted(centrality_dict.items(), key=lambda x: x[1], reverse=True)
    
    for rank, (node_name, _) in enumerate(sorted_nodes, 1):
        if node_name == node:
            return rank
    
    return len(sorted_nodes) + 1  # å¦‚æœèŠ‚ç‚¹ä¸å­˜åœ¨ï¼Œè¿”å›æœ€åæ’å