#!/usr/bin/env python3
"""
éª¨å¹²ç½‘ç»œåˆ†æä¸»ç¨‹åº
================

æ¸…æ™°çš„é€»è¾‘ä¸²è”è°ƒç”¨æ‰€æœ‰æ¨¡å—åŠŸèƒ½ï¼Œå®ç°"é»„é‡‘ä¸­é—´ç‚¹"çš„è®¾è®¡ç›®æ ‡ã€‚
æ”¯æŒçµæ´»çš„å¹´ä»½å‚æ•°æ§åˆ¶ï¼Œå…¼é¡¾"å…¨é¢è®¡ç®—"å’Œ"é‡ç‚¹å¯è§†åŒ–"ã€‚

æ ¸å¿ƒæµç¨‹ï¼š
1. æ•°æ®åŠ è½½
2. éª¨å¹²ç½‘ç»œæå–ï¼ˆå…¨é¢åˆ†æï¼‰
3. ç¨³å¥æ€§æ£€éªŒï¼ˆä¸è½¨é“ä¸€å¯¹æ¯”ï¼‰
4. å¯è§†åŒ–ç”Ÿæˆï¼ˆé‡ç‚¹å¹´ä»½ï¼‰
5. æŠ¥å‘Šç”Ÿæˆ

ä½¿ç”¨æ–¹æ³•ï¼š
    python main.py                          # é»˜è®¤åˆ†æ
    python main.py --config config.json    # ä½¿ç”¨é…ç½®æ–‡ä»¶
    python main.py --quick                  # å¿«é€Ÿæ¨¡å¼
    python main.py --full                   # å®Œæ•´åˆ†æ
    python main.py --years 2018,2020       # æŒ‡å®šå¹´ä»½

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import argparse
import sys
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import json
from datetime import datetime
import traceback

# å¯¼å…¥æ‰€æœ‰æ•´åˆæ¨¡å—
from config import (
    AnalysisConfig, 
    get_quick_demo_config, 
    get_full_analysis_config,
    get_validation_focused_config
)
from algorithms import batch_backbone_extraction
from validation import run_robustness_checks
from reporting import create_backbone_visualizations, generate_summary_report

# æ•°æ®åŠ è½½å·¥å…·
import networkx as nx
import numpy as np


def setup_logging(config: AnalysisConfig):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    
    log_level = getattr(logging, config.log_level.upper())
    
    handlers = [logging.StreamHandler()]
    
    if config.log_to_file:
        log_file = config.output_path / f"backbone_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers,
        force=True
    )
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ éª¨å¹²ç½‘ç»œåˆ†æç³»ç»Ÿå¯åŠ¨")
    logger.info(f"   é…ç½®: {len(config.analysis_years)}å¹´åˆ†æ, {len(config.algorithms)}ç§ç®—æ³•")
    
    return logger


def load_network_data(config: AnalysisConfig, logger: logging.Logger) -> Dict[int, nx.Graph]:
    """
    åŠ è½½ç½‘ç»œæ•°æ®
    
    Args:
        config: åˆ†æé…ç½®
        logger: æ—¥å¿—å™¨
        
    Returns:
        å¹´ä»½åˆ°ç½‘ç»œçš„æ˜ å°„å­—å…¸
    """
    
    logger.info("ğŸ“‚ å¼€å§‹åŠ è½½ç½‘ç»œæ•°æ®...")
    
    networks = {}
    loaded_years = []
    
    for year in config.analysis_years:
        # å°è¯•ä»å¤šä¸ªå¯èƒ½ä½ç½®åŠ è½½
        potential_paths = [
            config.data_path / "networks" / f"network_{year}.graphml",
            config.data_path / f"network_{year}.graphml", 
            config.data_path / f"{year}.graphml",
            Path("../02_net_analysis/outputs/networks") / f"network_{year}.graphml"
        ]
        
        loaded = False
        for path in potential_paths:
            if path.exists():
                try:
                    G = nx.read_graphml(path)
                    if G.number_of_nodes() > 0:
                        networks[year] = G
                        loaded_years.append(year)
                        logger.info(f"   âœ… {year}: {G.number_of_nodes()}èŠ‚ç‚¹, {G.number_of_edges()}è¾¹")
                        loaded = True
                        break
                except Exception as e:
                    logger.warning(f"   âš ï¸ åŠ è½½{path}å¤±è´¥: {e}")
                    continue
        
        if not loaded:
            logger.warning(f"   âš ï¸ {year}å¹´æ•°æ®æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    
    # å¦‚æœæ²¡æœ‰åŠ è½½åˆ°çœŸå®æ•°æ®ï¼Œåˆ›å»ºæ¼”ç¤ºæ•°æ®
    if not networks:
        logger.info("   åˆ›å»ºæ¼”ç¤ºæ•°æ®...")
        networks = create_demo_networks(config.analysis_years, logger)
    
    logger.info(f"âœ… ç½‘ç»œæ•°æ®åŠ è½½å®Œæˆ: {len(networks)}å¹´")
    
    return networks


def create_demo_networks(years: List[int], logger: logging.Logger) -> Dict[int, nx.Graph]:
    """åˆ›å»ºæ¼”ç¤ºç½‘ç»œæ•°æ®"""
    
    logger.info("   ç”Ÿæˆæ¼”ç¤ºç½‘ç»œæ•°æ®...")
    
    # ä¸»è¦èƒ½æºè´¸æ˜“å›½å®¶
    countries = [
        'USA', 'CAN', 'MEX', 'GBR', 'DEU', 'FRA', 'ITA', 'ESP', 'NLD', 'NOR',
        'CHN', 'JPN', 'KOR', 'IND', 'SGP', 'AUS', 'SAU', 'ARE', 'QAT', 'KWT',
        'RUS', 'BRA', 'VEN', 'COL', 'ARG', 'NGA', 'AGO', 'LBY', 'DZA'
    ]
    
    # åœ°åŒºæ˜ å°„
    region_map = {
        'North America': ['USA', 'CAN', 'MEX'],
        'Europe': ['GBR', 'DEU', 'FRA', 'ITA', 'ESP', 'NLD', 'NOR', 'RUS'],
        'Asia': ['CHN', 'JPN', 'KOR', 'IND', 'SGP'],
        'Middle East': ['SAU', 'ARE', 'QAT', 'KWT'],
        'Latin America': ['BRA', 'VEN', 'COL', 'ARG'],
        'Africa': ['NGA', 'AGO', 'LBY', 'DZA'],
        'Oceania': ['AUS']
    }
    
    def same_region(c1, c2):
        for region, region_countries in region_map.items():
            if c1 in region_countries and c2 in region_countries:
                return True
        return False
    
    networks = {}
    
    for year in years:
        G = nx.Graph()
        G.add_nodes_from(countries)
        
        np.random.seed(42 + year)  # ç¡®ä¿å¯é‡ç°ä½†æœ‰å¹´ä»½å˜åŒ–
        
        for i, c1 in enumerate(countries):
            for c2 in countries[i+1:]:
                # è´¸æ˜“æ¦‚ç‡
                prob = 0.15
                
                # ç¾å›½ç›¸å…³è´¸æ˜“æ›´é¢‘ç¹
                if 'USA' in [c1, c2]:
                    prob *= 2.5
                
                # åœ°åŒºå†…è´¸æ˜“æ›´é¢‘ç¹
                if same_region(c1, c2):
                    prob *= 1.8
                
                if np.random.random() < prob:
                    # è´¸æ˜“é‡ï¼ˆç¾å›½åœ¨2011å¹´åå¢é•¿æ›´å¿«ï¼‰
                    base_weight = np.random.exponential(50) * 1e6
                    
                    # é¡µå²©é©å‘½æ•ˆåº”
                    if 'USA' in [c1, c2] and year >= 2011:
                        growth_factor = 1.0 + (year - 2011) * 0.15
                        base_weight *= growth_factor
                    
                    G.add_edge(c1, c2, weight=base_weight)
        
        networks[year] = G
        logger.info(f"     {year}: {G.number_of_nodes()}èŠ‚ç‚¹, {G.number_of_edges()}è¾¹")
    
    return networks


def load_track1_results(config: AnalysisConfig, logger: logging.Logger) -> Optional[Dict]:
    """
    åŠ è½½è½¨é“ä¸€(03æ¨¡å—)çš„åˆ†æç»“æœ
    
    Args:
        config: åˆ†æé…ç½®
        logger: æ—¥å¿—å™¨
        
    Returns:
        è½¨é“ä¸€ç»“æœå­—å…¸ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    
    logger.info("ğŸ”— å°è¯•åŠ è½½è½¨é“ä¸€(03æ¨¡å—)åˆ†æç»“æœ...")
    
    # å°è¯•ä»å¤šä¸ªå¯èƒ½ä½ç½®åŠ è½½03æ¨¡å—ç»“æœ
    potential_paths = [
        Path("../03_metrics/all_metrics.csv"),
        Path("../03_metrics/node_centrality_metrics.csv"),
        config.data_path / "track1_results.json",
        config.output_path / "track1_results.json"
    ]
    
    for path in potential_paths:
        if path.exists():
            try:
                if path.suffix == '.csv':
                    import pandas as pd
                    df = pd.read_csv(path)
                    logger.info(f"   âœ… æ‰¾åˆ°03æ¨¡å—ç»“æœ: {path}")
                    # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                    return {'centrality_data': df.to_dict('records')}
                elif path.suffix == '.json':
                    with open(path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    logger.info(f"   âœ… æ‰¾åˆ°è½¨é“ä¸€ç»“æœ: {path}")
                    return data
            except Exception as e:
                logger.warning(f"   âš ï¸ åŠ è½½{path}å¤±è´¥: {e}")
                continue
    
    logger.info("   â„¹ï¸ æœªæ‰¾åˆ°è½¨é“ä¸€ç»“æœï¼Œå°†è·³è¿‡å¯¹æ¯”åˆ†æ")
    return None


def run_backbone_analysis(networks: Dict[int, nx.Graph], 
                         config: AnalysisConfig, 
                         logger: logging.Logger) -> Dict[str, Dict[int, nx.Graph]]:
    """
    è¿è¡Œéª¨å¹²ç½‘ç»œæå–åˆ†æ
    
    Args:
        networks: åŸå§‹ç½‘ç»œæ•°æ®
        config: åˆ†æé…ç½®
        logger: æ—¥å¿—å™¨
        
    Returns:
        éª¨å¹²ç½‘ç»œç»“æœ
    """
    
    logger.info("ğŸ”— å¼€å§‹éª¨å¹²ç½‘ç»œæå–åˆ†æ...")
    
    try:
        backbone_results = batch_backbone_extraction(
            networks=networks,
            alpha_values=config.alpha_values,
            beta=config.beta_value,
            weight_attr=config.weight_attr
        )
        
        # ç»Ÿè®¡ç»“æœ
        total_backbones = sum(len(yearly_data) for yearly_data in backbone_results.values())
        logger.info(f"âœ… éª¨å¹²ç½‘ç»œæå–å®Œæˆ: {total_backbones}ä¸ªéª¨å¹²ç½‘ç»œ")
        
        # ä¿å­˜éª¨å¹²ç½‘ç»œï¼ˆå¦‚æœéœ€è¦ï¼‰
        if config.save_networks:
            save_backbone_networks(backbone_results, config, logger)
        
        return backbone_results
        
    except Exception as e:
        logger.error(f"âŒ éª¨å¹²ç½‘ç»œæå–å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        raise


def save_backbone_networks(backbone_results: Dict[str, Dict[int, nx.Graph]], 
                          config: AnalysisConfig, 
                          logger: logging.Logger):
    """ä¿å­˜éª¨å¹²ç½‘ç»œåˆ°æ–‡ä»¶"""
    
    logger.info("ğŸ’¾ ä¿å­˜éª¨å¹²ç½‘ç»œ...")
    
    networks_dir = config.output_path / "networks"
    networks_dir.mkdir(exist_ok=True)
    
    saved_count = 0
    for algorithm, yearly_networks in backbone_results.items():
        alg_dir = networks_dir / algorithm
        alg_dir.mkdir(exist_ok=True)
        
        for year, network in yearly_networks.items():
            file_path = alg_dir / f"backbone_{algorithm}_{year}.graphml"
            try:
                nx.write_graphml(network, file_path)
                saved_count += 1
            except Exception as e:
                logger.warning(f"   âš ï¸ ä¿å­˜{file_path}å¤±è´¥: {e}")
    
    logger.info(f"   âœ… å·²ä¿å­˜ {saved_count} ä¸ªéª¨å¹²ç½‘ç»œæ–‡ä»¶")


def run_analysis_pipeline(config: AnalysisConfig) -> Dict[str, Any]:
    """
    è¿è¡Œå®Œæ•´çš„åˆ†ææµæ°´çº¿
    
    Args:
        config: åˆ†æé…ç½®
        
    Returns:
        å®Œæ•´åˆ†æç»“æœ
    """
    
    # è®¾ç½®æ—¥å¿—å’Œåˆ›å»ºè¾“å‡ºç›®å½•
    config.create_output_directories()
    logger = setup_logging(config)
    
    analysis_results = {
        'config': config.__dict__,
        'execution_time': {},
        'data_summary': {},
        'backbone_results': {},
        'validation_results': {},
        'visualization_paths': {},
        'report_path': '',
        'status': 'running',
        'start_time': datetime.now().isoformat()
    }
    
    try:
        # 1. æ•°æ®åŠ è½½
        start_time = datetime.now()
        logger.info("=" * 60)
        logger.info("Phase 1: æ•°æ®åŠ è½½")
        logger.info("=" * 60)
        
        networks = load_network_data(config, logger)
        track1_results = load_track1_results(config, logger)
        
        analysis_results['execution_time']['data_loading'] = (datetime.now() - start_time).total_seconds()
        analysis_results['data_summary'] = {
            'years_loaded': sorted(networks.keys()),
            'total_years': len(networks),
            'track1_available': track1_results is not None
        }
        
        # 2. éª¨å¹²ç½‘ç»œæå–
        start_time = datetime.now()
        logger.info("=" * 60)
        logger.info("Phase 2: éª¨å¹²ç½‘ç»œæå–")
        logger.info("=" * 60)
        
        backbone_networks = run_backbone_analysis(networks, config, logger)
        
        analysis_results['execution_time']['backbone_extraction'] = (datetime.now() - start_time).total_seconds()
        analysis_results['backbone_results'] = {
            'algorithms_applied': list(backbone_networks.keys()),
            'total_backbone_networks': sum(len(yearly) for yearly in backbone_networks.values())
        }
        
        # 3. ç¨³å¥æ€§æ£€éªŒ
        if config.run_validation:
            start_time = datetime.now()
            logger.info("=" * 60)
            logger.info("Phase 3: ç¨³å¥æ€§æ£€éªŒ")
            logger.info("=" * 60)
            
            validation_results = run_robustness_checks(
                full_networks=networks,
                backbone_networks=backbone_networks,
                track1_results=track1_results
            )
            
            analysis_results['execution_time']['validation'] = (datetime.now() - start_time).total_seconds()
            analysis_results['validation_results'] = validation_results
        else:
            logger.info("â­ï¸ è·³è¿‡ç¨³å¥æ€§æ£€éªŒï¼ˆé…ç½®ç¦ç”¨ï¼‰")
            analysis_results['validation_results'] = {}
        
        # 4. å¯è§†åŒ–ç”Ÿæˆ
        if config.create_visualizations:
            start_time = datetime.now()
            logger.info("=" * 60)
            logger.info("Phase 4: å¯è§†åŒ–ç”Ÿæˆ")
            logger.info("=" * 60)
            
            visualization_paths = create_backbone_visualizations(
                full_networks=networks,
                backbone_networks=backbone_networks,
                node_attributes=None,  # å°†æ¥å¯ä»¥ä»å…¶ä»–æ¨¡å—åŠ è½½
                output_dir=config.figures_path,
                visualization_years=config.visualization_years
            )
            
            analysis_results['execution_time']['visualization'] = (datetime.now() - start_time).total_seconds()
            analysis_results['visualization_paths'] = visualization_paths
        else:
            logger.info("â­ï¸ è·³è¿‡å¯è§†åŒ–ç”Ÿæˆï¼ˆé…ç½®ç¦ç”¨ï¼‰")
            analysis_results['visualization_paths'] = {}
        
        # 5. æŠ¥å‘Šç”Ÿæˆ
        if config.generate_reports:
            start_time = datetime.now()
            logger.info("=" * 60)
            logger.info("Phase 5: æŠ¥å‘Šç”Ÿæˆ")
            logger.info("=" * 60)
            
            report_path = generate_summary_report(
                full_networks=networks,
                backbone_networks=backbone_networks,
                robustness_results=analysis_results.get('validation_results', {}),
                visualization_paths=analysis_results.get('visualization_paths', {}),
                output_dir=config.output_path
            )
            
            analysis_results['execution_time']['reporting'] = (datetime.now() - start_time).total_seconds()
            analysis_results['report_path'] = report_path
        else:
            logger.info("â­ï¸ è·³è¿‡æŠ¥å‘Šç”Ÿæˆï¼ˆé…ç½®ç¦ç”¨ï¼‰")
            analysis_results['report_path'] = ''
        
        # åˆ†æå®Œæˆ
        analysis_results['status'] = 'completed'
        analysis_results['end_time'] = datetime.now().isoformat()
        analysis_results['total_time'] = sum(analysis_results['execution_time'].values())
        
        logger.info("=" * 60)
        logger.info("åˆ†æå®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"âœ… æ€»æ‰§è¡Œæ—¶é—´: {analysis_results['total_time']:.1f} ç§’")
        
        # ä¿å­˜åˆ†ææ‘˜è¦
        save_analysis_summary(analysis_results, config, logger)
        
    except Exception as e:
        logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        analysis_results['status'] = 'failed'
        analysis_results['error'] = str(e)
        analysis_results['end_time'] = datetime.now().isoformat()
    
    return analysis_results


def save_analysis_summary(results: Dict[str, Any], 
                         config: AnalysisConfig, 
                         logger: logging.Logger):
    """ä¿å­˜åˆ†ææ‘˜è¦"""
    
    summary_path = config.output_path / "analysis_summary.json"
    
    try:
        # å¤„ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        serializable_results = {}
        for key, value in results.items():
            if key == 'config':
                # è½¬æ¢Pathå¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                config_dict = {}
                for k, v in value.items():
                    if isinstance(v, Path):
                        config_dict[k] = str(v)
                    else:
                        config_dict[k] = v
                serializable_results[key] = config_dict
            else:
                serializable_results[key] = value
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“„ åˆ†ææ‘˜è¦å·²ä¿å­˜: {summary_path}")
        
    except Exception as e:
        logger.error(f"âŒ æ‘˜è¦ä¿å­˜å¤±è´¥: {e}")


def print_results_summary(results: Dict[str, Any]):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    
    print("\n" + "=" * 70)
    print("                    éª¨å¹²ç½‘ç»œåˆ†æç»“æœæ‘˜è¦")
    print("=" * 70)
    
    status = results.get('status', 'unknown')
    print(f"çŠ¶æ€: {'âœ… æˆåŠŸå®Œæˆ' if status == 'completed' else 'âŒ æ‰§è¡Œå¤±è´¥'}")
    
    if status == 'completed':
        total_time = results.get('total_time', 0)
        print(f"æ€»æ—¶é—´: {total_time:.1f} ç§’")
        
        # æ•°æ®æ‘˜è¦
        data_summary = results.get('data_summary', {})
        years = data_summary.get('years_loaded', [])
        if years:
            print(f"åˆ†æå¹´ä»½: {min(years)}-{max(years)} ({len(years)}å¹´)")
        
        # éª¨å¹²ç½‘ç»œç»“æœ
        backbone_summary = results.get('backbone_results', {})
        algorithms = backbone_summary.get('algorithms_applied', [])
        total_backbones = backbone_summary.get('total_backbone_networks', 0)
        print(f"ç®—æ³•: {', '.join(algorithms)}")
        print(f"éª¨å¹²ç½‘ç»œ: {total_backbones}ä¸ª")
        
        # éªŒè¯ç»“æœ
        validation_summary = results.get('validation_results', {})
        if validation_summary:
            overall_assessment = validation_summary.get('overall_assessment', {})
            score = overall_assessment.get('total_score', 0)
            rating = overall_assessment.get('rating', 'unknown')
            print(f"ç¨³å¥æ€§: {score:.3f} ({rating.upper()})")
        
        # å¯è§†åŒ–ç»“æœ
        viz_paths = results.get('visualization_paths', {})
        total_viz = sum(len(paths) for paths in viz_paths.values())
        if total_viz > 0:
            print(f"å¯è§†åŒ–: {total_viz}ä¸ªå›¾è¡¨")
        
        # æŠ¥å‘Šè·¯å¾„
        report_path = results.get('report_path', '')
        if report_path:
            print(f"æŠ¥å‘Š: {Path(report_path).name}")
    
    elif status == 'failed':
        error = results.get('error', 'æœªçŸ¥é”™è¯¯')
        print(f"é”™è¯¯: {error}")
    
    print("=" * 70)


def main():
    """ä¸»å‡½æ•°"""
    
    parser = argparse.ArgumentParser(
        description="éª¨å¹²ç½‘ç»œåˆ†æç³»ç»Ÿ v3.0 - é»„é‡‘ä¸­é—´ç‚¹ç‰ˆæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py                           # é»˜è®¤åˆ†æ
  python main.py --config config.json     # ä½¿ç”¨é…ç½®æ–‡ä»¶
  python main.py --quick                   # å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼
  python main.py --full                    # å®Œæ•´åˆ†ææ¨¡å¼
  python main.py --validation              # éªŒè¯é‡ç‚¹æ¨¡å¼
  python main.py --years 2018,2020        # æŒ‡å®šåˆ†æå¹´ä»½
  python main.py --viz-years 2018,2020    # æŒ‡å®šå¯è§†åŒ–å¹´ä»½
        """
    )
    
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼')
    parser.add_argument('--full', action='store_true', help='å®Œæ•´åˆ†ææ¨¡å¼')
    parser.add_argument('--validation', action='store_true', help='éªŒè¯é‡ç‚¹æ¨¡å¼')
    parser.add_argument('--years', type=str, help='åˆ†æå¹´ä»½ï¼Œé€—å·åˆ†éš” (å¦‚: 2018,2020)')
    parser.add_argument('--viz-years', type=str, help='å¯è§†åŒ–å¹´ä»½ï¼Œé€—å·åˆ†éš”')
    parser.add_argument('--output', type=str, default='./outputs', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--data-path', type=str, help='æ•°æ®è·¯å¾„')
    
    args = parser.parse_args()
    
    try:
        # ç¡®å®šé…ç½®
        if args.config and Path(args.config).exists():
            config = AnalysisConfig.load_config(Path(args.config))
        elif args.quick:
            config = get_quick_demo_config()
        elif args.full:
            config = get_full_analysis_config()
        elif args.validation:
            config = get_validation_focused_config()
        else:
            config = AnalysisConfig()  # é»˜è®¤é…ç½®
        
        # è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
        if args.years:
            try:
                years = [int(y.strip()) for y in args.years.split(',')]
                config.analysis_years = years
            except ValueError:
                print("âŒ å¹´ä»½æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨é€—å·åˆ†éš”çš„æ•°å­—ï¼Œå¦‚: 2018,2020")
                return 1
        
        if args.viz_years:
            try:
                viz_years = [int(y.strip()) for y in args.viz_years.split(',')]
                config.visualization_years = viz_years
            except ValueError:
                print("âŒ å¯è§†åŒ–å¹´ä»½æ ¼å¼é”™è¯¯")
                return 1
        
        if args.output:
            config.output_path = Path(args.output)
            config.figures_path = config.output_path / "figures"
        
        if args.data_path:
            config.data_path = Path(args.data_path)
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        print("ğŸš€ éª¨å¹²ç½‘ç»œåˆ†æç³»ç»Ÿ v3.0")
        print(f"   åˆ†æå¹´ä»½: {len(config.analysis_years)}å¹´ ({min(config.analysis_years)}-{max(config.analysis_years)})")
        print(f"   å¯è§†åŒ–å¹´ä»½: {config.visualization_years}")
        print(f"   ç®—æ³•: {', '.join(config.algorithms)}")
        print(f"   è¾“å‡ºç›®å½•: {config.output_path}")
        
        # è¿è¡Œåˆ†æ
        results = run_analysis_pipeline(config)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print_results_summary(results)
        
        return 0 if results['status'] == 'completed' else 1
        
    except KeyboardInterrupt:
        print("âš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 1
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)