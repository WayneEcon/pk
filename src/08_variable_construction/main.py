#!/usr/bin/env python3
"""
ä¸»æ„å»ºå™¨ - å›å½’åˆå¿ƒç‰ˆ
===============================

æ ¸å¿ƒç›®æ ‡:
æ ¹æ®research_outline.mdï¼ˆé‡æ„ç‰ˆï¼‰ï¼Œæ„å»ºä»¥ä¸‹4ä¸ªæ ¸å¿ƒå˜é‡ï¼š
1. Node-DLI_US: ç¾å›½é”šå®šåŠ¨æ€é”å®šæŒ‡æ•° (æ¥è‡ª04_dli_analysis)
2. HHI_imports: è¿›å£æ¥æºå¤šæ ·åŒ–æŒ‡æ•° (æ›¿ä»£vul_usï¼Œé¿å…æ„é€ å†…ç”Ÿæ€§)
3. OVI: å¤©ç„¶æ°”ç‰©ç†å†—ä½™æŒ‡æ•° (LNG+ç®¡é“/æ¶ˆè´¹)
4. US_ProdShock: ç¾å›½äº§é‡å†²å‡» - AR(2)æ®‹å·®æ–¹æ³• (é¡µå²©é©å‘½å¤–ç”Ÿå†²å‡»)

è¾“å‡º:
- analytical_panel.csv: å›½åˆ«-å¹´åº¦é¢æ¿æ•°æ® (500è¡ŒÃ—æ ¸å¿ƒå˜é‡)
- data_dictionary.md: æ•°æ®å­—å…¸

ä½œè€…: Energy Network Analysis Team  
ç‰ˆæœ¬: v3.0 - å›å½’åˆå¿ƒç‰ˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
import warnings
from typing import Dict, List, Tuple, Optional, Any
import sys
import os
import requests

# å¯¼å…¥ç®€åŒ–çš„å¤©ç„¶æ°”OVIæ„å»ºå™¨
from simple_gas_ovi_builder import SimpleGasOVIBuilder

# å¯¼å…¥æ–°çš„USäº§é‡å†²å‡»æ„å»ºå™¨ (AR(2)æ®‹å·®æ–¹æ³•)
from us_prod_shock_builder import USProdShockBuilder

# å¯¼å…¥æ–°çš„å®è§‚æ§åˆ¶å˜é‡æ„å»ºå™¨
from macro_controls_builder import MacroControlsBuilder

warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('clean_variable_construction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class VariableConstructor:
    """å˜é‡æ„å»ºå™¨ - åªæ„å»ºç ”ç©¶å¤§çº²è¦æ±‚çš„æ ¸å¿ƒå˜é‡"""
    
    def __init__(self, base_dir: str = None):
        """åˆå§‹åŒ–æ„å»ºå™¨"""
        if base_dir is None:
            self.base_dir = Path(__file__).parent.parent.parent
        else:
            self.base_dir = Path(base_dir)
        
        self.data_dir = self.base_dir / "data"
        self.output_dir = Path(__file__).parent / "outputs"
        self.temp_data_dir = Path(__file__).parent / "08data"
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        self.temp_data_dir.mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.macro_data = None
        self.base_data = {}
        self.core_variables = {}
        self.final_panel = None
        
        logger.info("ğŸ—ï¸ æ¸…æ™°ç‰ˆå˜é‡æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   é¡¹ç›®æ ¹ç›®å½•: {self.base_dir}")
        logger.info(f"   è¾“å‡ºç›®å½•: {self.output_dir}")

    def _ensure_macro_data(self) -> None:
        """ç¡®ä¿å®è§‚ç»æµæ•°æ®å¯ç”¨ï¼Œä½¿ç”¨ç‹¬ç«‹çš„æ„å»ºå™¨ã€‚"""
        logger.info("ğŸŒ æ­£åœ¨å‡†å¤‡å®è§‚ç»æµæ§åˆ¶å˜é‡...")
        
        # ä½¿ç”¨ç‹¬ç«‹çš„å®è§‚æ§åˆ¶å˜é‡æ„å»ºå™¨
        macro_builder = MacroControlsBuilder(
            data_dir=self.temp_data_dir,
            output_dir=self.output_dir
        )
        
        self.macro_data = macro_builder.build_macro_controls()
        if self.macro_data is None:
            logger.error("âŒ æ— æ³•è·å–å®è§‚æ•°æ®ï¼Œåç»­æ­¥éª¤å¯èƒ½å¤±è´¥ã€‚")
    
    def load_base_data(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½åŸºç¡€æ•°æ® (01, 03, 04æ¨¡å—è¾“å‡º)"""
        logger.info("ğŸ“ å¼€å§‹åŠ è½½åŸºç¡€æ•°æ®...")
        
        base_data = {}
        
        # åŠ è½½01æ¨¡å—è¾“å‡º - è´¸æ˜“æµæ•°æ®
        trade_data_files = list((self.base_dir / "data" / "processed_data").glob("cleaned_energy_trade_*.csv"))
        
        if trade_data_files:
            try:
                trade_data_list = []
                for file_path in sorted(trade_data_files):
                    yearly_data = pd.read_csv(file_path)
                    trade_data_list.append(yearly_data)
                
                trade_data = pd.concat(trade_data_list, ignore_index=True)
                base_data['trade_flow'] = trade_data
                logger.info(f"âœ… åŠ è½½è´¸æ˜“æµæ•°æ®: åˆå¹¶ {len(trade_data_files)} ä¸ªæ–‡ä»¶ ({len(trade_data)} è¡Œ)")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½è´¸æ˜“æ•°æ®: {str(e)}")
        
        # åŠ è½½03æ¨¡å—è¾“å‡º - ç½‘ç»œæŒ‡æ ‡
        metrics_data_path = self.base_dir / "src" / "03_metrics"
        
        node_metrics_path = metrics_data_path / "node_centrality_metrics.csv"
        if node_metrics_path.exists():
            try:
                node_metrics = pd.read_csv(node_metrics_path)
                base_data['node_metrics'] = node_metrics
                logger.info(f"âœ… åŠ è½½èŠ‚ç‚¹æŒ‡æ ‡: {len(node_metrics)} è¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½èŠ‚ç‚¹æŒ‡æ ‡: {str(e)}")
        
        # åŠ è½½04æ¨¡å—è¾“å‡º - DLIæ•°æ®
        dli_panel_path = self.base_dir / "src" / "04_dli_analysis" / "dli_panel_data.csv"
        if dli_panel_path.exists():
            try:
                dli_data = pd.read_csv(dli_panel_path)
                base_data['dli_panel'] = dli_data
                logger.info(f"âœ… åŠ è½½DLIé¢æ¿æ•°æ®: {len(dli_data)} è¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½DLIé¢æ¿æ•°æ®: {str(e)}")

        # ä¿®å¤è·¯å¾„ï¼šä»08dataç›®å½•åŠ è½½Node-DLI_USæ•°æ®
        node_dli_us_path = self.temp_data_dir / "node_dli_us.csv"
        if node_dli_us_path.exists():
            try:
                node_dli_us_data = pd.read_csv(node_dli_us_path)
                base_data['node_dli_us'] = node_dli_us_data
                logger.info(f"âœ… åŠ è½½Node-DLI_USæ•°æ®: {len(node_dli_us_data)} è¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½Node-DLI_USæ•°æ®: {str(e)}")

        # ä½¿ç”¨ç‹¬ç«‹çš„HHI_importsæ„å»ºå™¨
        from hhi_imports_builder import HHIImportsBuilder
        hhi_builder = HHIImportsBuilder(self.base_dir, self.output_dir, self.temp_data_dir)
        hhi_imports_data = hhi_builder.load_hhi_imports()
        
        if hhi_imports_data is not None:
            base_data['hhi_imports'] = hhi_imports_data
        else:
            logger.warning("âš ï¸ æœªèƒ½åŠ è½½HHI_importsæ•°æ®")
        
        self.base_data = base_data
        logger.info(f"âœ… åŸºç¡€æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(base_data)} ä¸ªæ•°æ®é›†")
        
        return base_data
    
    def construct_gas_ovi(self) -> Optional[pd.DataFrame]:
        """æ„å»ºå¤©ç„¶æ°”OVI"""
        logger.info("   æ„å»ºå¤©ç„¶æ°”OVI...")
        
        try:
            # ä½¿ç”¨ç®€åŒ–çš„å¤©ç„¶æ°”OVIæ„å»ºå™¨
            builder = SimpleGasOVIBuilder(self.temp_data_dir)
            ovi_data = builder.build_gas_ovi()
            
            if len(ovi_data) > 0:
                # OVIæ•°æ®å·²ç»ä½¿ç”¨æ ‡å‡†ISOä»£ç ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸åšè½¬æ¢
                
                # åªä¿ç•™æ ¸å¿ƒåˆ—ç”¨äºåˆå¹¶
                result = ovi_data[['country', 'year', 'ovi_gas']].copy()
                
                logger.info(f"âœ… å¤©ç„¶æ°”OVIæ„å»ºå®Œæˆ: {len(result)} è¡Œè®°å½•")
                logger.info(f"   ä½¿ç”¨æ ‡å‡†ISOä»£ç : {sorted(result['country'].unique())}")
                return result
            else:
                logger.warning("âš ï¸ æœªèƒ½æ„å»ºå¤©ç„¶æ°”OVIæ•°æ®")
                return None
                
        except Exception as e:
            logger.error(f"âŒ å¤©ç„¶æ°”OVIæ„å»ºå¤±è´¥: {str(e)}")
            return None
    
    def construct_us_prod_shock(self) -> Optional[pd.DataFrame]:
        """æ„å»ºç¾å›½å¤©ç„¶æ°”äº§é‡å†²å‡» - AR(2)æ®‹å·®æ–¹æ³•"""
        logger.info("   æ„å»ºUS_ProdShockï¼ˆAR(2)æ®‹å·®æ–¹æ³•ï¼‰...")
        
        try:
            # ä½¿ç”¨æ–°çš„ä¸“é—¨æ„å»ºå™¨
            shock_builder = USProdShockBuilder()
            
            # æ„å»ºAR(2)æ®‹å·®å†²å‡»
            shock_data = shock_builder.build_us_prod_shock(
                start_year=2000, 
                end_year=2024,
                save_path=self.output_dir / "us_prod_shock_ar2.csv"
            )
            
            if shock_data is None:
                logger.error("âŒ AR(2)æ®‹å·®å†²å‡»æ„å»ºå¤±è´¥")
                return None
            
            # é€‰æ‹©éœ€è¦çš„åˆ—
            result_columns = ['year', 'us_gas_production', 'us_prod_shock']
            available_columns = [col for col in result_columns if col in shock_data.columns]
            
            result_df = shock_data[available_columns].copy()
            
            # é‡å‘½ååˆ—ä»¥ä¿æŒå‘åå…¼å®¹
            if 'us_gas_production' in result_df.columns:
                result_df = result_df.rename(columns={'us_gas_production': 'us_production_gas'})
            
            logger.info(f"âœ… AR(2)æ®‹å·®US_ProdShockæ„å»ºå®Œæˆ: {len(result_df)} å¹´æ•°æ®")
            logger.info(f"   æœ‰æ•ˆå†²å‡»å€¼: {result_df['us_prod_shock'].notna().sum()} ä¸ª")
            logger.info(f"   ç¼ºå¤±å€¼(å‰2å¹´): {result_df['us_prod_shock'].isna().sum()} ä¸ª")
            
            return result_df
                
        except Exception as e:
            logger.error(f"âŒ AR(2)æ®‹å·®US_ProdShockæ„å»ºå¤±è´¥: {str(e)}")
            return None

    def construct_price_quantity_variables(self) -> Optional[pd.DataFrame]:
        """æ„å»ºä»·æ ¼ä»£ç†å˜é‡P_itå’Œæ•°é‡å¢é•¿å˜é‡g_it (åŸºäºUN Comtradeæ•°æ®)"""
        logger.info("   æ„å»ºä»·æ ¼ä»£ç†å˜é‡P_itå’Œæ•°é‡å¢é•¿å˜é‡g_it...")
        
        try:
            raw_data_dir = self.base_dir / "data" / "raw_data"
            if not raw_data_dir.exists():
                logger.error(f"âŒ UN ComtradeåŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {raw_data_dir}")
                return None
            
            # è·å–æ‰€æœ‰å¹´ä»½çš„CSVæ–‡ä»¶
            csv_files = list(raw_data_dir.glob("*.csv"))
            if not csv_files:
                logger.error(f"âŒ æœªæ‰¾åˆ°UN Comtrade CSVæ–‡ä»¶")
                return None
            
            logger.info(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªå¹´ä»½çš„æ•°æ®æ–‡ä»¶")
            
            all_gas_data = []
            
            for csv_file in sorted(csv_files):
                year = csv_file.stem
                if not year.isdigit() or int(year) < 2000 or int(year) > 2024:
                    continue
                    
                logger.info(f"   å¤„ç† {year} å¹´æ•°æ®...")
                
                try:
                    # è¯»å–å¹´åº¦æ•°æ®
                    df_year = pd.read_csv(csv_file)
                    
                    # ç­›é€‰å¤©ç„¶æ°”è¿›å£è®°å½• (å•†å“ç¼–ç 2711: Petroleum gases and other gaseous hydrocarbons)
                    gas_imports = df_year[
                        (df_year['flowCode'] == 'M') &  # è¿›å£
                        (df_year['cmdCode'] == 2711) &  # å¤©ç„¶æ°”
                        (df_year['cifvalue'].notna()) & (df_year['cifvalue'] > 0) &  # æœ‰æ•ˆä»·å€¼
                        (df_year['qty'].notna()) & (df_year['qty'] > 0)  # æœ‰æ•ˆæ•°é‡
                    ].copy()
                    
                    if len(gas_imports) == 0:
                        logger.info(f"     {year}å¹´æ— æœ‰æ•ˆå¤©ç„¶æ°”è¿›å£è®°å½•")
                        continue
                    
                    logger.info(f"     {year}å¹´: {len(gas_imports)}æ¡å¤©ç„¶æ°”è¿›å£è®°å½•")
                    
                    # æ ‡å‡†åŒ–æ•°é‡å•ä½åˆ°kg
                    gas_imports['qty_standardized_kg'] = gas_imports.apply(
                        self._standardize_quantity_to_kg, axis=1
                    )
                    
                    # è¿‡æ»¤æ— æ•ˆçš„æ ‡å‡†åŒ–æ•°é‡
                    gas_imports = gas_imports[gas_imports['qty_standardized_kg'] > 0]
                    
                    if len(gas_imports) == 0:
                        logger.info(f"     {year}å¹´: æ ‡å‡†åŒ–åæ— æœ‰æ•ˆè®°å½•")
                        continue
                    
                    # æŒ‰å›½å®¶(reporterISO)å’Œå¹´ä»½èšåˆ
                    yearly_agg = gas_imports.groupby('reporterISO').agg({
                        'cifvalue': 'sum',      # æ€»è¿›å£ä»·å€¼(ç¾å…ƒ)
                        'qty_standardized_kg': 'sum'  # æ€»è¿›å£é‡(kg)
                    }).reset_index()
                    
                    yearly_agg['year'] = int(year)
                    yearly_agg['country'] = yearly_agg['reporterISO']  # ä½¿ç”¨ISOä»£ç ä½œä¸ºå›½å®¶æ ‡è¯†
                    
                    # è®¡ç®—å¹´åº¦å¹³å‡è¿›å£ä»·æ ¼ P_it (å•ä½ä»·å€¼ = ç¾å…ƒ/kg)
                    yearly_agg['P_it'] = yearly_agg['cifvalue'] / yearly_agg['qty_standardized_kg']
                    
                    # ä¿ç•™éœ€è¦çš„åˆ—
                    yearly_result = yearly_agg[['country', 'year', 'P_it', 'qty_standardized_kg']].copy()
                    all_gas_data.append(yearly_result)
                    
                    logger.info(f"     {year}å¹´: {len(yearly_result)}ä¸ªå›½å®¶æœ‰å¤©ç„¶æ°”è¿›å£æ•°æ®")
                    
                except Exception as e:
                    logger.warning(f"     âŒ {year}å¹´æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
                    continue
            
            if not all_gas_data:
                logger.error("âŒ æœªèƒ½å¤„ç†ä»»ä½•å¹´ä»½çš„å¤©ç„¶æ°”è¿›å£æ•°æ®")
                return None
            
            # åˆå¹¶æ‰€æœ‰å¹´ä»½æ•°æ®
            combined_data = pd.concat(all_gas_data, ignore_index=True)
            logger.info(f"âœ… åˆå¹¶æ•°æ®: {len(combined_data)}æ¡è®°å½•ï¼Œ{combined_data['country'].nunique()}ä¸ªå›½å®¶")
            
            # æ’åºæ•°æ®ä»¥ä¾¿è®¡ç®—å¢é•¿ç‡
            combined_data = combined_data.sort_values(['country', 'year']).reset_index(drop=True)
            
            # è®¡ç®—æ•°é‡å¢é•¿ç‡ g_it = ln(qty_t) - ln(qty_{t-1})
            combined_data['qty_ln'] = np.log(combined_data['qty_standardized_kg'])
            combined_data['g_it'] = combined_data.groupby('country')['qty_ln'].diff()
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            output_path = self.temp_data_dir / "gas_price_quantity_data.csv"
            combined_data.to_csv(output_path, index=False)
            
            # è¿”å›æœ€ç»ˆç»“æœ
            result = combined_data[['country', 'year', 'P_it', 'g_it']].copy()
            
            # åŒæ—¶ä¿å­˜åˆ°outputsç›®å½•ä¾›09æ¨¡å—ä½¿ç”¨
            final_output_path = self.output_dir / "price_quantity_variables.csv"
            result.to_csv(final_output_path, index=False)
            logger.info(f"ğŸ’¾ P_itå’Œg_itå˜é‡ä¿å­˜è‡³: {final_output_path}")
            
            logger.info(f"âœ… ä»·æ ¼æ•°é‡å˜é‡æ„å»ºå®Œæˆ:")
            logger.info(f"   ğŸ“Š æ•°æ®è®°å½•: {len(result)} æ¡")
            logger.info(f"   ğŸŒ è¦†ç›–å›½å®¶: {result['country'].nunique()} ä¸ª")
            logger.info(f"   ğŸ“… å¹´ä»½èŒƒå›´: {result['year'].min()}-{result['year'].max()}")
            logger.info(f"   ğŸ’¾ ä¸­é—´æ–‡ä»¶: {output_path}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ä»·æ ¼æ•°é‡å˜é‡æ„å»ºå¤±è´¥: {str(e)}")
            return None

    def _standardize_quantity_to_kg(self, row) -> float:
        """
        å°†ä¸åŒå•ä½çš„æ•°é‡æ ‡å‡†åŒ–ä¸ºåƒå…‹(kg)
        
        æ ¹æ®UN Comtradeæ•°æ®ä¸­çš„qtyUnitAbbrå­—æ®µè¿›è¡Œå•ä½è½¬æ¢
        """
        qty = row.get('qty', 0)
        unit = row.get('qtyUnitAbbr', '')
        
        if pd.isna(qty) or qty <= 0:
            return 0
        
        if pd.isna(unit):
            unit = ''
        
        unit = str(unit).strip().lower()
        
        # å•ä½è½¬æ¢æ˜ å°„ (è½¬æ¢ä¸ºkg)
        unit_conversion = {
            'kg': 1.0,           # åƒå…‹
            't': 1000.0,         # å¨ = 1000kg
            'l': 0.5,           # å‡ (LNGå¯†åº¦çº¦0.5kg/L)
            'm3': 0.8,          # ç«‹æ–¹ç±³ (å¤©ç„¶æ°”å¯†åº¦çº¦0.8kg/m3)
            'mt': 1000.0,       # å…¬å¨ = 1000kg
            'g': 0.001,         # å…‹ = 0.001kg
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„å•ä½è½¬æ¢å› å­
        conversion_factor = unit_conversion.get(unit, 1.0)  # é»˜è®¤å‡è®¾å·²ç»æ˜¯kg
        
        return float(qty) * conversion_factor
    
    def construct_core_variables(self) -> Dict[str, pd.DataFrame]:
        """æ„å»ºæ ¸å¿ƒå˜é‡"""
        logger.info("âš™ï¸ å¼€å§‹æ„å»ºæ ¸å¿ƒå˜é‡...")
        
        core_vars = {}
        
        # 1. åŠ è½½ Node-DLI_US (ç”±04æ¨¡å—æ„å»º)
        if 'node_dli_us' in self.base_data:
            core_vars['node_dli_us'] = self.base_data['node_dli_us']
        else:
            logger.warning("âš ï¸ æœªèƒ½åŠ è½½Node-DLI_USæ•°æ®ï¼Œå°†è·³è¿‡æ­¤å˜é‡ã€‚")

        # 2. åŠ è½½ HHI_imports (ç”±05æ¨¡å—æ„å»ºï¼Œæ›¿ä»£vul_us)
        if 'hhi_imports' in self.base_data:
            core_vars['hhi_imports'] = self.base_data['hhi_imports']
        else:
            logger.warning("âš ï¸ æœªèƒ½åŠ è½½HHI_importsæ•°æ®ï¼Œå°†è·³è¿‡æ­¤å˜é‡ã€‚")
        
        # 3. æ„å»ºå¤©ç„¶æ°”OVI
        gas_ovi = self.construct_gas_ovi()
        if gas_ovi is not None:
            core_vars['ovi_gas'] = gas_ovi
        
        # 4. æ„å»º USäº§é‡å†²å‡»
        us_shock = self.construct_us_prod_shock()
        if us_shock is not None:
            core_vars['us_prod_shock'] = us_shock
        
        # 5. æ„å»ºä»·æ ¼å’Œæ•°é‡å˜é‡ (P_it å’Œ g_it)
        price_qty_data = self.construct_price_quantity_variables()
        if price_qty_data is not None:
            core_vars['price_quantity'] = price_qty_data
        
        self.core_variables = core_vars
        logger.info(f"âœ… æ ¸å¿ƒå˜é‡æ„å»ºå®Œæˆï¼Œå…± {len(core_vars)} ä¸ªå˜é‡")
        
        return core_vars
    
    def create_analytical_panel(self) -> pd.DataFrame:
        """åˆ›å»ºæœ€ç»ˆåˆ†æé¢æ¿"""
        logger.info("ğŸ”— å¼€å§‹åˆ›å»ºæœ€ç»ˆåˆ†æé¢æ¿...")
        
        # ä»å®è§‚æ•°æ®å¼€å§‹æ„å»ºé¢æ¿
        if self.macro_data is not None:
            panel = self.macro_data.copy()
            
            # å¤„ç†countryåˆ—çš„æ˜ å°„
            if 'country_code' in panel.columns:
                # ä¸–ç•Œé“¶è¡Œæ•°æ®å·²æœ‰æ ‡å‡†ISOä»£ç ï¼Œç›´æ¥ä½¿ç”¨
                panel['country'] = panel['country_code']
                logger.info(f"   åŸºäºä¸–ç•Œé“¶è¡Œå®è§‚æ•°æ®æ„å»ºèµ·å§‹é¢æ¿: {len(panel)} è¡Œ")
                logger.info(f"   è¦†ç›–å›½å®¶: {panel['country'].nunique()} ä¸ªï¼ˆä½¿ç”¨æ ‡å‡†ISOä»£ç ï¼‰")
            elif 'country_name' in panel.columns and 'country' not in panel.columns:
                # æ—§ç‰ˆæœ¬æ•°æ®ï¼Œéœ€è¦æ˜ å°„
                country_name_to_iso = {
                    'Australia': 'AUS', 'Brazil': 'BRA', 'Canada': 'CAN',
                    'China': 'CHN', 'France': 'FRA', 'Germany': 'DEU',
                    'Indonesia': 'IDN', 'Italy': 'ITA', 'Japan': 'JPN',
                    'Korea, Rep.': 'KOR', 'Mexico': 'MEX', 'Netherlands': 'NLD',
                    'Russian Federation': 'RUS', 'Saudi Arabia': 'SAU',
                    'Spain': 'ESP', 'Switzerland': 'CHE', 'Turkiye': 'TUR',
                    'United Kingdom': 'GBR', 'United States': 'USA'
                }
                panel['country'] = panel['country_name'].map(country_name_to_iso)
                panel = panel.dropna(subset=['country'])  # ç§»é™¤æ— æ³•æ˜ å°„çš„å›½å®¶
                logger.info(f"   åŸºäºå®è§‚æ•°æ®æ„å»ºèµ·å§‹é¢æ¿: {len(panel)} è¡Œï¼ˆä½¿ç”¨æ ‡å‡†ISOä»£ç ï¼‰")
        else:
            # åˆ›å»ºåŸºç¡€æ¡†æ¶é¢æ¿
            logger.info("âš ï¸ å®è§‚æ•°æ®ç¼ºå¤±ï¼Œåˆ›å»ºåŸºç¡€å›½å®¶-å¹´ä»½é¢æ¿æ¡†æ¶ï¼ˆä½¿ç”¨æ ‡å‡†ISOä»£ç ï¼‰")
            countries = ['USA', 'CHN', 'DEU', 'JPN', 'GBR', 'FRA', 'IND', 'ITA', 'BRA', 'CAN',
                        'RUS', 'AUS', 'KOR', 'ESP', 'MEX', 'IDN', 'NLD', 'SAU', 'TUR', 'CHE']
            years = list(range(2000, 2025))
            
            country_year_pairs = []
            for country in countries:
                for year in years:
                    country_year_pairs.append({'country': country, 'year': year})
            
            panel = pd.DataFrame(country_year_pairs)
            logger.info(f"   åˆ›å»ºåŸºç¡€é¢æ¿æ¡†æ¶: {len(panel)} è¡Œ")
        
        # åˆå¹¶æ ¸å¿ƒå˜é‡
        merge_count = 0
        for var_name, var_data in self.core_variables.items():
            if var_data is not None and len(var_data) > 0:
                try:
                    before_len = len(panel)
                    
                    if var_name == 'us_prod_shock':
                        # US_ProdShockåªæœ‰å¹´ä»½æ•°æ®ï¼Œä¸ºæ‰€æœ‰å›½å®¶å¤åˆ¶
                        panel = panel.merge(var_data, on='year', how='left')
                    elif var_name == 'price_quantity':
                        # ä»·æ ¼æ•°é‡å˜é‡å•ç‹¬ä¿å­˜ï¼Œä¸åˆå¹¶åˆ°ä¸»é¢æ¿
                        logger.info(f"   ğŸ“Š {var_name}: å•ç‹¬ä¿å­˜ï¼Œä¸åˆå¹¶åˆ°ä¸»é¢æ¿")
                    else:
                        # å…¶ä»–å˜é‡æŒ‰yearå’Œcountryåˆå¹¶
                        panel = panel.merge(var_data, on=['year', 'country'], how='left')
                    
                    after_len = len(panel)
                    
                    if after_len == before_len:
                        merge_count += 1
                        # ç»Ÿè®¡è¦†ç›–ç‡ - æ”¹è¿›ç‰ˆæœ¬
                        if var_name != 'us_prod_shock' and var_name != 'price_quantity':
                            # è·å–åˆå¹¶åå®é™…çš„éç©ºå€¼æ•°é‡
                            var_columns = [col for col in var_data.columns if col not in ['year', 'country']]
                            if var_columns:
                                main_var_col = var_columns[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªéç´¢å¼•åˆ—ä½œä¸ºä¸»è¦å˜é‡
                                actual_coverage = panel[main_var_col].notna().sum()
                                total_possible = len(panel)
                                coverage_rate = actual_coverage / total_possible * 100
                                logger.info(f"   âœ… åˆå¹¶ {var_name}: {actual_coverage}/{total_possible} è§‚æµ‹å€¼ ({coverage_rate:.1f}% è¦†ç›–ç‡)")
                        else:
                            logger.info(f"   âœ… åˆå¹¶ {var_name}: {len(var_data)} å¹´æ•°æ®ï¼ˆå…¨é¢æ¿å¤åˆ¶ï¼‰")
                            
                except Exception as e:
                    logger.warning(f"   âŒ æ— æ³•åˆå¹¶ {var_name}: {str(e)}")
        
        # åˆå¹¶åœ°ç†è·ç¦»æ•°æ®ï¼ˆæ–°å¢ï¼šç”¨äºåœ°ç†å¼‚è´¨æ€§åˆ†æï¼‰
        try:
            import json
            logger.info("ğŸŒ åˆå¹¶åœ°ç†è·ç¦»æ•°æ®...")
            
            # è¯»å–åœ°ç†è·ç¦»JSONæ–‡ä»¶
            distance_json_path = Path("../04_dli_analysis/complete_us_distances_cepii.json")
            if distance_json_path.exists():
                with open(distance_json_path, 'r', encoding='utf-8') as f:
                    distance_data = json.load(f)
                
                # è½¬æ¢ä¸ºDataFrame
                distance_df = pd.DataFrame(list(distance_data.items()), columns=['country', 'distance_to_us'])
                distance_df['distance_to_us'] = pd.to_numeric(distance_df['distance_to_us'])
                
                # å·¦è¿æ¥åˆ°ä¸»é¢æ¿ï¼ˆè·ç¦»æ•°æ®å¯¹æ‰€æœ‰å¹´ä»½éƒ½ç›¸åŒï¼‰
                before_merge = len(panel)
                panel = panel.merge(distance_df, on='country', how='left')
                after_merge = len(panel)
                
                if after_merge == before_merge:
                    coverage = panel['distance_to_us'].notna().sum()
                    coverage_rate = coverage / len(panel) * 100
                    logger.info(f"   âœ… åœ°ç†è·ç¦»æ•°æ®åˆå¹¶æˆåŠŸ: {coverage}/{len(panel)} è§‚æµ‹å€¼ ({coverage_rate:.1f}% è¦†ç›–ç‡)")
                    logger.info(f"   è¦†ç›–å›½å®¶æ ·ä¾‹: {sorted(distance_df['country'].head(10).tolist())}")
                else:
                    logger.warning(f"   âš ï¸  åˆå¹¶åæ•°æ®è¡Œæ•°å˜åŒ–: {before_merge} -> {after_merge}")
                    
            else:
                logger.warning(f"   âš ï¸  åœ°ç†è·ç¦»æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {distance_json_path}")
                # æ·»åŠ ç©ºçš„distance_to_usåˆ—ä»¥ä¿æŒæ•°æ®ç»“æ„ä¸€è‡´
                panel['distance_to_us'] = np.nan
                
        except Exception as e:
            logger.warning(f"   âŒ åœ°ç†è·ç¦»æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
            # æ·»åŠ ç©ºçš„distance_to_usåˆ—ä»¥ä¿æŒæ•°æ®ç»“æ„ä¸€è‡´
            panel['distance_to_us'] = np.nan
        
        # æ•°æ®æ¸…æ´—
        panel = self._clean_final_panel(panel)
        
        # éªŒè¯å…³é”®å˜é‡å®Œæ•´æ€§
        self._validate_panel_completeness(panel)
        
        # ä¿å­˜æœ€ç»ˆé¢æ¿
        output_path = self.output_dir / "analytical_panel.csv"
        panel.to_csv(output_path, index=False)
        
        self.final_panel = panel
        
        logger.info(f"âœ… æœ€ç»ˆåˆ†æé¢æ¿åˆ›å»ºå®Œæˆ:")
        logger.info(f"   è¡Œæ•°: {len(panel)}")
        logger.info(f"   åˆ—æ•°: {len(panel.columns)}")
        logger.info(f"   å¹´ä»½èŒƒå›´: {panel['year'].min()}-{panel['year'].max()}")
        logger.info(f"   å›½å®¶æ•°é‡: {panel['country'].nunique()}")
        logger.info(f"   æˆåŠŸåˆå¹¶: {merge_count} ä¸ªæ ¸å¿ƒå˜é‡")
        logger.info(f"   ä¿å­˜è‡³: {output_path}")
        
        return panel
    
    def _clean_final_panel(self, panel: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æœ€ç»ˆé¢æ¿æ•°æ®"""
        logger.info("   æ¸…æ´—æœ€ç»ˆé¢æ¿æ•°æ®...")
        
        # åˆ é™¤é‡å¤è¡Œ
        initial_len = len(panel)
        panel = panel.drop_duplicates(subset=['year', 'country'])
        if len(panel) != initial_len:
            logger.info(f"   åˆ é™¤é‡å¤è¡Œ: {initial_len} -> {len(panel)}")
        
        # ç¡®ä¿å¹´ä»½ä¸ºæ•´æ•°
        panel['year'] = panel['year'].astype(int)
        
        # é™åˆ¶å¹´ä»½èŒƒå›´
        panel = panel[(panel['year'] >= 2000) & (panel['year'] <= 2024)]
        
        # æ¸…ç†æ— æ•ˆå€¼
        numeric_columns = panel.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            panel[col] = panel[col].replace([np.inf, -np.inf], np.nan)
        
        # æŒ‰å¹´ä»½å’Œå›½å®¶æ’åº
        panel = panel.sort_values(['year', 'country']).reset_index(drop=True)
        
        return panel
    
    def _validate_panel_completeness(self, panel: pd.DataFrame) -> None:
        """éªŒè¯åˆ†æé¢æ¿çš„å…³é”®å˜é‡å®Œæ•´æ€§"""
        logger.info("ğŸ” éªŒè¯åˆ†æé¢æ¿å…³é”®å˜é‡å®Œæ•´æ€§...")
        
        # å®šä¹‰æœŸæœ›çš„æ ¸å¿ƒå˜é‡ï¼ˆæ›´æ–°ï¼šhhi_importsæ›¿ä»£vul_usï¼‰
        expected_core_vars = ['node_dli_us', 'hhi_imports', 'ovi_gas', 'us_prod_shock']
        
        validation_results = {}
        total_observations = len(panel)
        
        for var in expected_core_vars:
            if var in panel.columns:
                non_null_count = panel[var].notna().sum()
                coverage_rate = non_null_count / total_observations * 100
                validation_results[var] = {
                    'present': True,
                    'non_null_count': non_null_count,
                    'coverage_rate': coverage_rate
                }
                
                # è¯„ä¼°è¦†ç›–ç‡
                if coverage_rate >= 70:
                    status = "âœ… ä¼˜ç§€"
                elif coverage_rate >= 50:
                    status = "âš ï¸ è‰¯å¥½"
                elif coverage_rate >= 30:
                    status = "âš ï¸ ä¸€èˆ¬"
                else:
                    status = "âŒ ä¸è¶³"
                
                logger.info(f"   {var}: {non_null_count}/{total_observations} ({coverage_rate:.1f}%) {status}")
            else:
                validation_results[var] = {'present': False}
                logger.error(f"   âŒ {var}: å˜é‡ç¼ºå¤±ï¼")
        
        # æ€»ä½“è¯„ä¼°
        present_vars = sum(1 for v in validation_results.values() if v.get('present', False))
        logger.info(f"ğŸ“Š å˜é‡å®Œæ•´æ€§æ€»ç»“: {present_vars}/{len(expected_core_vars)} ä¸ªæ ¸å¿ƒå˜é‡å­˜åœ¨")
        
        if present_vars < len(expected_core_vars):
            logger.warning(f"âš ï¸ è­¦å‘Šï¼š{len(expected_core_vars) - present_vars} ä¸ªæ ¸å¿ƒå˜é‡ç¼ºå¤±ï¼Œå¯èƒ½å½±å“åç»­åˆ†æ")
        else:
            logger.info("âœ… æ‰€æœ‰æ ¸å¿ƒå˜é‡å‡å·²æˆåŠŸæ•´åˆåˆ°åˆ†æé¢æ¿")
        
        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        validation_report_path = self.output_dir / "data_validation_report.txt"
        with open(validation_report_path, 'w', encoding='utf-8') as f:
            f.write("08æ¨¡å—æ•°æ®å®Œæ•´æ€§éªŒè¯æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"æ€»è§‚æµ‹æ•°: {total_observations}\n")
            f.write(f"æ€»å›½å®¶æ•°: {panel['country'].nunique()}\n")
            f.write(f"å¹´ä»½èŒƒå›´: {panel['year'].min()}-{panel['year'].max()}\n\n")
            
            f.write("æ ¸å¿ƒå˜é‡è¦†ç›–ç‡:\n")
            for var, result in validation_results.items():
                if result.get('present', False):
                    f.write(f"  {var}: {result['non_null_count']}/{total_observations} ({result['coverage_rate']:.1f}%)\n")
                else:
                    f.write(f"  {var}: å˜é‡ç¼ºå¤±\n")
        
        logger.info(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {validation_report_path}")

    def run_pipeline(self) -> None:
        """è¿è¡Œå˜é‡æ„å»ºæµæ°´çº¿"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå˜é‡æ„å»ºæµæ°´çº¿...")
        
        try:
            # æ­¥éª¤1: å‡†å¤‡å®è§‚æ§åˆ¶å˜é‡ (æŒ‰éœ€ä¸‹è½½å’Œæ¸…ç†)
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤1: å‡†å¤‡å®è§‚ç»æµæ§åˆ¶å˜é‡")
            logger.info("="*50)
            self._ensure_macro_data()
            
            # æ­¥éª¤2: åŠ è½½åŸºç¡€æ•°æ®
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤2: åŠ è½½åŸºç¡€æ•°æ®")
            logger.info("="*50)
            self.load_base_data()
            
            # æ­¥éª¤3: æ„å»ºæ ¸å¿ƒå˜é‡
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤3: æ„å»ºæ ¸å¿ƒå˜é‡")
            logger.info("="*50)
            self.construct_core_variables()
            
            # æ­¥éª¤4: åˆ›å»ºæœ€ç»ˆé¢æ¿
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤4: åˆ›å»ºæœ€ç»ˆåˆ†æé¢æ¿")
            logger.info("="*50)
            self.create_analytical_panel()
            
            logger.info("\n" + "="*60)
            logger.info("ğŸ‰ å˜é‡æ„å»ºæµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
            logger.info("="*60)
            logger.info(f"âœ… æœ€ç»ˆè¾“å‡º:")
            logger.info(f"   - åˆ†æé¢æ¿: analytical_panel.csv ({len(self.final_panel)} è¡Œ)")
            logger.info(f"   - ä¸­é—´æ–‡ä»¶: {self.temp_data_dir}")
            logger.info(f"   - è¾“å‡ºç›®å½•: {self.output_dir}")
            
        except Exception as e:
            logger.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise

def main():
    """ä¸»å‡½æ•°ï¼šå˜é‡æ„å»ºæµæ°´çº¿"""
    print("ğŸ—ï¸ 08_variable_construction - æ•°æ®å·¥å‚")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–å˜é‡æ„å»ºå™¨
        constructor = VariableConstructor()
        
        # è¿è¡Œæµæ°´çº¿
        constructor.run_pipeline()
        
        print("\nâœ… å˜é‡æ„å»ºæ¨¡å—æ‰§è¡ŒæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶:")
        print(f"   - {constructor.output_dir / 'analytical_panel.csv'}")
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    main()