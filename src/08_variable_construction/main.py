#!/usr/bin/env python3
"""
å˜é‡æ„å»ºæ¨¡å— (Variable Construction Module)
========================================

æœ¬æ¨¡å—æ˜¯é¡¹ç›®ç ”ç©¶èŒƒå¼æ›´æ–°åçš„æ•°æ®å¥ åŸºæ¨¡å—ã€‚
æ ¸å¿ƒç›®æ ‡ï¼šä»åŸºç¡€æ•°æ®æºå‡ºå‘ï¼Œæœé›†ã€è®¡ç®—å¹¶æ•´åˆæ‰€æœ‰ç ”ç©¶éœ€è¦çš„å˜é‡ï¼Œ
æœ€ç»ˆç”Ÿæˆä¸€ä»½å¹²å‡€ã€å®Œæ•´çš„å›½åˆ«-å¹´åº¦é¢æ¿æ•°æ®é›† analytical_panel.csvã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. æœé›†å®è§‚ç»æµæ§åˆ¶å˜é‡ (World Bank API)
2. åŠ è½½åŸºç¡€æ•°æ® (01, 03, 04æ¨¡å—è¾“å‡º)
3. æ„å»ºæ ¸å¿ƒå˜é‡ (Node-DLI_US, Vul_US, OVI, US_ProdShock)
4. æ•´åˆè¾“å‡ºæœ€ç»ˆåˆ†æé¢æ¿

ä½œè€…ï¼šEnergy Network Analysis Team
ç‰ˆæœ¬ï¼šv1.0
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
import warnings
import json
from typing import Dict, List, Tuple, Optional, Any
import sys
import os
# é›†æˆæ–°ç‰ˆæ—¶é—´åºåˆ— OVI è®¡ç®—å™¨
from timeseries_ovi_builder import TimeSeriesOVIBuilder

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('variable_construction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å¯é€‰ä¾èµ–
try:
    import wbdata
    HAS_WBDATA = True
    logger.info("âœ… wbdataåº“å¯ç”¨ï¼Œå°†ä½¿ç”¨World Bank API")
except ImportError:
    HAS_WBDATA = False
    logger.warning("âš ï¸ wbdataåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

try:
    import requests
    HAS_REQUESTS = True
    logger.info("âœ… requestsåº“å¯ç”¨ï¼Œå°†ä½¿ç”¨EIA API")
except ImportError:
    HAS_REQUESTS = False
    logger.warning("âš ï¸ requestsåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

try:
    from scipy.signal import savgol_filter
    from statsmodels.tsa.filters.hp_filter import hpfilter
    HAS_FILTERING = True
    logger.info("âœ… æ»¤æ³¢åº“å¯ç”¨ï¼Œå°†ä½¿ç”¨HPæ»¤æ³¢")
except ImportError:
    HAS_FILTERING = False
    logger.warning("âš ï¸ æ»¤æ³¢åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–å¤„ç†")

class VariableConstructor:
    """å˜é‡æ„å»ºä¸»ç±»"""
    
    def __init__(self, base_dir: str = None):
        """åˆå§‹åŒ–å˜é‡æ„å»ºå™¨"""
        if base_dir is None:
            self.base_dir = Path(__file__).parent.parent.parent
        else:
            self.base_dir = Path(base_dir)
        
        self.data_dir = self.base_dir / "data"
        self.output_dir = Path(__file__).parent / "outputs"
        self.temp_data_dir = Path(__file__).parent / "08data"  # ä½¿ç”¨08dataç›®å½•
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        self.temp_data_dir.mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.macro_data = None
        self.base_data = {}
        self.core_variables = {}
        self.final_panel = None
        
        logger.info("ğŸ—ï¸ å˜é‡æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   é¡¹ç›®æ ¹ç›®å½•: {self.base_dir}")
        logger.info(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def fetch_macro_controls(self) -> pd.DataFrame:
        """
        æœé›†å®è§‚ç»æµæ§åˆ¶å˜é‡
        
        Returns:
            åŒ…å«å®è§‚ç»æµå˜é‡çš„DataFrame
        """
        logger.info("ğŸŒ å¼€å§‹æœé›†å®è§‚ç»æµæ§åˆ¶å˜é‡...")
        
        # ä¼˜å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
        cache_path = self.temp_data_dir / "macro_controls.csv"
        if cache_path.exists():
            try:
                logger.info("   ä»ç¼“å­˜åŠ è½½å®è§‚æ•°æ®...")
                macro_data = pd.read_csv(cache_path)
                
                # ç¡®ä¿å¹´ä»½ä¸ºæ•´æ•°ç±»å‹
                if 'year' in macro_data.columns:
                    # å¤„ç†å¯èƒ½çš„datetimeå­—ç¬¦ä¸²
                    macro_data['year'] = pd.to_datetime(macro_data['year']).dt.year
                
                logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½å®è§‚æ•°æ®: {len(macro_data)} è¡Œè®°å½•")
                logger.info(f"   æ•°æ®èŒƒå›´: {macro_data['year'].min()}-{macro_data['year'].max()}")
                
                self.macro_data = macro_data
                return macro_data
                
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜æ•°æ®åŠ è½½å¤±è´¥: {str(e)}ï¼Œå°†é‡æ–°è·å–")
        
        if not HAS_WBDATA:
            logger.warning("âš ï¸ wbdataåº“ä¸å¯ç”¨ï¼Œæ— æ³•è·å–World Bankæ•°æ®")
            return None
        
        try:
            # å®šä¹‰å˜é‡æ˜ å°„
            indicators = {
                'NY.GDP.MKTP.CD': 'gdp_current_usd',
                'SP.POP.TOTL': 'population_total', 
                'NE.TRD.GNFS.ZS': 'trade_openness_gdp_pct'
            }
            
            # å®šä¹‰æ—¶é—´èŒƒå›´å’Œå›½å®¶
            import datetime
            date_range = (datetime.datetime(2000, 1, 1), datetime.datetime(2024, 12, 31))
            
            # ä»ç°æœ‰æ•°æ®æ¨æ–­å›½å®¶åˆ—è¡¨
            countries = self._get_country_list()
            
            # ä»World Bank APIè·å–æ•°æ®
            logger.info(f"   ä»World Bankè·å– {len(indicators)} ä¸ªæŒ‡æ ‡ï¼Œ{len(countries)} ä¸ªå›½å®¶")
            
            macro_data = wbdata.get_dataframe(
                indicators, 
                country=countries,
                date=date_range,
                parse_dates=True
            ).reset_index()
            
            # æ•°æ®æ¸…æ´—
            macro_data = macro_data.rename(columns={
                'country': 'country_name',
                'date': 'year'
            })
            
            # è½¬æ¢å¹´ä»½ä¸ºæ•´æ•°ï¼ˆè§£å†³ä¸å…¶ä»–æ•°æ®åˆå¹¶æ—¶çš„ç±»å‹å†²çªï¼‰
            if 'year' in macro_data.columns:
                macro_data['year'] = pd.to_datetime(macro_data['year']).dt.year
            
            # è®¡ç®—å¯¹æ•°å˜æ¢
            if 'gdp_current_usd' in macro_data.columns:
                macro_data['log_gdp'] = np.log(macro_data['gdp_current_usd'].replace(0, np.nan))
            
            if 'population_total' in macro_data.columns:
                macro_data['log_population'] = np.log(macro_data['population_total'].replace(0, np.nan))
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            output_path = self.temp_data_dir / "macro_controls.csv"
            macro_data.to_csv(output_path, index=False)
            
            logger.info(f"âœ… å®è§‚æ•°æ®æœé›†å®Œæˆ: {len(macro_data)} è¡Œè®°å½•")
            logger.info(f"   æ•°æ®èŒƒå›´: {macro_data['year'].min()}-{macro_data['year'].max()}")
            logger.info(f"   ä¿å­˜è‡³: {output_path}")
            
            self.macro_data = macro_data
            return macro_data
            
        except Exception as e:
            logger.error(f"âŒ World Bank APIè°ƒç”¨å¤±è´¥: {str(e)}")
            logger.warning("âš ï¸ æ— æ³•è·å–World Bankæ•°æ®ï¼Œå®è§‚å˜é‡å°†æ ‡è®°ä¸ºç¼ºå¤±å€¼")
            return None
    
    def _get_country_list(self) -> List[str]:
        """ä»ç°æœ‰æ•°æ®æ¨æ–­å›½å®¶åˆ—è¡¨"""
        try:
            # å°è¯•ä»01æ¨¡å—çš„è¾“å‡ºè·å–å›½å®¶åˆ—è¡¨
            trade_data_path = self.base_dir / "src" / "01_data_processing" / "cleaned_trade_flow.csv"
            if trade_data_path.exists():
                trade_data = pd.read_csv(trade_data_path, nrows=1000)  # åªè¯»å‰1000è¡Œæ¨æ–­
                countries = set()
                if 'exporter_iso3' in trade_data.columns:
                    countries.update(trade_data['exporter_iso3'].unique())
                if 'importer_iso3' in trade_data.columns:
                    countries.update(trade_data['importer_iso3'].unique())
                countries = list(countries)
                logger.info(f"   ä»è´¸æ˜“æ•°æ®æ¨æ–­å‡º {len(countries)} ä¸ªå›½å®¶")
                return countries
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•ä»ç°æœ‰æ•°æ®æ¨æ–­å›½å®¶åˆ—è¡¨: {str(e)}")
        
        # ä½¿ç”¨é»˜è®¤ä¸»è¦å›½å®¶åˆ—è¡¨
        default_countries = [
            'USA', 'CHN', 'DEU', 'JPN', 'GBR', 'FRA', 'IND', 'ITA', 'BRA', 'CAN',
            'RUS', 'AUS', 'KOR', 'ESP', 'MEX', 'IDN', 'NLD', 'SAU', 'TUR', 'CHE'
        ]
        logger.info(f"   ä½¿ç”¨é»˜è®¤å›½å®¶åˆ—è¡¨: {len(default_countries)} ä¸ªå›½å®¶")
        return default_countries

    def load_base_data(self) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½åŸºç¡€æ•°æ® (01, 03, 04æ¨¡å—è¾“å‡º)
        
        Returns:
            åŒ…å«å„æ¨¡å—æ•°æ®çš„å­—å…¸
        """
        logger.info("ğŸ“ å¼€å§‹åŠ è½½åŸºç¡€æ•°æ®...")
        
        base_data = {}
        
        # åŠ è½½01æ¨¡å—è¾“å‡º - è´¸æ˜“æµæ•°æ®
        trade_data_files = list((self.base_dir / "data" / "processed_data").glob("cleaned_energy_trade_*.csv"))
        
        if trade_data_files:
            try:
                # åˆå¹¶æ‰€æœ‰å¹´ä»½çš„è´¸æ˜“æ•°æ®
                trade_data_list = []
                for file_path in sorted(trade_data_files):
                    yearly_data = pd.read_csv(file_path)
                    trade_data_list.append(yearly_data)
                
                trade_data = pd.concat(trade_data_list, ignore_index=True)
                base_data['trade_flow'] = trade_data
                logger.info(f"âœ… åŠ è½½è´¸æ˜“æµæ•°æ®: åˆå¹¶ {len(trade_data_files)} ä¸ªæ–‡ä»¶ ({len(trade_data)} è¡Œ)")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½è´¸æ˜“æ•°æ®: {str(e)}")
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°cleaned_energy_tradeæ•°æ®æ–‡ä»¶")
        
        # åŠ è½½03æ¨¡å—è¾“å‡º - ç½‘ç»œæŒ‡æ ‡
        metrics_data_path = self.base_dir / "src" / "03_metrics"
        
        # èŠ‚ç‚¹ä¸­å¿ƒæ€§æŒ‡æ ‡
        node_metrics_path = metrics_data_path / "node_centrality_metrics.csv"
        if node_metrics_path.exists():
            try:
                node_metrics = pd.read_csv(node_metrics_path)
                base_data['node_metrics'] = node_metrics
                logger.info(f"âœ… åŠ è½½èŠ‚ç‚¹æŒ‡æ ‡: {len(node_metrics)} è¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½èŠ‚ç‚¹æŒ‡æ ‡: {str(e)}")
        
        # å…¨å±€ç½‘ç»œæŒ‡æ ‡
        global_metrics_path = metrics_data_path / "global_network_metrics.csv"
        if global_metrics_path.exists():
            try:
                global_metrics = pd.read_csv(global_metrics_path)
                base_data['global_metrics'] = global_metrics
                logger.info(f"âœ… åŠ è½½å…¨å±€æŒ‡æ ‡: {len(global_metrics)} è¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½å…¨å±€æŒ‡æ ‡: {str(e)}")
        
        # åŠ è½½04æ¨¡å—è¾“å‡º - DLIæ•°æ®
        dli_data_path = self.base_dir / "src" / "04_dli_analysis" / "dli_panel_data.csv"
        if dli_data_path.exists():
            try:
                dli_data = pd.read_csv(dli_data_path)
                base_data['dli_panel'] = dli_data
                logger.info(f"âœ… åŠ è½½DLIé¢æ¿æ•°æ®: {len(dli_data)} è¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½DLIæ•°æ®: {str(e)}")
        
        # å¦‚æœå…³é”®æ•°æ®ç¼ºå¤±ï¼Œè®°å½•ç¼ºå¤±ä½†ä¸ç”Ÿæˆè™šå‡æ•°æ®
        if not base_data:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•åŸºç¡€æ•°æ®ï¼Œç›¸å…³å˜é‡å°†æ ‡è®°ä¸ºç¼ºå¤±å€¼")
        
        self.base_data = base_data
        logger.info(f"âœ… åŸºç¡€æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(base_data)} ä¸ªæ•°æ®é›†")
        
        return base_data
    
    def construct_core_variables(self) -> Dict[str, pd.DataFrame]:
        """
        æ„å»ºæ ¸å¿ƒå˜é‡
        
        Returns:
            åŒ…å«æ ¸å¿ƒå˜é‡çš„å­—å…¸
        """
        logger.info("âš™ï¸ å¼€å§‹æ„å»ºæ ¸å¿ƒå˜é‡...")
        
        core_vars = {}
        
        # æ„å»º Node-DLI_US
        node_dli_us = self._construct_node_dli_us()
        if node_dli_us is not None:
            core_vars['node_dli_us'] = node_dli_us
        
        # æ„å»º Vul_US
        vul_us = self._construct_vul_us()
        if vul_us is not None:
            core_vars['vul_us'] = vul_us
        
        # æ„å»ºOVIï¼ˆæ–°ç‰ˆæ—¶é—´åºåˆ—ï¼‰
        try:
            builder = TimeSeriesOVIBuilder(self.temp_data_dir)
            gas_ovi, oil_ovi = builder.build_complete_ovi_timeseries()
            
            if gas_ovi is not None:
                core_vars['ovi_gas'] = gas_ovi
                logger.info(f"âœ… OVI_gas æ—¶é—´åºåˆ—å·²ç”Ÿæˆ: {len(gas_ovi)} è¡Œ")
            
            if oil_ovi is not None:
                core_vars['ovi_oil'] = oil_ovi
                logger.info(f"âœ… OVI_oil æ—¶é—´åºåˆ—å·²ç”Ÿæˆ: {len(oil_ovi)} è¡Œ")
                
        except Exception as e:
            logger.error(f"âŒ OVI æ—¶é—´åºåˆ—æ„å»ºå¤±è´¥: {e}")
        
        # æ„å»º US_ProdShock
        us_prod_shock = self._construct_us_prod_shock()
        if us_prod_shock is not None:
            core_vars['us_prod_shock'] = us_prod_shock
        
        self.core_variables = core_vars
        logger.info(f"âœ… æ ¸å¿ƒå˜é‡æ„å»ºå®Œæˆï¼Œå…± {len(core_vars)} ä¸ªå˜é‡")
        
        return core_vars
    
    def _construct_node_dli_us(self) -> Optional[pd.DataFrame]:
        """æ„å»º Node-DLI_US (ç¾å›½é”šå®šåŠ¨æ€é”å®šæŒ‡æ•°)"""
        logger.info("   æ„å»º Node-DLI_US...")
        
        try:
            if 'dli_panel' not in self.base_data or 'trade_flow' not in self.base_data:
                logger.warning("âš ï¸ ç¼ºå°‘DLIæˆ–è´¸æ˜“æ•°æ®ï¼Œè·³è¿‡Node-DLI_USæ„å»º")
                return None
            
            dli_data = self.base_data['dli_panel'].copy()
            trade_data = self.base_data['trade_flow'].copy()
            
            # ç­›é€‰ä¸ç¾å›½ç›¸å…³çš„è´¸æ˜“ (é€‚é…å®é™…æ•°æ®æ ¼å¼)
            us_trade = trade_data[
                (trade_data['reporter'] == 'USA') | 
                (trade_data['partner'] == 'USA')
            ].copy()
            
            if len(us_trade) == 0:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¾å›½ç›¸å…³è´¸æ˜“æ•°æ®")
                return None
            
            # è®¡ç®—è´¸æ˜“ä»½é¢ (é€‚é…å®é™…æ ¼å¼)
            us_trade['partner_country'] = np.where(
                us_trade['reporter'] == 'USA',
                us_trade['partner'],
                us_trade['reporter']
            )
            
            # ç¡®å®šç¾å›½è§’è‰² (Export æˆ– Import from ç¾å›½)
            us_trade['us_role'] = np.where(
                (us_trade['reporter'] == 'USA') & (us_trade['flow'] == 'X'),
                'exporter',
                'importer'
            )
            
            # è®¡ç®—çœŸå®çš„è¿›å£ä»½é¢
            logger.info("   è®¡ç®—çœŸå®è´¸æ˜“ä»½é¢...")
            
            # è®¡ç®—å„å›½æ€»è¿›å£é¢ï¼ˆä»æ‰€æœ‰å›½å®¶ï¼‰
            total_imports = trade_data[trade_data['flow'] == 'M'].groupby(['year', 'reporter']).agg({
                'trade_value_raw_usd': 'sum'
            }).reset_index()
            total_imports.columns = ['year', 'country', 'total_imports']
            
            # è®¡ç®—å„å›½ä»ç¾å›½çš„è¿›å£é¢
            us_imports = us_trade[
                (us_trade['partner'] == 'USA') & (us_trade['flow'] == 'M')
            ].groupby(['year', 'reporter']).agg({
                'trade_value_raw_usd': 'sum'
            }).reset_index()
            us_imports.columns = ['year', 'country', 'us_imports']
            
            # åˆå¹¶è®¡ç®—çœŸå®è¿›å£ä»½é¢
            trade_shares = total_imports.merge(us_imports, on=['year', 'country'], how='left')
            trade_shares['us_imports'] = trade_shares['us_imports'].fillna(0)
            trade_shares['import_share_from_us'] = trade_shares['us_imports'] / trade_shares['total_imports']
            trade_shares['import_share_from_us'] = trade_shares['import_share_from_us'].fillna(0).clip(0, 1)
            
            logger.info(f"   è®¡ç®—äº† {len(trade_shares)} ä¸ªå›½å®¶-å¹´ä»½çš„çœŸå®è´¸æ˜“ä»½é¢")
            
            # åŸºäºçœŸå®DLIæ•°æ®æ„å»ºNode-DLI_US
            logger.info("   åŸºäºçœŸå®DLIæ•°æ®æ„å»ºNode-DLI_US...")
            node_dli_records = []
            
            for _, trade_row in trade_shares.iterrows():
                year = trade_row['year']
                country = trade_row['country']
                s_imp = trade_row['import_share_from_us']
                
                # æŸ¥æ‰¾å¯¹åº”çš„DLIæ•°æ®
                # DLI_{US->i,t}: ç¾å›½å‡ºå£åˆ°è¯¥å›½çš„é”å®šæŒ‡æ•°
                dli_us_to_i = dli_data[
                    (dli_data['year'] == year) &
                    (dli_data['us_partner'] == country) &
                    (dli_data['us_role'] == 'exporter')
                ]['dli_score_adjusted'].mean()
                
                # DLI_{i->US,t}: è¯¥å›½å‡ºå£åˆ°ç¾å›½çš„é”å®šæŒ‡æ•°  
                dli_i_to_us = dli_data[
                    (dli_data['year'] == year) &
                    (dli_data['us_partner'] == country) &
                    (dli_data['us_role'] == 'importer')
                ]['dli_score_adjusted'].mean()
                
                # åº”ç”¨Node-DLIå…¬å¼
                if pd.isna(dli_us_to_i):
                    dli_us_to_i = 0
                if pd.isna(dli_i_to_us):
                    dli_i_to_us = 0
                
                # NodeDLI^US_{i,t} = s^{imp}_{i,US,t} Ã— DLI_{USâ†’i,t} + (1-s^{imp}_{i,US,t}) Ã— DLI_{iâ†’US,t}
                node_dli_us = s_imp * dli_us_to_i + (1 - s_imp) * dli_i_to_us
                
                node_dli_records.append({
                    'year': year,
                    'country': country,
                    'node_dli_us': node_dli_us,
                    'import_share_from_us': s_imp,
                    'dli_us_to_i': dli_us_to_i,
                    'dli_i_to_us': dli_i_to_us,
                    'us_imports': trade_row['us_imports'],
                    'total_imports': trade_row['total_imports']
                })
            
            node_dli_df = pd.DataFrame(node_dli_records)
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            non_zero_dli = node_dli_df[node_dli_df['node_dli_us'] > 0]
            logger.info(f"   æœ‰æ•ˆDLIè®°å½•: {len(non_zero_dli)}/{len(node_dli_df)}")
            logger.info(f"   Node-DLI_USèŒƒå›´: {node_dli_df['node_dli_us'].min():.3f} - {node_dli_df['node_dli_us'].max():.3f}")
            logger.info(f"   å¹³å‡è¿›å£ä»½é¢: {node_dli_df['import_share_from_us'].mean():.3f}")
            
            # åªä¿ç•™æ ¸å¿ƒå˜é‡ç”¨äºåˆå¹¶
            final_node_dli = node_dli_df[['year', 'country', 'node_dli_us', 'import_share_from_us']].copy()
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            output_path = self.temp_data_dir / "node_dli_us.csv"
            node_dli_df.to_csv(output_path, index=False)
            
            logger.info(f"âœ… çœŸå®Node-DLI_USæ„å»ºå®Œæˆ: {len(final_node_dli)} è¡Œè®°å½•")
            return final_node_dli
            
        except Exception as e:
            logger.error(f"âŒ Node-DLI_USæ„å»ºå¤±è´¥: {str(e)}")
            return None
    
    def _construct_vul_us(self) -> Optional[pd.DataFrame]:
        """æ„å»º Vul_US (ç¾å›½é”šå®šè„†å¼±æ€§æŒ‡æ•°)"""
        logger.info("   æ„å»º Vul_US...")
        
        try:
            if 'trade_flow' not in self.base_data:
                logger.warning("âš ï¸ ç¼ºå°‘è´¸æ˜“æ•°æ®ï¼Œè·³è¿‡Vul_USæ„å»º")
                return None
            
            trade_data = self.base_data['trade_flow'].copy()
            
            # è®¡ç®—å„å›½çš„è¿›å£ä¾èµ–åº¦å’Œå¤šæ ·åŒ–ç¨‹åº¦ (é€‚é…å®é™…æ ¼å¼)
            import_data = trade_data[trade_data['flow'] == 'M'].copy()  # åªè¦è¿›å£æ•°æ®
            import_data = import_data.groupby(['year', 'reporter', 'partner']).agg({
                'trade_value_raw_usd': 'sum'
            }).reset_index()
            
            # è®¡ç®—HHIæŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            total_imports = import_data.groupby(['year', 'reporter']).agg({
                'trade_value_raw_usd': 'sum'
            }).reset_index().rename(columns={'trade_value_raw_usd': 'total_imports'})
            
            import_data = import_data.merge(total_imports, on=['year', 'reporter'])
            import_data['import_share'] = import_data['trade_value_raw_usd'] / import_data['total_imports']
            
            # è®¡ç®—HHI
            hhi_data = import_data.groupby(['year', 'reporter']).apply(
                lambda x: (x['import_share'] ** 2).sum()
            ).reset_index(name='hhi_imports')
            
            # è®¡ç®—å¯¹ç¾ä¾èµ–åº¦
            us_imports = import_data[import_data['partner'] == 'USA'].copy()
            us_imports = us_imports.rename(columns={
                'import_share': 'us_import_share',
                'reporter': 'country'
            })[['year', 'country', 'us_import_share']]
            
            # åˆå¹¶æ•°æ®è®¡ç®—Vul_US
            vul_data = hhi_data.merge(us_imports, left_on=['year', 'reporter'], 
                                    right_on=['year', 'country'], how='left')
            
            vul_data['us_import_share'] = vul_data['us_import_share'].fillna(0)
            vul_data['vul_us'] = vul_data['us_import_share'] * vul_data['hhi_imports']
            
            vul_df = vul_data[['year', 'country', 'vul_us', 'us_import_share', 'hhi_imports']].copy()
            vul_df = vul_df.dropna()
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            output_path = self.temp_data_dir / "vul_us.csv"
            vul_df.to_csv(output_path, index=False)
            
            logger.info(f"âœ… Vul_USæ„å»ºå®Œæˆ: {len(vul_df)} è¡Œè®°å½•")
            return vul_df
            
        except Exception as e:
            logger.error(f"âŒ Vul_USæ„å»ºå¤±è´¥: {str(e)}")
            return None

    
    def _construct_us_prod_shock(self) -> Optional[pd.DataFrame]:
        """æ„å»ºç»¼åˆäº§é‡å†²å‡»æŒ‡æ•° (åŸæ²¹+å¤©ç„¶æ°”)"""
        logger.info("   æ„å»ºç»¼åˆUS_ProdShockï¼ˆåŸæ²¹+å¤©ç„¶æ°”ï¼‰...")
        
        try:
            if not HAS_REQUESTS:
                logger.warning("âš ï¸ requestsåº“ä¸å¯ç”¨ï¼Œæ— æ³•è·å–EIAæ•°æ®")
                return None
            
            # ä½¿ç”¨æä¾›çš„EIA APIå¯†é’¥
            eia_api_key = "kCKMXECZ7EZxHpYPXekyOhSdccpNc85aeOpDGIwm"
            logger.info(f"   ä½¿ç”¨EIA API Key: {eia_api_key[:8]}...")
            
            # ç¬¬1æ­¥ï¼šè·å–ç¾å›½åŸæ²¹äº§é‡æ•°æ®
            logger.info("   è·å–ç¾å›½åŸæ²¹äº§é‡æ•°æ®...")
            oil_url = "https://api.eia.gov/v2/petroleum/crd/crpdn/data/"
            oil_params = {
                'api_key': eia_api_key,
                'frequency': 'annual',
                'data[0]': 'value',
                'start': '2000',
                'end': '2023',
                'length': 1000
            }
            
            oil_response = requests.get(oil_url, params=oil_params, timeout=30)
            oil_data = None
            if oil_response.status_code == 200:
                oil_json = oil_response.json()
                if 'response' in oil_json and 'data' in oil_json['response']:
                    oil_df = pd.DataFrame(oil_json['response']['data'])
                    # ç­›é€‰ç¾å›½æ•°æ®
                    us_oil = oil_df[oil_df['area-name'].str.contains('USA', na=False)].copy()
                    us_oil['year'] = us_oil['period'].astype(int)
                    us_oil['value'] = pd.to_numeric(us_oil['value'], errors='coerce')
                    oil_data = us_oil.groupby('year')['value'].sum().reset_index()
                    oil_data.columns = ['year', 'us_production_oil']
                    logger.info(f"   åŸæ²¹æ•°æ®: {len(oil_data)} å¹´ï¼ŒèŒƒå›´ {oil_data['year'].min()}-{oil_data['year'].max()}")
            
            # ç¬¬2æ­¥ï¼šè·å–ç¾å›½å¤©ç„¶æ°”äº§é‡æ•°æ®
            logger.info("   è·å–ç¾å›½å¤©ç„¶æ°”äº§é‡æ•°æ®...")
            gas_url = "https://api.eia.gov/v2/natural-gas/prod/sum/data/"
            gas_params = {
                'api_key': eia_api_key,
                'frequency': 'annual',
                'data[0]': 'value',
                'start': '2000',
                'end': '2023',
                'length': 500
            }
            
            gas_response = requests.get(gas_url, params=gas_params, timeout=30)
            gas_data = None
            if gas_response.status_code == 200:
                gas_json = gas_response.json()
                if 'response' in gas_json and 'data' in gas_json['response']:
                    gas_df = pd.DataFrame(gas_json['response']['data'])
                    gas_df['year'] = gas_df['period'].astype(int)
                    gas_df['value'] = pd.to_numeric(gas_df['value'], errors='coerce')
                    gas_data = gas_df.groupby('year')['value'].sum().reset_index()
                    gas_data.columns = ['year', 'us_production_gas']
                    logger.info(f"   å¤©ç„¶æ°”æ•°æ®: {len(gas_data)} å¹´ï¼ŒèŒƒå›´ {gas_data['year'].min()}-{gas_data['year'].max()}")
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–ä¸¤ç§æ•°æ®
            if oil_data is None or gas_data is None:
                logger.warning("âš ï¸ EIA APIæ•°æ®è·å–ä¸å®Œæ•´ï¼ŒUS_ProdShockå˜é‡å°†æ ‡è®°ä¸ºç¼ºå¤±å€¼")
                return None
            
            # ç¬¬3æ­¥ï¼šåˆå¹¶æ•°æ®
            combined_data = oil_data.merge(gas_data, on='year', how='outer').sort_values('year')
            combined_data = combined_data.dropna()
            
            if len(combined_data) < 10:
                logger.warning("âš ï¸ åˆå¹¶åæ•°æ®ç‚¹è¿‡å°‘ï¼Œæ— æ³•è®¡ç®—å¯é çš„HPæ»¤æ³¢å†²å‡»")
                return None
            
            # ç¬¬4æ­¥ï¼šåˆ†åˆ«è®¡ç®—HPæ»¤æ³¢å†²å‡»
            logger.info("   è®¡ç®—HPæ»¤æ³¢å†²å‡»...")
            if HAS_FILTERING:
                # åŸæ²¹å†²å‡»
                oil_cycle, oil_trend = hpfilter(combined_data['us_production_oil'].values, lamb=100)
                # å¤©ç„¶æ°”å†²å‡»
                gas_cycle, gas_trend = hpfilter(combined_data['us_production_gas'].values, lamb=100)
            else:
                # ç®€åŒ–å†²å‡»è®¡ç®—
                oil_cycle = (combined_data['us_production_oil'] - 
                           combined_data['us_production_oil'].rolling(3).mean()).fillna(0).values
                gas_cycle = (combined_data['us_production_gas'] - 
                           combined_data['us_production_gas'].rolling(3).mean()).fillna(0).values
            
            # ç¬¬5æ­¥ï¼šæ ‡å‡†åŒ–å†²å‡»åºåˆ—
            z_shock_oil = (oil_cycle - oil_cycle.mean()) / oil_cycle.std()
            z_shock_gas = (gas_cycle - gas_cycle.mean()) / gas_cycle.std()
            
            # ç¬¬6æ­¥ï¼šç­‰æƒé‡åˆæˆç»¼åˆå†²å‡»æŒ‡æ•°
            us_prod_shock = 0.5 * z_shock_oil + 0.5 * z_shock_gas
            
            # æ„å»ºæœ€ç»ˆæ•°æ®æ¡†
            shock_df = pd.DataFrame({
                'year': combined_data['year'].values,
                'us_production_oil': combined_data['us_production_oil'].values,
                'us_production_gas': combined_data['us_production_gas'].values,
                'us_prod_shock': us_prod_shock
            })
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            output_path = self.temp_data_dir / "us_prod_shock.csv"
            shock_df.to_csv(output_path, index=False)
            
            logger.info(f"âœ… ç»¼åˆUS_ProdShockæ„å»ºå®Œæˆ: {len(shock_df)} å¹´æ•°æ®")
            logger.info(f"   åŸæ²¹äº§é‡èŒƒå›´: {shock_df['us_production_oil'].min():.0f} - {shock_df['us_production_oil'].max():.0f}")
            logger.info(f"   å¤©ç„¶æ°”äº§é‡èŒƒå›´: {shock_df['us_production_gas'].min():.0f} - {shock_df['us_production_gas'].max():.0f}")
            logger.info(f"   ç»¼åˆå†²å‡»èŒƒå›´: {shock_df['us_prod_shock'].min():.3f} - {shock_df['us_prod_shock'].max():.3f}")
            
            return shock_df
                
        except Exception as e:
            logger.error(f"âŒ ç»¼åˆUS_ProdShockæ„å»ºå¤±è´¥: {str(e)}")
            return None
    
    def create_analytical_panel(self) -> pd.DataFrame:
        """
        åˆ›å»ºæœ€ç»ˆåˆ†æé¢æ¿
        
        Returns:
            æ•´åˆåçš„åˆ†æé¢æ¿DataFrame
        """
        logger.info("ğŸ”— å¼€å§‹åˆ›å»ºæœ€ç»ˆåˆ†æé¢æ¿...")
        
        # æ£€æŸ¥å¿…è¦æ•°æ®å¹¶æ„å»ºåŸºç¡€é¢æ¿
        if self.macro_data is not None:
            # ä»å®è§‚æ•°æ®å¼€å§‹æ„å»ºé¢æ¿
            panel = self.macro_data.copy()
            
            # æ·»åŠ countryåˆ—ï¼ˆä»country_nameæå–ISO3ä»£ç ï¼‰
            if 'country_name' in panel.columns and 'country' not in panel.columns:
                panel['country'] = panel['country_name'].str[:3].str.upper()
                logger.info(f"   ä»country_nameæå–countryåˆ—: {panel['country'].nunique()} ä¸ªå›½å®¶")
            
            logger.info(f"   åŸºäºå®è§‚æ•°æ®æ„å»ºèµ·å§‹é¢æ¿: {len(panel)} è¡Œ")
        else:
            # å®è§‚æ•°æ®ç¼ºå¤±ï¼Œåˆ›å»ºåŸºç¡€æ¡†æ¶é¢æ¿
            logger.warning("âš ï¸ å®è§‚æ•°æ®ç¼ºå¤±ï¼Œåˆ›å»ºåŸºç¡€å›½å®¶-å¹´ä»½é¢æ¿æ¡†æ¶")
            countries = self._get_country_list()
            years = list(range(2000, 2025))
            
            # åˆ›å»ºå›½å®¶-å¹´ä»½ç¬›å¡å°”ç§¯
            country_year_pairs = []
            for country in countries:
                for year in years:
                    country_year_pairs.append({'country': country, 'year': year})
            
            panel = pd.DataFrame(country_year_pairs)
            logger.info(f"   åˆ›å»ºåŸºç¡€é¢æ¿æ¡†æ¶: {len(panel)} è¡Œ")
        
        # æ ‡å‡†åŒ–å›½å®¶åç§°åˆ—
        if 'country_name' in panel.columns:
            panel['country'] = panel['country_name']
        
        logger.info(f"   èµ·å§‹é¢æ¿: {len(panel)} è¡Œ")
        
        # é€æ­¥åˆå¹¶å…¶ä»–æ•°æ®
        merge_count = 0
        
        # åˆå¹¶æ ¸å¿ƒå˜é‡
        for var_name, var_data in self.core_variables.items():
            if var_data is not None and len(var_data) > 0:
                try:
                    before_len = len(panel)
                    
                    # ç‰¹æ®Šå¤„ç†US_ProdShock - åªæœ‰å¹´ä»½æ•°æ®ï¼Œéœ€è¦ä¸ºæ‰€æœ‰å›½å®¶å¤åˆ¶
                    if var_name == 'us_prod_shock':
                        # US_ProdShockæ•°æ®åªæœ‰å¹´ä»½ï¼Œä¸ºæ‰€æœ‰å›½å®¶å¤åˆ¶
                        panel = panel.merge(var_data, on='year', how='left')
                    else:
                        # å…¶ä»–å˜é‡æŒ‰yearå’Œcountryåˆå¹¶
                        panel = panel.merge(var_data, on=['year', 'country'], how='left')
                    
                    after_len = len(panel)
                    
                    if after_len == before_len:
                        merge_count += 1
                        logger.info(f"   âœ… åˆå¹¶ {var_name}: {len(var_data)} è¡Œ")
                    else:
                        logger.warning(f"   âš ï¸ åˆå¹¶ {var_name} æ”¹å˜äº†é¢æ¿è¡Œæ•°: {before_len} -> {after_len}")
                        
                except Exception as e:
                    logger.warning(f"   âŒ æ— æ³•åˆå¹¶ {var_name}: {str(e)}")
        
        # åˆå¹¶ç½‘ç»œæŒ‡æ ‡
        if 'node_metrics' in self.base_data:
            try:
                node_metrics = self.base_data['node_metrics'].copy()
                
                # æ ‡å‡†åŒ–åˆ—å - å¦‚æœæœ‰country_codeï¼Œé‡å‘½åä¸ºcountry
                if 'country_code' in node_metrics.columns:
                    node_metrics['country'] = node_metrics['country_code']
                
                before_len = len(panel)
                panel = panel.merge(node_metrics, on=['year', 'country'], how='left')
                after_len = len(panel)
                
                if after_len == before_len:
                    merge_count += 1
                    logger.info(f"   âœ… åˆå¹¶ç½‘ç»œæŒ‡æ ‡: {len(node_metrics)} è¡Œ")
                else:
                    logger.warning(f"   âš ï¸ åˆå¹¶ç½‘ç»œæŒ‡æ ‡æ”¹å˜äº†é¢æ¿è¡Œæ•°: {before_len} -> {after_len}")
                    
            except Exception as e:
                logger.warning(f"   âŒ æ— æ³•åˆå¹¶ç½‘ç»œæŒ‡æ ‡: {str(e)}")
        
        # æ•°æ®æ¸…æ´—å’Œæœ€ç»ˆå¤„ç†
        panel = self._clean_final_panel(panel)
        
        # ä¿å­˜æœ€ç»ˆé¢æ¿
        output_path = self.base_dir / "data" / "processed_data" / "analytical_panel.csv"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        panel.to_csv(output_path, index=False)
        
        # åŒæ—¶ä¿å­˜åˆ°æ¨¡å—è¾“å‡ºç›®å½•
        module_output_path = self.output_dir / "analytical_panel.csv"
        panel.to_csv(module_output_path, index=False)
        
        self.final_panel = panel
        
        logger.info(f"âœ… æœ€ç»ˆåˆ†æé¢æ¿åˆ›å»ºå®Œæˆ:")
        logger.info(f"   è¡Œæ•°: {len(panel)}")
        logger.info(f"   åˆ—æ•°: {len(panel.columns)}")
        logger.info(f"   å¹´ä»½èŒƒå›´: {panel['year'].min()}-{panel['year'].max()}")
        logger.info(f"   å›½å®¶æ•°é‡: {panel['country'].nunique()}")
        logger.info(f"   æˆåŠŸåˆå¹¶: {merge_count} ä¸ªæ•°æ®é›†")
        logger.info(f"   ä¿å­˜è‡³: {output_path}")
        
        return panel
    
    def _clean_final_panel(self, panel: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æœ€ç»ˆé¢æ¿æ•°æ®"""
        logger.info("   æ¸…æ´—æœ€ç»ˆé¢æ¿æ•°æ®...")
        
        # åˆ é™¤é‡å¤è¡Œ
        initial_len = len(panel)
        panel = panel.drop_duplicates(subset=['year', 'country'])
        if len(panel) != initial_len:
            logger.info(f"   åˆ é™¤é‡å¤è¡Œ: {initial_len} -> {len(panel)}")
        
        # ç¡®ä¿å¹´ä»½ä¸ºæ•´æ•°
        panel['year'] = panel['year'].astype(int)
        
        # é™åˆ¶å¹´ä»½èŒƒå›´
        panel = panel[(panel['year'] >= 2000) & (panel['year'] <= 2024)]
        
        # æ¸…ç†æ— æ•ˆå€¼
        numeric_columns = panel.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            # æ›¿æ¢æ— ç©·å¤§å€¼ä¸ºNaN
            panel[col] = panel[col].replace([np.inf, -np.inf], np.nan)
        
        # æŒ‰å¹´ä»½å’Œå›½å®¶æ’åº
        panel = panel.sort_values(['year', 'country']).reset_index(drop=True)
        
        return panel
    
    def create_data_dictionary(self) -> None:
        """åˆ›å»ºæ•°æ®å­—å…¸"""
        logger.info("ğŸ“– åˆ›å»ºæ•°æ®å­—å…¸...")
        
        if self.final_panel is None:
            logger.warning("âš ï¸ æœ€ç»ˆé¢æ¿æœªåˆ›å»ºï¼Œæ— æ³•ç”Ÿæˆæ•°æ®å­—å…¸")
            return
        
        # æ„å»ºæ•°æ®å­—å…¸å†…å®¹
        dictionary_content = f"""# åˆ†æé¢æ¿æ•°æ®å­—å…¸
## Analytical Panel Data Dictionary

**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æ¨¡å—**: 08_variable_construction v1.0  
**æ€»è¡Œæ•°**: {len(self.final_panel):,}  
**æ€»åˆ—æ•°**: {len(self.final_panel.columns)}  
**å¹´ä»½èŒƒå›´**: {self.final_panel['year'].min()}-{self.final_panel['year'].max()}  
**å›½å®¶æ•°é‡**: {self.final_panel['country'].nunique()}

---

## å˜é‡è¯¦ç»†è¯´æ˜

### åŸºç¡€æ ‡è¯†å˜é‡
"""
        
        # åŸºç¡€å˜é‡
        basic_vars = {
            'year': 'å¹´ä»½ (æ•´æ•°)',
            'country': 'å›½å®¶ISO3ä»£ç  (å­—ç¬¦ä¸²)',
            'country_name': 'å›½å®¶å…¨å (å­—ç¬¦ä¸²ï¼Œæ¥æºï¼šWorld Bank)'
        }
        
        for var, desc in basic_vars.items():
            if var in self.final_panel.columns:
                dictionary_content += f"- **{var}**: {desc}\n"
        
        # å®è§‚ç»æµå˜é‡
        dictionary_content += "\n### å®è§‚ç»æµæ§åˆ¶å˜é‡ (æ¥æºï¼šWorld Bank WDI API)\n"
        macro_vars = {
            'gdp_current_usd': 'GDPï¼Œç°ä»·ç¾å…ƒ (NY.GDP.MKTP.CD)',
            'population_total': 'æ€»äººå£æ•° (SP.POP.TOTL)',
            'trade_openness_gdp_pct': 'è´¸æ˜“å¼€æ”¾åº¦ï¼Œå GDPç™¾åˆ†æ¯” (NE.TRD.GNFS.ZS)',
            'log_gdp': 'GDPçš„è‡ªç„¶å¯¹æ•°',
            'log_population': 'äººå£çš„è‡ªç„¶å¯¹æ•°'
        }
        
        for var, desc in macro_vars.items():
            if var in self.final_panel.columns:
                dictionary_content += f"- **{var}**: {desc}\n"
        
        # æ ¸å¿ƒç ”ç©¶å˜é‡
        dictionary_content += "\n### æ ¸å¿ƒç ”ç©¶å˜é‡ (æœ¬æ¨¡å—æ„å»º)\n"
        core_vars = {
            'node_dli_us': 'Node-DLI_US: ç¾å›½é”šå®šåŠ¨æ€é”å®šæŒ‡æ•°ï¼ŒåŸºäº04_dli_analysisçš„è¾¹çº§DLIèšåˆ',
            'vul_us': 'Vul_US: ç¾å›½é”šå®šè„†å¼±æ€§æŒ‡æ•°ï¼ŒåŸºäºè¿›å£ä»½é¢Ã—HHIæŒ‡æ•°',
            'ovi_gas': 'OVI_gas: å¤©ç„¶æ°”ç‰©ç†å†—ä½™æŒ‡æ•° (ä¸»æŒ‡æ ‡)ï¼ŒåŸºäºLNGæ¥æ”¶ç«™å’Œç®¡é“å®¹é‡',
            'ovi_oil': 'OVI_oil: çŸ³æ²¹ç‰©ç†å†—ä½™æŒ‡æ•° (ç¨³å¥æ€§æ£€éªŒæŒ‡æ ‡)ï¼ŒåŸºäºç‚¼æ²¹å‚å’Œç®¡é“å®¹é‡',
            'us_prod_shock': 'US_ProdShock: ç¾å›½é¡µå²©æ²¹æ°”äº§é‡å†²å‡»ï¼ŒHPæ»¤æ³¢åçš„å‘¨æœŸæˆåˆ†'
        }
        
        for var, desc in core_vars.items():
            if var in self.final_panel.columns:
                dictionary_content += f"- **{var}**: {desc}\n"
        
        # ç½‘ç»œæŒ‡æ ‡å˜é‡
        dictionary_content += "\n### ç½‘ç»œæ‹“æ‰‘æŒ‡æ ‡ (æ¥æºï¼š03_metrics)\n"
        network_vars = {
            'betweenness_centrality': 'ä»‹æ•°ä¸­å¿ƒæ€§ï¼Œè¡¡é‡èŠ‚ç‚¹åœ¨ç½‘ç»œä¸­çš„æ¡¥æ¢ä½œç”¨',
            'closeness_centrality': 'æ¥è¿‘ä¸­å¿ƒæ€§ï¼Œè¡¡é‡èŠ‚ç‚¹åˆ°å…¶ä»–èŠ‚ç‚¹çš„å¹³å‡è·ç¦»',
            'eigenvector_centrality': 'ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§ï¼Œè€ƒè™‘é‚»å±…é‡è¦æ€§çš„ä¸­å¿ƒæ€§',
            'degree_centrality': 'åº¦ä¸­å¿ƒæ€§ï¼Œè¡¡é‡èŠ‚ç‚¹çš„è¿æ¥æ•°é‡'
        }
        
        for var, desc in network_vars.items():
            if var in self.final_panel.columns:
                dictionary_content += f"- **{var}**: {desc}\n"
        
        # ç»Ÿè®¡æ‘˜è¦
        dictionary_content += "\n---\n\n## æ•°æ®è´¨é‡æ‘˜è¦\n\n"
        
        # ç¼ºå¤±å€¼ç»Ÿè®¡
        missing_stats = self.final_panel.isnull().sum()
        missing_pct = (missing_stats / len(self.final_panel) * 100).round(2)
        
        dictionary_content += "### ç¼ºå¤±å€¼ç»Ÿè®¡\n\n"
        dictionary_content += "| å˜é‡å | ç¼ºå¤±å€¼æ•°é‡ | ç¼ºå¤±ç‡(%) |\n"
        dictionary_content += "|--------|------------|----------|\n"
        
        for var in self.final_panel.columns:
            if missing_stats[var] > 0:
                dictionary_content += f"| {var} | {missing_stats[var]} | {missing_pct[var]}% |\n"
        
        # æ•°å€¼å˜é‡ç»Ÿè®¡
        numeric_cols = self.final_panel.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            dictionary_content += "\n### æ•°å€¼å˜é‡åŸºç¡€ç»Ÿè®¡\n\n"
            stats_df = self.final_panel[numeric_cols].describe()
            dictionary_content += stats_df.round(4).to_markdown()
        
        # æ•°æ®æ¥æºè¯´æ˜
        dictionary_content += "\n\n---\n\n## æ•°æ®æ¥æºä¸æ„å»ºæ–¹æ³•\n\n"
        dictionary_content += """
1. **å®è§‚ç»æµæ•°æ®**: é€šè¿‡wbdataåŒ…ä»ä¸–ç•Œé“¶è¡ŒWDIæ•°æ®åº“è·å–
2. **è´¸æ˜“ç½‘ç»œæ•°æ®**: åŸºäº01_data_processingæ¨¡å—çš„æ¸…æ´—è´¸æ˜“æµæ•°æ®
3. **ç½‘ç»œæ‹“æ‰‘æŒ‡æ ‡**: åŸºäº03_metricsæ¨¡å—è®¡ç®—çš„ä¸­å¿ƒæ€§æŒ‡æ ‡
4. **DLIæŒ‡æ ‡**: åŸºäº04_dli_analysisæ¨¡å—çš„è¾¹çº§åŠ¨æ€é”å®šæŒ‡æ•°
5. **ç‰©ç†åŸºç¡€è®¾æ–½æ•°æ®**: æ‰‹åŠ¨æ”¶é›†çš„LNGæ¥æ”¶ç«™å’Œç®¡é“å®¹é‡æ•°æ®
6. **ç¾å›½äº§é‡æ•°æ®**: é€šè¿‡EIA APIè·å–çš„ç¾å›½çŸ³æ²¹å¤©ç„¶æ°”äº§é‡æ•°æ®

## ä½¿ç”¨å»ºè®®

1. **å› å˜é‡é€‰æ‹©**: å»ºè®®ä½¿ç”¨vul_usä½œä¸ºä¸»è¦çš„è„†å¼±æ€§æŒ‡æ ‡
2. **è§£é‡Šå˜é‡**: node_dli_uså’Œoviæ˜¯æ ¸å¿ƒè§£é‡Šå˜é‡
3. **æ§åˆ¶å˜é‡**: å»ºè®®æ§åˆ¶log_gdp, log_population, trade_openness_gdp_pct
4. **å·¥å…·å˜é‡**: us_prod_shockå¯ä½œä¸ºå¤–ç”Ÿå†²å‡»çš„å·¥å…·å˜é‡
5. **ç½‘ç»œæ§åˆ¶**: å¯åŠ å…¥ç½‘ç»œä¸­å¿ƒæ€§æŒ‡æ ‡ä½œä¸ºé¢å¤–æ§åˆ¶

---

*æœ¬æ•°æ®å­—å…¸ç”±08_variable_constructionæ¨¡å—è‡ªåŠ¨ç”Ÿæˆ*  
*Energy Network Analysis Team*
"""
        
        # ä¿å­˜æ•°æ®å­—å…¸
        dict_path = self.output_dir / "data_dictionary.md"
        with open(dict_path, 'w', encoding='utf-8') as f:
            f.write(dictionary_content)
        
        logger.info(f"âœ… æ•°æ®å­—å…¸åˆ›å»ºå®Œæˆ: {dict_path}")
    
    def run_full_pipeline(self) -> None:
        """è¿è¡Œå®Œæ•´çš„å˜é‡æ„å»ºæµæ°´çº¿"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„å˜é‡æ„å»ºæµæ°´çº¿...")
        
        try:
            # æ­¥éª¤1: æœé›†å®è§‚æ§åˆ¶å˜é‡
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤1: æœé›†å®è§‚ç»æµæ§åˆ¶å˜é‡")
            logger.info("="*50)
            self.fetch_macro_controls()
            
            # æ­¥éª¤2: åŠ è½½åŸºç¡€æ•°æ®
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤2: åŠ è½½åŸºç¡€æ•°æ®")
            logger.info("="*50)
            self.load_base_data()
            
            # æ­¥éª¤3: æ„å»ºæ ¸å¿ƒå˜é‡
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤3: æ„å»ºæ ¸å¿ƒå˜é‡")
            logger.info("="*50)
            self.construct_core_variables()
            
            # æ­¥éª¤4: åˆ›å»ºæœ€ç»ˆé¢æ¿
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤4: åˆ›å»ºæœ€ç»ˆåˆ†æé¢æ¿")
            logger.info("="*50)
            self.create_analytical_panel()
            
            # æ­¥éª¤5: ç”Ÿæˆæ•°æ®å­—å…¸
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤5: ç”Ÿæˆæ•°æ®å­—å…¸")
            logger.info("="*50)
            self.create_data_dictionary()
            
            logger.info("\n" + "="*60)
            logger.info("ğŸ‰ å˜é‡æ„å»ºæµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
            logger.info("="*60)
            logger.info(f"âœ… æœ€ç»ˆè¾“å‡º:")
            logger.info(f"   - åˆ†æé¢æ¿: analytical_panel.csv ({len(self.final_panel)} è¡Œ)")
            logger.info(f"   - æ•°æ®å­—å…¸: data_dictionary.md")
            logger.info(f"   - ä¸­é—´æ–‡ä»¶: {self.temp_data_dir}")
            logger.info(f"   - è¾“å‡ºç›®å½•: {self.output_dir}")
            
        except Exception as e:
            logger.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå˜é‡æ„å»ºæµæ°´çº¿"""
    print("ğŸ—ï¸ 08_variable_construction - è¶…çº§æ•°æ®å·¥å‚")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–å˜é‡æ„å»ºå™¨
        constructor = VariableConstructor()
        
        # è¿è¡Œå®Œæ•´æµæ°´çº¿
        constructor.run_full_pipeline()
        
        print("\nâœ… å˜é‡æ„å»ºæ¨¡å—æ‰§è¡ŒæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶:")
        print(f"   - {constructor.output_dir / 'analytical_panel.csv'}")
        print(f"   - {constructor.output_dir / 'data_dictionary.md'}")
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    main()