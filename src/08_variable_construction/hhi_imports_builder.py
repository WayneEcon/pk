"""
HHI_imports æ„å»ºå™¨ - ç‹¬ç«‹è„šæœ¬
=====================================

è´Ÿè´£å¤„ç†HHI_importsï¼ˆè¿›å£å¤šæ ·åŒ–æŒ‡æ•°ï¼‰çš„åŠ è½½ã€æå–å’Œæ„å»ºé€»è¾‘ã€‚
ä»05æ¨¡å—è½¬ç§»è‡³08æ¨¡å—ï¼Œä¿æŒä»£ç ç»“æ„æ¸…æ™°ã€‚

åŠŸèƒ½ï¼š
1. ä¼˜å…ˆä»05æ¨¡å—åŠ è½½å·²æ„å»ºçš„hhi_imports.csv
2. å¤‡ç”¨ï¼šä»æ—§çš„vul_us.csvä¸­æå–hhi_importsæ•°æ®  
3. ä¿å­˜æå–ç»“æœåˆ°outputs/hhi_imports_extracted.csv

Author: Energy Network Analysis Team
Date: 2025-08-22
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import Optional

logger = logging.getLogger(__name__)

class HHIImportsBuilder:
    """HHI_imports æ•°æ®æ„å»ºå™¨"""
    
    def __init__(self, base_dir: Path, output_dir: Path, temp_data_dir: Path):
        self.base_dir = base_dir
        self.output_dir = output_dir
        self.temp_data_dir = temp_data_dir
        
    def load_hhi_imports(self) -> Optional[pd.DataFrame]:
        """
        åŠ è½½æˆ–æ„å»ºHHI_importsæ•°æ®
        
        åŠ è½½ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä»05æ¨¡å—åŠ è½½å·²æ„å»ºçš„hhi_imports.csv
        2. å¤‡ç”¨ï¼šä»08dataç›®å½•çš„hhi_imports_extracted.csvåŠ è½½
        3. æœ€åï¼šä»æ—§çš„vul_us.csvä¸­æå–å¹¶ä¿å­˜
        
        Returns:
            pd.DataFrame or None: HHI_importsæ•°æ®
        """
        logger.info("ğŸ”„ å¼€å§‹åŠ è½½HHI_importsæ•°æ®...")
        
        # ç­–ç•¥1ï¼šä»05æ¨¡å—åŠ è½½å·²æ„å»ºçš„hhi_imports.csv
        hhi_imports_path = self.base_dir / "src" / "05_causal_validation" / "outputs" / "hhi_imports.csv"
        if hhi_imports_path.exists():
            try:
                hhi_imports_data = pd.read_csv(hhi_imports_path)
                logger.info(f"âœ… ä»05æ¨¡å—åŠ è½½HHI_importsæ•°æ®: {len(hhi_imports_data)} è¡Œ")
                logger.info("   âš ï¸ æ³¨æ„ï¼šå·²åºŸå¼ƒvul_usæ„é€ ï¼Œæ”¹ç”¨hhi_importsé¿å…æ„é€ å†…ç”Ÿæ€§")
                return hhi_imports_data
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•ä»05æ¨¡å—åŠ è½½HHI_importsæ•°æ®: {str(e)}")
        else:
            logger.info("â„¹ï¸ 05æ¨¡å—hhi_imports.csvä¸å­˜åœ¨ï¼Œå°è¯•å¤‡ç”¨ç­–ç•¥...")
        
        # ç­–ç•¥2ï¼šä»08dataç›®å½•åŠ è½½å·²æå–çš„hhi_imports_extracted.csv
        hhi_extracted_path = self.output_dir / "hhi_imports_extracted.csv"
        if hhi_extracted_path.exists():
            try:
                hhi_imports_data = pd.read_csv(hhi_extracted_path)
                logger.info(f"âœ… åŠ è½½æå–çš„HHI_importsæ•°æ®: {len(hhi_imports_data)} è¡Œ")
                return hhi_imports_data
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½æå–çš„HHI_importsæ•°æ®: {str(e)}")
        else:
            logger.info("â„¹ï¸ æå–çš„hhi_imports_extracted.csvä¸å­˜åœ¨ï¼Œå°è¯•ä»vul_us.csvæå–...")
        
        # ç­–ç•¥3ï¼šä»æ—§çš„vul_us.csvä¸­æå–hhi_imports
        return self._extract_from_vul_us()
    
    def _extract_from_vul_us(self) -> Optional[pd.DataFrame]:
        """
        ä»æ—§çš„vul_us.csvæ–‡ä»¶ä¸­æå–hhi_importsæ•°æ®
        
        Returns:
            pd.DataFrame or None: æå–çš„HHI_importsæ•°æ®
        """
        vul_us_path = self.temp_data_dir / "vul_us.csv"
        
        if not vul_us_path.exists():
            logger.warning("âš ï¸ vul_us.csvæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æå–HHI_importsæ•°æ®")
            return None
        
        try:
            logger.info("ğŸ”„ ä»vul_us.csvæå–HHI_importsæ•°æ®...")
            vul_us_data = pd.read_csv(vul_us_path)
            
            # éªŒè¯å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['year', 'country', 'hhi_imports']
            missing_columns = [col for col in required_columns if col not in vul_us_data.columns]
            if missing_columns:
                logger.error(f"âŒ vul_us.csvç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
                return None
            
            # æå–hhi_importsç›¸å…³åˆ—
            available_columns = ['year', 'country', 'hhi_imports']
            if 'us_import_share' in vul_us_data.columns:
                available_columns.append('us_import_share')
            
            hhi_imports_data = vul_us_data[available_columns].copy()
            
            # æ•°æ®æ¸…ç†
            hhi_imports_data = hhi_imports_data.dropna(subset=['hhi_imports'])
            
            # ä¿å­˜æå–çš„æ•°æ®åˆ°outputsç›®å½•
            hhi_extracted_path = self.output_dir / "hhi_imports_extracted.csv"
            hhi_imports_data.to_csv(hhi_extracted_path, index=False)
            
            logger.info(f"âœ… ä»vul_us.csvæå–HHI_imports: {len(hhi_imports_data)} è¡Œ")
            logger.info(f"   ğŸ’¾ å·²ä¿å­˜åˆ°: {hhi_extracted_path}")
            logger.info(f"   ğŸ“Š æ•°æ®æ¦‚å†µ:")
            logger.info(f"      - å›½å®¶æ•°: {hhi_imports_data['country'].nunique()}")
            logger.info(f"      - å¹´ä»½èŒƒå›´: {hhi_imports_data['year'].min()}-{hhi_imports_data['year'].max()}")
            logger.info(f"      - HHIèŒƒå›´: {hhi_imports_data['hhi_imports'].min():.4f}-{hhi_imports_data['hhi_imports'].max():.4f}")
            logger.warning("âš ï¸ ä½¿ç”¨æ—§æ•°æ®æºï¼Œå»ºè®®é‡æ–°è¿è¡Œ05æ¨¡å—ç”Ÿæˆæ–°çš„hhi_imports.csv")
            
            return hhi_imports_data
            
        except Exception as e:
            logger.error(f"âŒ ä»vul_us.csvæå–HHI_importså¤±è´¥: {str(e)}", exc_info=True)
            return None
    
    def construct_from_trade_data(self, trade_data: pd.DataFrame, output_path: Optional[Path] = None) -> Optional[pd.DataFrame]:
        """
        ä»è´¸æ˜“æ•°æ®ç›´æ¥æ„å»ºHHI_importsï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        
        è¿™æ˜¯ä»05æ¨¡å—ç§»æ¤è¿‡æ¥çš„æ„å»ºé€»è¾‘ï¼Œä½œä¸ºæœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            trade_data: è´¸æ˜“æµæ•°æ®
            output_path: è¾“å‡ºè·¯å¾„ï¼Œé»˜è®¤ä¸ºoutputs/hhi_imports_constructed.csv
            
        Returns:
            pd.DataFrame or None: æ„å»ºçš„HHI_importsæ•°æ®
        """
        logger.info("ğŸ”„ ä»è´¸æ˜“æ•°æ®æ„å»ºHHI_imports...")
        
        if output_path is None:
            output_path = self.output_dir / "hhi_imports_constructed.csv"
        
        try:
            # è®¡ç®—å„å›½çš„è¿›å£ä¾èµ–åº¦å’Œå¤šæ ·åŒ–ç¨‹åº¦
            import_data = trade_data[trade_data['flow'] == 'M'].copy()
            import_data = import_data.groupby(['year', 'reporter', 'partner']).agg(
                trade_value_raw_usd=('trade_value_raw_usd', 'sum')
            ).reset_index()
            
            # è®¡ç®—HHIæŒ‡æ•°ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰
            total_imports = import_data.groupby(['year', 'reporter']).agg(
                total_imports=('trade_value_raw_usd', 'sum')
            ).reset_index()
            
            import_data = import_data.merge(total_imports, on=['year', 'reporter'])
            import_data['import_share'] = import_data['trade_value_raw_usd'] / import_data['total_imports']
            
            hhi_data = import_data.groupby(['year', 'reporter']).apply(
                lambda x: (x['import_share'] ** 2).sum()
            ).reset_index(name='hhi_imports')
            
            # é‡å‘½åcountryåˆ—ä»¥ä¾¿åˆå¹¶
            hhi_data = hhi_data.rename(columns={'reporter': 'country'})
            
            # è®¡ç®—å¯¹ç¾ä¾èµ–åº¦ï¼ˆè¾…åŠ©å˜é‡ï¼‰
            us_imports = import_data[import_data['partner'] == 'USA'].copy()
            us_imports = us_imports.rename(columns={
                'import_share': 'us_import_share',
                'reporter': 'country'
            })[['year', 'country', 'us_import_share']]
            
            # åˆå¹¶æ•°æ®
            final_data = hhi_data.merge(us_imports, on=['year', 'country'], how='left')
            final_data['us_import_share'] = final_data['us_import_share'].fillna(0)
            
            # ç”Ÿæˆæœ€ç»ˆæ•°æ®
            hhi_df = final_data[['year', 'country', 'hhi_imports', 'us_import_share']].copy().dropna()
            
            # ä¿å­˜æ„å»ºçš„æ•°æ®
            hhi_df.to_csv(output_path, index=False)
            
            logger.info(f"âœ… HHI_importsæ„å»ºå®Œæˆ: {len(hhi_df)} è¡Œè®°å½•")
            logger.info(f"   ğŸ’¾ ä¿å­˜è‡³: {output_path}")
            logger.info(f"   ğŸ“Š hhi_importsèŒƒå›´: {hhi_df['hhi_imports'].min():.4f} - {hhi_df['hhi_imports'].max():.4f}")
            logger.info(f"   ğŸŒ è¦†ç›–å›½å®¶: {hhi_df['country'].nunique()} ä¸ª")
            logger.info(f"   ğŸ“… å¹´ä»½èŒƒå›´: {hhi_df['year'].min()}-{hhi_df['year'].max()}")
            
            return hhi_df
            
        except Exception as e:
            logger.error(f"âŒ ä»è´¸æ˜“æ•°æ®æ„å»ºHHI_importså¤±è´¥: {str(e)}", exc_info=True)
            return None

def main():
    """ç‹¬ç«‹è¿è¡Œæ—¶çš„æµ‹è¯•å‡½æ•°"""
    import sys
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # è®¾ç½®è·¯å¾„
    current_dir = Path(__file__).parent
    base_dir = current_dir.parent.parent  # project root
    output_dir = current_dir / "outputs"
    temp_data_dir = current_dir / "08data"
    
    # åˆ›å»ºæ„å»ºå™¨
    builder = HHIImportsBuilder(base_dir, output_dir, temp_data_dir)
    
    # æµ‹è¯•åŠ è½½
    result = builder.load_hhi_imports()
    
    if result is not None:
        print(f"\nâœ… æµ‹è¯•æˆåŠŸï¼åŠ è½½äº† {len(result)} è¡ŒHHI_importsæ•°æ®")
        print(f"   åˆ—å: {list(result.columns)}")
        print(f"   æ•°æ®é¢„è§ˆ:")
        print(result.head())
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šæœªèƒ½åŠ è½½HHI_importsæ•°æ®")
        
if __name__ == "__main__":
    main()