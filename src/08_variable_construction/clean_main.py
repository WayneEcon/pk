#!/usr/bin/env python3
"""
æ¸…æ™°ç‰ˆä¸»æ„å»ºå™¨ v3.0 - å›å½’åˆå¿ƒç‰ˆ
===============================

æ ¸å¿ƒç›®æ ‡:
æ ¹æ®research_outline.mdï¼Œæ„å»ºä»¥ä¸‹4ä¸ªæ ¸å¿ƒå˜é‡ï¼š
1. Node-DLI_US: ç¾å›½é”šå®šåŠ¨æ€é”å®šæŒ‡æ•° (æ¥è‡ª04_dli_analysis)
2. Vul_US: ç¾å›½é”šå®šè„†å¼±æ€§æŒ‡æ•° (åŸºäºè´¸æ˜“ä»½é¢Ã—HHI)
3. OVI: å¤©ç„¶æ°”ç‰©ç†å†—ä½™æŒ‡æ•° (LNG+ç®¡é“/æ¶ˆè´¹)
4. US_ProdShock: ç¾å›½äº§é‡å†²å‡» (é¡µå²©é©å‘½å¤–ç”Ÿå†²å‡»)

è¾“å‡º:
- analytical_panel.csv: å›½åˆ«-å¹´åº¦é¢æ¿æ•°æ® (500è¡ŒÃ—æ ¸å¿ƒå˜é‡)
- data_dictionary.md: æ•°æ®å­—å…¸

ä½œè€…: Energy Network Analysis Team  
ç‰ˆæœ¬: v3.0 - å›å½’åˆå¿ƒç‰ˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
import warnings
from typing import Dict, List, Tuple, Optional, Any
import sys
import os
import requests

# å¯¼å…¥ç®€åŒ–çš„å¤©ç„¶æ°”OVIæ„å»ºå™¨
from simple_gas_ovi_builder import SimpleGasOVIBuilder

warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('clean_variable_construction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CleanVariableConstructor:
    """æ¸…æ™°ç‰ˆå˜é‡æ„å»ºå™¨ - åªæ„å»ºç ”ç©¶å¤§çº²è¦æ±‚çš„æ ¸å¿ƒå˜é‡"""
    
    def __init__(self, base_dir: str = None):
        """åˆå§‹åŒ–æ„å»ºå™¨"""
        if base_dir is None:
            self.base_dir = Path(__file__).parent.parent.parent
        else:
            self.base_dir = Path(base_dir)
        
        self.data_dir = self.base_dir / "data"
        self.output_dir = Path(__file__).parent / "outputs"
        self.temp_data_dir = Path(__file__).parent / "08data"
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        self.temp_data_dir.mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.macro_data = None
        self.base_data = {}
        self.core_variables = {}
        self.final_panel = None
        
        logger.info("ğŸ—ï¸ æ¸…æ™°ç‰ˆå˜é‡æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   é¡¹ç›®æ ¹ç›®å½•: {self.base_dir}")
        logger.info(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_cached_macro_data(self) -> pd.DataFrame:
        """åŠ è½½ç¼“å­˜çš„å®è§‚æ•°æ®"""
        logger.info("ğŸŒ åŠ è½½å®è§‚ç»æµæ§åˆ¶å˜é‡...")
        
        cache_path = self.temp_data_dir / "macro_controls.csv"
        if cache_path.exists():
            try:
                macro_data = pd.read_csv(cache_path)
                macro_data['year'] = pd.to_datetime(macro_data['year']).dt.year
                
                logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½å®è§‚æ•°æ®: {len(macro_data)} è¡Œè®°å½•")
                logger.info(f"   æ•°æ®èŒƒå›´: {macro_data['year'].min()}-{macro_data['year'].max()}")
                
                self.macro_data = macro_data
                return macro_data
                
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        
        logger.warning("âš ï¸ æœªæ‰¾åˆ°å®è§‚æ•°æ®ç¼“å­˜ï¼Œå°†åˆ›å»ºåŸºç¡€æ¨¡æ¿")
        return None
    
    def load_base_data(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½åŸºç¡€æ•°æ® (01, 03, 04æ¨¡å—è¾“å‡º)"""
        logger.info("ğŸ“ å¼€å§‹åŠ è½½åŸºç¡€æ•°æ®...")
        
        base_data = {}
        
        # åŠ è½½01æ¨¡å—è¾“å‡º - è´¸æ˜“æµæ•°æ®
        trade_data_files = list((self.base_dir / "data" / "processed_data").glob("cleaned_energy_trade_*.csv"))
        
        if trade_data_files:
            try:
                trade_data_list = []
                for file_path in sorted(trade_data_files):
                    yearly_data = pd.read_csv(file_path)
                    trade_data_list.append(yearly_data)
                
                trade_data = pd.concat(trade_data_list, ignore_index=True)
                base_data['trade_flow'] = trade_data
                logger.info(f"âœ… åŠ è½½è´¸æ˜“æµæ•°æ®: åˆå¹¶ {len(trade_data_files)} ä¸ªæ–‡ä»¶ ({len(trade_data)} è¡Œ)")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½è´¸æ˜“æ•°æ®: {str(e)}")
        
        # åŠ è½½03æ¨¡å—è¾“å‡º - ç½‘ç»œæŒ‡æ ‡
        metrics_data_path = self.base_dir / "src" / "03_metrics"
        
        node_metrics_path = metrics_data_path / "node_centrality_metrics.csv"
        if node_metrics_path.exists():
            try:
                node_metrics = pd.read_csv(node_metrics_path)
                base_data['node_metrics'] = node_metrics
                logger.info(f"âœ… åŠ è½½èŠ‚ç‚¹æŒ‡æ ‡: {len(node_metrics)} è¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½èŠ‚ç‚¹æŒ‡æ ‡: {str(e)}")
        
        # åŠ è½½04æ¨¡å—è¾“å‡º - DLIæ•°æ®
        dli_data_path = self.base_dir / "src" / "04_dli_analysis" / "dli_panel_data.csv"
        if dli_data_path.exists():
            try:
                dli_data = pd.read_csv(dli_data_path)
                base_data['dli_panel'] = dli_data
                logger.info(f"âœ… åŠ è½½DLIé¢æ¿æ•°æ®: {len(dli_data)} è¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½DLIæ•°æ®: {str(e)}")
        
        self.base_data = base_data
        logger.info(f"âœ… åŸºç¡€æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(base_data)} ä¸ªæ•°æ®é›†")
        
        return base_data
    
    def construct_node_dli_us(self) -> Optional[pd.DataFrame]:
        """æ„å»º Node-DLI_US (ç¾å›½é”šå®šåŠ¨æ€é”å®šæŒ‡æ•°)"""
        logger.info("   æ„å»º Node-DLI_US...")
        
        try:
            if 'dli_panel' not in self.base_data or 'trade_flow' not in self.base_data:
                logger.warning("âš ï¸ ç¼ºå°‘DLIæˆ–è´¸æ˜“æ•°æ®ï¼Œè·³è¿‡Node-DLI_USæ„å»º")
                return None
            
            dli_data = self.base_data['dli_panel'].copy()
            trade_data = self.base_data['trade_flow'].copy()
            
            # ç­›é€‰ä¸ç¾å›½ç›¸å…³çš„è´¸æ˜“
            us_trade = trade_data[
                (trade_data['reporter'] == 'USA') | 
                (trade_data['partner'] == 'USA')
            ].copy()
            
            if len(us_trade) == 0:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¾å›½ç›¸å…³è´¸æ˜“æ•°æ®")
                return None
            
            # è®¡ç®—è´¸æ˜“ä»½é¢
            us_trade['partner_country'] = np.where(
                us_trade['reporter'] == 'USA',
                us_trade['partner'],
                us_trade['reporter']
            )
            
            # è®¡ç®—å„å›½æ€»è¿›å£é¢
            total_imports = trade_data[trade_data['flow'] == 'M'].groupby(['year', 'reporter']).agg({
                'trade_value_raw_usd': 'sum'
            }).reset_index()
            total_imports.columns = ['year', 'country', 'total_imports']
            
            # è®¡ç®—å„å›½ä»ç¾å›½çš„è¿›å£é¢
            us_imports = us_trade[
                (us_trade['partner'] == 'USA') & (us_trade['flow'] == 'M')
            ].groupby(['year', 'reporter']).agg({
                'trade_value_raw_usd': 'sum'
            }).reset_index()
            us_imports.columns = ['year', 'country', 'us_imports']
            
            # åˆå¹¶è®¡ç®—çœŸå®è¿›å£ä»½é¢
            trade_shares = total_imports.merge(us_imports, on=['year', 'country'], how='left')
            trade_shares['us_imports'] = trade_shares['us_imports'].fillna(0)
            trade_shares['import_share_from_us'] = trade_shares['us_imports'] / trade_shares['total_imports']
            trade_shares['import_share_from_us'] = trade_shares['import_share_from_us'].fillna(0).clip(0, 1)
            
            logger.info(f"   è®¡ç®—äº† {len(trade_shares)} ä¸ªå›½å®¶-å¹´ä»½çš„çœŸå®è´¸æ˜“ä»½é¢")
            
            # åŸºäºçœŸå®DLIæ•°æ®æ„å»ºNode-DLI_US
            node_dli_records = []
            
            for _, trade_row in trade_shares.iterrows():
                year = trade_row['year']
                country = trade_row['country']
                s_imp = trade_row['import_share_from_us']
                
                # æŸ¥æ‰¾å¯¹åº”çš„DLIæ•°æ®
                dli_us_to_i = dli_data[
                    (dli_data['year'] == year) &
                    (dli_data['us_partner'] == country) &
                    (dli_data['us_role'] == 'exporter')
                ]['dli_score_adjusted'].mean()
                
                dli_i_to_us = dli_data[
                    (dli_data['year'] == year) &
                    (dli_data['us_partner'] == country) &
                    (dli_data['us_role'] == 'importer')
                ]['dli_score_adjusted'].mean()
                
                # åº”ç”¨Node-DLIå…¬å¼
                if pd.isna(dli_us_to_i):
                    dli_us_to_i = 0
                if pd.isna(dli_i_to_us):
                    dli_i_to_us = 0
                
                # NodeDLI^US_{i,t} = s^{imp}_{i,US,t} Ã— DLI_{USâ†’i,t} + (1-s^{imp}_{i,US,t}) Ã— DLI_{iâ†’US,t}
                node_dli_us = s_imp * dli_us_to_i + (1 - s_imp) * dli_i_to_us
                
                node_dli_records.append({
                    'year': year,
                    'country': country,
                    'node_dli_us': node_dli_us,
                    'import_share_from_us': s_imp
                })
            
            node_dli_df = pd.DataFrame(node_dli_records)
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            non_zero_dli = node_dli_df[node_dli_df['node_dli_us'] > 0]
            logger.info(f"   æœ‰æ•ˆDLIè®°å½•: {len(non_zero_dli)}/{len(node_dli_df)}")
            logger.info(f"   Node-DLI_USèŒƒå›´: {node_dli_df['node_dli_us'].min():.3f} - {node_dli_df['node_dli_us'].max():.3f}")
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            output_path = self.temp_data_dir / "node_dli_us_clean.csv"
            node_dli_df.to_csv(output_path, index=False)
            
            logger.info(f"âœ… Node-DLI_USæ„å»ºå®Œæˆ: {len(node_dli_df)} è¡Œè®°å½•")
            return node_dli_df[['year', 'country', 'node_dli_us', 'import_share_from_us']].copy()
            
        except Exception as e:
            logger.error(f"âŒ Node-DLI_USæ„å»ºå¤±è´¥: {str(e)}")
            return None
    
    def construct_vul_us(self) -> Optional[pd.DataFrame]:
        """æ„å»º Vul_US (ç¾å›½é”šå®šè„†å¼±æ€§æŒ‡æ•°)"""
        logger.info("   æ„å»º Vul_US...")
        
        try:
            if 'trade_flow' not in self.base_data:
                logger.warning("âš ï¸ ç¼ºå°‘è´¸æ˜“æ•°æ®ï¼Œè·³è¿‡Vul_USæ„å»º")
                return None
            
            trade_data = self.base_data['trade_flow'].copy()
            
            # è®¡ç®—å„å›½çš„è¿›å£ä¾èµ–åº¦å’Œå¤šæ ·åŒ–ç¨‹åº¦
            import_data = trade_data[trade_data['flow'] == 'M'].copy()
            import_data = import_data.groupby(['year', 'reporter', 'partner']).agg({
                'trade_value_raw_usd': 'sum'
            }).reset_index()
            
            # è®¡ç®—HHIæŒ‡æ•°
            total_imports = import_data.groupby(['year', 'reporter']).agg({
                'trade_value_raw_usd': 'sum'
            }).reset_index().rename(columns={'trade_value_raw_usd': 'total_imports'})
            
            import_data = import_data.merge(total_imports, on=['year', 'reporter'])
            import_data['import_share'] = import_data['trade_value_raw_usd'] / import_data['total_imports']
            
            # è®¡ç®—HHI
            hhi_data = import_data.groupby(['year', 'reporter']).apply(
                lambda x: (x['import_share'] ** 2).sum()
            ).reset_index(name='hhi_imports')
            
            # è®¡ç®—å¯¹ç¾ä¾èµ–åº¦
            us_imports = import_data[import_data['partner'] == 'USA'].copy()
            us_imports = us_imports.rename(columns={
                'import_share': 'us_import_share',
                'reporter': 'country'
            })[['year', 'country', 'us_import_share']]
            
            # åˆå¹¶æ•°æ®è®¡ç®—Vul_US
            vul_data = hhi_data.merge(us_imports, left_on=['year', 'reporter'], 
                                    right_on=['year', 'country'], how='left')
            
            vul_data['us_import_share'] = vul_data['us_import_share'].fillna(0)
            vul_data['vul_us'] = vul_data['us_import_share'] * vul_data['hhi_imports']
            
            vul_df = vul_data[['year', 'country', 'vul_us', 'us_import_share', 'hhi_imports']].copy()
            vul_df = vul_df.dropna()
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            output_path = self.temp_data_dir / "vul_us_clean.csv"
            vul_df.to_csv(output_path, index=False)
            
            logger.info(f"âœ… Vul_USæ„å»ºå®Œæˆ: {len(vul_df)} è¡Œè®°å½•")
            return vul_df
            
        except Exception as e:
            logger.error(f"âŒ Vul_USæ„å»ºå¤±è´¥: {str(e)}")
            return None
    
    def construct_gas_ovi(self) -> Optional[pd.DataFrame]:
        """æ„å»ºå¤©ç„¶æ°”OVI"""
        logger.info("   æ„å»ºå¤©ç„¶æ°”OVI...")
        
        try:
            # ä½¿ç”¨ç®€åŒ–çš„å¤©ç„¶æ°”OVIæ„å»ºå™¨
            builder = SimpleGasOVIBuilder(self.temp_data_dir)
            ovi_data = builder.build_gas_ovi()
            
            if len(ovi_data) > 0:
                # åªä¿ç•™æ ¸å¿ƒåˆ—ç”¨äºåˆå¹¶
                result = ovi_data[['country', 'year', 'ovi_gas']].copy()
                logger.info(f"âœ… å¤©ç„¶æ°”OVIæ„å»ºå®Œæˆ: {len(result)} è¡Œè®°å½•")
                return result
            else:
                logger.warning("âš ï¸ æœªèƒ½æ„å»ºå¤©ç„¶æ°”OVIæ•°æ®")
                return None
                
        except Exception as e:
            logger.error(f"âŒ å¤©ç„¶æ°”OVIæ„å»ºå¤±è´¥: {str(e)}")
            return None
    
    def construct_us_prod_shock(self) -> Optional[pd.DataFrame]:
        """æ„å»ºç»¼åˆäº§é‡å†²å‡»æŒ‡æ•° (åŸæ²¹+å¤©ç„¶æ°”)"""
        logger.info("   æ„å»ºç»¼åˆUS_ProdShockï¼ˆåŸæ²¹+å¤©ç„¶æ°”ï¼‰...")
        
        try:
            # ä½¿ç”¨EIA APIè·å–æ•°æ®
            eia_api_key = "kCKMXECZ7EZxHpYPXekyOhSdccpNc85aeOpDGIwm"
            logger.info(f"   ä½¿ç”¨EIA API Key: {eia_api_key[:8]}...")
            
            # è·å–ç¾å›½åŸæ²¹äº§é‡æ•°æ®
            oil_url = "https://api.eia.gov/v2/petroleum/crd/crpdn/data/"
            oil_params = {
                'api_key': eia_api_key,
                'frequency': 'annual',
                'data[0]': 'value',
                'start': '2000',
                'end': '2023',
                'length': 1000
            }
            
            oil_response = requests.get(oil_url, params=oil_params, timeout=30)
            oil_data = None
            if oil_response.status_code == 200:
                oil_json = oil_response.json()
                if 'response' in oil_json and 'data' in oil_json['response']:
                    oil_df = pd.DataFrame(oil_json['response']['data'])
                    us_oil = oil_df[oil_df['area-name'].str.contains('USA', na=False)].copy()
                    us_oil['year'] = us_oil['period'].astype(int)
                    us_oil['value'] = pd.to_numeric(us_oil['value'], errors='coerce')
                    oil_data = us_oil.groupby('year')['value'].sum().reset_index()
                    oil_data.columns = ['year', 'us_production_oil']
                    logger.info(f"   åŸæ²¹æ•°æ®: {len(oil_data)} å¹´")
            
            # è·å–ç¾å›½å¤©ç„¶æ°”äº§é‡æ•°æ®
            gas_url = "https://api.eia.gov/v2/natural-gas/prod/sum/data/"
            gas_params = {
                'api_key': eia_api_key,
                'frequency': 'annual',
                'data[0]': 'value',
                'start': '2000',
                'end': '2023',
                'length': 500
            }
            
            gas_response = requests.get(gas_url, params=gas_params, timeout=30)
            gas_data = None
            if gas_response.status_code == 200:
                gas_json = gas_response.json()
                if 'response' in gas_json and 'data' in gas_json['response']:
                    gas_df = pd.DataFrame(gas_json['response']['data'])
                    gas_df['year'] = gas_df['period'].astype(int)
                    gas_df['value'] = pd.to_numeric(gas_df['value'], errors='coerce')
                    gas_data = gas_df.groupby('year')['value'].sum().reset_index()
                    gas_data.columns = ['year', 'us_production_gas']
                    logger.info(f"   å¤©ç„¶æ°”æ•°æ®: {len(gas_data)} å¹´")
            
            if oil_data is None or gas_data is None:
                logger.warning("âš ï¸ EIA APIæ•°æ®è·å–ä¸å®Œæ•´")
                return None
            
            # åˆå¹¶æ•°æ®
            combined_data = oil_data.merge(gas_data, on='year', how='outer').sort_values('year')
            combined_data = combined_data.dropna()
            
            if len(combined_data) < 10:
                logger.warning("âš ï¸ åˆå¹¶åæ•°æ®ç‚¹è¿‡å°‘")
                return None
            
            # è®¡ç®—HPæ»¤æ³¢å†²å‡»
            try:
                from statsmodels.tsa.filters.hp_filter import hpfilter
                oil_cycle, oil_trend = hpfilter(combined_data['us_production_oil'].values, lamb=100)
                gas_cycle, gas_trend = hpfilter(combined_data['us_production_gas'].values, lamb=100)
            except ImportError:
                # ç®€åŒ–å†²å‡»è®¡ç®—
                oil_cycle = (combined_data['us_production_oil'] - 
                           combined_data['us_production_oil'].rolling(3).mean()).fillna(0).values
                gas_cycle = (combined_data['us_production_gas'] - 
                           combined_data['us_production_gas'].rolling(3).mean()).fillna(0).values
            
            # æ ‡å‡†åŒ–å†²å‡»åºåˆ—
            z_shock_oil = (oil_cycle - oil_cycle.mean()) / oil_cycle.std()
            z_shock_gas = (gas_cycle - gas_cycle.mean()) / gas_cycle.std()
            
            # ç­‰æƒé‡åˆæˆç»¼åˆå†²å‡»æŒ‡æ•°
            us_prod_shock = 0.5 * z_shock_oil + 0.5 * z_shock_gas
            
            # æ„å»ºæœ€ç»ˆæ•°æ®æ¡†
            shock_df = pd.DataFrame({
                'year': combined_data['year'].values,
                'us_production_oil': combined_data['us_production_oil'].values,
                'us_production_gas': combined_data['us_production_gas'].values,
                'us_prod_shock': us_prod_shock
            })
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            output_path = self.temp_data_dir / "us_prod_shock_clean.csv"
            shock_df.to_csv(output_path, index=False)
            
            logger.info(f"âœ… ç»¼åˆUS_ProdShockæ„å»ºå®Œæˆ: {len(shock_df)} å¹´æ•°æ®")
            
            return shock_df
                
        except Exception as e:
            logger.error(f"âŒ ç»¼åˆUS_ProdShockæ„å»ºå¤±è´¥: {str(e)}")
            return None
    
    def construct_core_variables(self) -> Dict[str, pd.DataFrame]:
        """æ„å»ºæ ¸å¿ƒå˜é‡"""
        logger.info("âš™ï¸ å¼€å§‹æ„å»ºæ ¸å¿ƒå˜é‡...")
        
        core_vars = {}
        
        # 1. æ„å»º Node-DLI_US
        node_dli_us = self.construct_node_dli_us()
        if node_dli_us is not None:
            core_vars['node_dli_us'] = node_dli_us
        
        # 2. æ„å»º Vul_US
        vul_us = self.construct_vul_us()
        if vul_us is not None:
            core_vars['vul_us'] = vul_us
        
        # 3. æ„å»ºå¤©ç„¶æ°”OVI
        gas_ovi = self.construct_gas_ovi()
        if gas_ovi is not None:
            core_vars['ovi_gas'] = gas_ovi
        
        # 4. æ„å»º USäº§é‡å†²å‡»
        us_shock = self.construct_us_prod_shock()
        if us_shock is not None:
            core_vars['us_prod_shock'] = us_shock
        
        self.core_variables = core_vars
        logger.info(f"âœ… æ ¸å¿ƒå˜é‡æ„å»ºå®Œæˆï¼Œå…± {len(core_vars)} ä¸ªå˜é‡")
        
        return core_vars
    
    def create_analytical_panel(self) -> pd.DataFrame:
        """åˆ›å»ºæœ€ç»ˆåˆ†æé¢æ¿"""
        logger.info("ğŸ”— å¼€å§‹åˆ›å»ºæœ€ç»ˆåˆ†æé¢æ¿...")
        
        # ä»å®è§‚æ•°æ®å¼€å§‹æ„å»ºé¢æ¿
        if self.macro_data is not None:
            panel = self.macro_data.copy()
            if 'country_name' in panel.columns and 'country' not in panel.columns:
                panel['country'] = panel['country_name'].str[:3].str.upper()
            logger.info(f"   åŸºäºå®è§‚æ•°æ®æ„å»ºèµ·å§‹é¢æ¿: {len(panel)} è¡Œ")
        else:
            # åˆ›å»ºåŸºç¡€æ¡†æ¶é¢æ¿
            logger.info("âš ï¸ å®è§‚æ•°æ®ç¼ºå¤±ï¼Œåˆ›å»ºåŸºç¡€å›½å®¶-å¹´ä»½é¢æ¿æ¡†æ¶")
            countries = ['USA', 'CHN', 'DEU', 'JPN', 'GBR', 'FRA', 'IND', 'ITA', 'BRA', 'CAN',
                        'RUS', 'AUS', 'KOR', 'ESP', 'MEX', 'IDN', 'NLD', 'SAU', 'TUR', 'ARE']
            years = list(range(2000, 2025))
            
            country_year_pairs = []
            for country in countries:
                for year in years:
                    country_year_pairs.append({'country': country, 'year': year})
            
            panel = pd.DataFrame(country_year_pairs)
            logger.info(f"   åˆ›å»ºåŸºç¡€é¢æ¿æ¡†æ¶: {len(panel)} è¡Œ")
        
        # åˆå¹¶æ ¸å¿ƒå˜é‡
        merge_count = 0
        for var_name, var_data in self.core_variables.items():
            if var_data is not None and len(var_data) > 0:
                try:
                    before_len = len(panel)
                    
                    if var_name == 'us_prod_shock':
                        # US_ProdShockåªæœ‰å¹´ä»½æ•°æ®ï¼Œä¸ºæ‰€æœ‰å›½å®¶å¤åˆ¶
                        panel = panel.merge(var_data, on='year', how='left')
                    else:
                        # å…¶ä»–å˜é‡æŒ‰yearå’Œcountryåˆå¹¶
                        panel = panel.merge(var_data, on=['year', 'country'], how='left')
                    
                    after_len = len(panel)
                    
                    if after_len == before_len:
                        merge_count += 1
                        # ç»Ÿè®¡è¦†ç›–ç‡
                        if var_name != 'us_prod_shock':
                            coverage = var_data.shape[0]
                            total_possible = len(panel)
                            logger.info(f"   âœ… åˆå¹¶ {var_name}: {coverage} è¡Œæ•°æ®")
                        else:
                            logger.info(f"   âœ… åˆå¹¶ {var_name}: {len(var_data)} å¹´æ•°æ®ï¼ˆå…¨é¢æ¿å¤åˆ¶ï¼‰")
                            
                except Exception as e:
                    logger.warning(f"   âŒ æ— æ³•åˆå¹¶ {var_name}: {str(e)}")
        
        # æ•°æ®æ¸…æ´—
        panel = self._clean_final_panel(panel)
        
        # ä¿å­˜æœ€ç»ˆé¢æ¿
        output_path = self.output_dir / "analytical_panel.csv"
        panel.to_csv(output_path, index=False)
        
        self.final_panel = panel
        
        logger.info(f"âœ… æœ€ç»ˆåˆ†æé¢æ¿åˆ›å»ºå®Œæˆ:")
        logger.info(f"   è¡Œæ•°: {len(panel)}")
        logger.info(f"   åˆ—æ•°: {len(panel.columns)}")
        logger.info(f"   å¹´ä»½èŒƒå›´: {panel['year'].min()}-{panel['year'].max()}")
        logger.info(f"   å›½å®¶æ•°é‡: {panel['country'].nunique()}")
        logger.info(f"   æˆåŠŸåˆå¹¶: {merge_count} ä¸ªæ ¸å¿ƒå˜é‡")
        logger.info(f"   ä¿å­˜è‡³: {output_path}")
        
        return panel
    
    def _clean_final_panel(self, panel: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æœ€ç»ˆé¢æ¿æ•°æ®"""
        logger.info("   æ¸…æ´—æœ€ç»ˆé¢æ¿æ•°æ®...")
        
        # åˆ é™¤é‡å¤è¡Œ
        initial_len = len(panel)
        panel = panel.drop_duplicates(subset=['year', 'country'])
        if len(panel) != initial_len:
            logger.info(f"   åˆ é™¤é‡å¤è¡Œ: {initial_len} -> {len(panel)}")
        
        # ç¡®ä¿å¹´ä»½ä¸ºæ•´æ•°
        panel['year'] = panel['year'].astype(int)
        
        # é™åˆ¶å¹´ä»½èŒƒå›´
        panel = panel[(panel['year'] >= 2000) & (panel['year'] <= 2024)]
        
        # æ¸…ç†æ— æ•ˆå€¼
        numeric_columns = panel.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            panel[col] = panel[col].replace([np.inf, -np.inf], np.nan)
        
        # æŒ‰å¹´ä»½å’Œå›½å®¶æ’åº
        panel = panel.sort_values(['year', 'country']).reset_index(drop=True)
        
        return panel
    
    def run_clean_pipeline(self) -> None:
        """è¿è¡Œæ¸…æ™°ç‰ˆå˜é‡æ„å»ºæµæ°´çº¿"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ¸…æ™°ç‰ˆå˜é‡æ„å»ºæµæ°´çº¿...")
        
        try:
            # æ­¥éª¤1: åŠ è½½å®è§‚æ§åˆ¶å˜é‡
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤1: åŠ è½½å®è§‚ç»æµæ§åˆ¶å˜é‡")
            logger.info("="*50)
            self.load_cached_macro_data()
            
            # æ­¥éª¤2: åŠ è½½åŸºç¡€æ•°æ®
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤2: åŠ è½½åŸºç¡€æ•°æ®")
            logger.info("="*50)
            self.load_base_data()
            
            # æ­¥éª¤3: æ„å»ºæ ¸å¿ƒå˜é‡
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤3: æ„å»ºæ ¸å¿ƒå˜é‡")
            logger.info("="*50)
            self.construct_core_variables()
            
            # æ­¥éª¤4: åˆ›å»ºæœ€ç»ˆé¢æ¿
            logger.info("\n" + "="*50)
            logger.info("æ­¥éª¤4: åˆ›å»ºæœ€ç»ˆåˆ†æé¢æ¿")
            logger.info("="*50)
            self.create_analytical_panel()
            
            logger.info("\n" + "="*60)
            logger.info("ğŸ‰ æ¸…æ™°ç‰ˆå˜é‡æ„å»ºæµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
            logger.info("="*60)
            logger.info(f"âœ… æœ€ç»ˆè¾“å‡º:")
            logger.info(f"   - åˆ†æé¢æ¿: analytical_panel.csv ({len(self.final_panel)} è¡Œ)")
            logger.info(f"   - ä¸­é—´æ–‡ä»¶: {self.temp_data_dir}")
            logger.info(f"   - è¾“å‡ºç›®å½•: {self.output_dir}")
            
        except Exception as e:
            logger.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºæ¸…æ™°ç‰ˆå˜é‡æ„å»ºæµæ°´çº¿"""
    print("ğŸ—ï¸ 08_variable_construction - æ¸…æ™°ç‰ˆæ•°æ®å·¥å‚ v3.0")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–å˜é‡æ„å»ºå™¨
        constructor = CleanVariableConstructor()
        
        # è¿è¡Œæ¸…æ™°ç‰ˆæµæ°´çº¿
        constructor.run_clean_pipeline()
        
        print("\nâœ… æ¸…æ™°ç‰ˆå˜é‡æ„å»ºæ¨¡å—æ‰§è¡ŒæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶:")
        print(f"   - {constructor.output_dir / 'analytical_panel.csv'}")
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    main()