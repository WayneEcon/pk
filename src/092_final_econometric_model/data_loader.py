#!/usr/bin/env python3
"""
092_final_econometric_model æ•°æ®åŠ è½½å™¨
================================

æœ€ç»ˆè®¡é‡åˆ†ææ¨¡å—çš„æ•°æ®æ•´åˆç»„ä»¶
- åŠ è½½åŸºç¡€åˆ†æé¢æ¿æ•°æ®
- æ•´åˆåœ°ç†è·ç¦»æ•°æ®
- æ„å»ºçº¯å‡€LNGä»·æ ¼å˜é‡
- ä¸ºæœ€ç»ˆLP-IRFæ¨¡å‹å‡†å¤‡å®Œæ•´æ•°æ®

ä½œè€…ï¼šEnergy Network Analysis Team
ç‰ˆæœ¬ï¼šv1.0 - å†³å®šæ€§å› æœæ¨æ–­ç‰ˆæœ¬
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
import logging
from typing import Optional, Dict, Tuple
from scipy.stats import mstats
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class FinalDataLoader:
    """
    æœ€ç»ˆæ•°æ®åŠ è½½å™¨ - æ•´åˆæ‰€æœ‰æ•°æ®æºä¸ºLP-IRFåˆ†æåšå‡†å¤‡
    """
    
    def __init__(self, project_root: Optional[Path] = None):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        if project_root is None:
            self.project_root = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network")
        else:
            self.project_root = Path(project_root)
        
        # å®šä¹‰æ–°çš„æ•°æ®è·¯å¾„ - æŒ‰ç…§æœ€æ–°è¦æ±‚æ›´æ–°
        # 1. HHIæ•°æ®
        self.hhi_path = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network/src/08_variable_construction/outputs/hhi_imports_extracted.csv")
        
        # 2. å®è§‚æ§åˆ¶å˜é‡
        self.macro_controls_path = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network/src/08_variable_construction/outputs/macro_controls.csv")
        
        # 3. æ ¸å¿ƒOVI
        self.ovi_gas_path = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network/src/08_variable_construction/outputs/ovi_gas.csv")
        
        # 4. ç¾å›½å¤©ç„¶æ°”äº§é‡
        self.us_prod_shock_path = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network/src/08_variable_construction/outputs/us_prod_shock_ar2.csv")
        
        # 5. LNGä»·æ ¼å’Œæ¶ˆè´¹æ•°æ®
        self.price_quantity_path = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network/src/08_variable_construction/outputs/price_quantity_variables.csv")
        
        # 6. DLIæ•°æ®ï¼ˆä¸¤ä¸ªé€‰æ‹©ï¼‰
        self.dli_pagerank_path = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network/src/04_dli_analysis/outputs/dli_pagerank.csv")
        self.dli_export_path = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network/src/04_dli_analysis/outputs/export_dli.csv")
        
        # ä¿ç•™åŸæœ‰çš„è·ç¦»æ•°æ®è·¯å¾„
        self.distance_data_path = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network/src/04_dli_analysis/complete_us_distances_cepii.json")
        
        logger.info(f"092æ¨¡å—æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        
    def load_hhi_data(self) -> pd.DataFrame:
        """åŠ è½½HHIè¿›å£æ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½HHIè¿›å£æ•°æ®...")
        
        if not self.hhi_path.exists():
            logger.warning(f"âš ï¸ HHIæ•°æ®ä¸å­˜åœ¨: {self.hhi_path}")
            return pd.DataFrame()
        
        try:
            df = pd.read_csv(self.hhi_path)
            logger.info(f"âœ… HHIæ•°æ®åŠ è½½å®Œæˆ: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
            return df
        except Exception as e:
            logger.error(f"âŒ åŠ è½½HHIæ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def load_macro_controls(self) -> pd.DataFrame:
        """åŠ è½½å®è§‚æ§åˆ¶å˜é‡æ•°æ®"""
        logger.info("ğŸŒ åŠ è½½å®è§‚æ§åˆ¶å˜é‡æ•°æ®...")
        
        if not self.macro_controls_path.exists():
            logger.warning(f"âš ï¸ å®è§‚æ§åˆ¶å˜é‡æ•°æ®ä¸å­˜åœ¨: {self.macro_controls_path}")
            return pd.DataFrame()
        
        try:
            df = pd.read_csv(self.macro_controls_path)
            logger.info(f"âœ… å®è§‚æ§åˆ¶å˜é‡åŠ è½½å®Œæˆ: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
            return df
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å®è§‚æ§åˆ¶å˜é‡å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def load_ovi_gas_data(self) -> pd.DataFrame:
        """åŠ è½½æ ¸å¿ƒOVIå¤©ç„¶æ°”æ•°æ®"""
        logger.info("âš¡ åŠ è½½æ ¸å¿ƒOVIå¤©ç„¶æ°”æ•°æ®...")
        
        if not self.ovi_gas_path.exists():
            logger.error(f"âŒ æ ¸å¿ƒOVIæ•°æ®ä¸å­˜åœ¨: {self.ovi_gas_path}")
            return pd.DataFrame()
        
        try:
            df = pd.read_csv(self.ovi_gas_path)
            logger.info(f"âœ… OVIå¤©ç„¶æ°”æ•°æ®åŠ è½½å®Œæˆ: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
            return df
        except Exception as e:
            logger.error(f"âŒ åŠ è½½OVIæ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def load_us_prod_shock_data(self) -> pd.DataFrame:
        """åŠ è½½ç¾å›½å¤©ç„¶æ°”äº§é‡å†²å‡»æ•°æ®"""
        logger.info("ğŸ‡ºğŸ‡¸ åŠ è½½ç¾å›½äº§é‡å†²å‡»æ•°æ®...")
        
        if not self.us_prod_shock_path.exists():
            logger.error(f"âŒ ç¾å›½äº§é‡å†²å‡»æ•°æ®ä¸å­˜åœ¨: {self.us_prod_shock_path}")
            return pd.DataFrame()
        
        try:
            df = pd.read_csv(self.us_prod_shock_path)
            logger.info(f"âœ… ç¾å›½äº§é‡å†²å‡»æ•°æ®åŠ è½½å®Œæˆ: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
            return df
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç¾å›½äº§é‡å†²å‡»æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def load_dli_data(self, use_pagerank_version: bool = None) -> pd.DataFrame:
        """
        åŠ è½½DLIæ•°æ®ï¼Œæ”¯æŒä¸¤ä¸ªç‰ˆæœ¬çš„é€‰æ‹©
        
        Args:
            use_pagerank_version: Trueä½¿ç”¨pagerankç‰ˆæœ¬ï¼ŒFalseä½¿ç”¨exportç‰ˆæœ¬ï¼ŒNoneè‡ªåŠ¨é€‰æ‹©
            
        Returns:
            DLIæ•°æ®DataFrame
        """
        logger.info("ğŸ”— åŠ è½½DLIæ•°æ®...")
        
        # æ£€æŸ¥ä¸¤ä¸ªç‰ˆæœ¬çš„å¯ç”¨æ€§
        pagerank_available = self.dli_pagerank_path.exists()
        export_available = self.dli_export_path.exists()
        
        logger.info(f"   DLI PageRankç‰ˆæœ¬å¯ç”¨: {pagerank_available}")
        logger.info(f"   DLI Exportç‰ˆæœ¬å¯ç”¨: {export_available}")
        
        # è‡ªåŠ¨é€‰æ‹©é€»è¾‘
        if use_pagerank_version is None:
            if pagerank_available and export_available:
                logger.info("   ğŸ“Š ä¸¤ä¸ªç‰ˆæœ¬éƒ½å¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨PageRankå¢å¼ºç‰ˆ")
                use_pagerank_version = True
            elif pagerank_available:
                use_pagerank_version = True
            elif export_available:
                use_pagerank_version = False
            else:
                logger.error("âŒ ä¸¤ä¸ªDLIç‰ˆæœ¬éƒ½ä¸å¯ç”¨")
                return pd.DataFrame()
        
        # åŠ è½½é€‰å®šç‰ˆæœ¬
        try:
            if use_pagerank_version:
                if not pagerank_available:
                    logger.error(f"âŒ PageRankç‰ˆæœ¬ä¸å­˜åœ¨: {self.dli_pagerank_path}")
                    return pd.DataFrame()
                
                df = pd.read_csv(self.dli_pagerank_path)
                logger.info(f"âœ… DLI PageRankç‰ˆæœ¬åŠ è½½å®Œæˆ: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
                df['dli_version'] = 'pagerank_enhanced'
                
            else:
                if not export_available:
                    logger.error(f"âŒ Exportç‰ˆæœ¬ä¸å­˜åœ¨: {self.dli_export_path}")
                    return pd.DataFrame()
                
                df = pd.read_csv(self.dli_export_path)
                logger.info(f"âœ… DLI Exportç‰ˆæœ¬åŠ è½½å®Œæˆ: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
                df['dli_version'] = 'export_only'
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½DLIæ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def _merge_base_components(self, df_ovi: pd.DataFrame, df_hhi: pd.DataFrame, 
                              df_macro: pd.DataFrame, df_prod_shock: pd.DataFrame, 
                              df_dli: pd.DataFrame) -> pd.DataFrame:
        """
        åˆå¹¶æ‰€æœ‰åŸºç¡€æ•°æ®ç»„ä»¶
        
        Args:
            df_ovi: æ ¸å¿ƒOVIæ•°æ® 
            df_hhi: HHIæ•°æ®
            df_macro: å®è§‚æ§åˆ¶å˜é‡
            df_prod_shock: ç¾å›½äº§é‡å†²å‡»æ•°æ®
            df_dli: DLIæ•°æ®
            
        Returns:
            åˆå¹¶åçš„åŸºç¡€é¢æ¿æ•°æ®
        """
        logger.info("ğŸ”— å¼€å§‹åˆå¹¶åŸºç¡€æ•°æ®ç»„ä»¶...")
        
        # ä»¥OVIæ•°æ®ä½œä¸ºä¸»è¡¨
        df_merged = df_ovi.copy()
        logger.info(f"   ä¸»è¡¨(OVI): {df_merged.shape}")
        
        # åˆå¹¶HHIæ•°æ®
        if not df_hhi.empty:
            merge_keys = ['country', 'year'] if 'country' in df_hhi.columns else ['year']
            df_merged = df_merged.merge(df_hhi, on=merge_keys, how='left', suffixes=('', '_hhi'))
            logger.info(f"   åˆå¹¶HHIå: {df_merged.shape}")
        else:
            logger.warning("   âš ï¸ HHIæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
        
        # åˆå¹¶å®è§‚æ§åˆ¶å˜é‡
        if not df_macro.empty:
            merge_keys = ['country', 'year'] if 'country' in df_macro.columns else ['year']
            df_merged = df_merged.merge(df_macro, on=merge_keys, how='left', suffixes=('', '_macro'))
            logger.info(f"   åˆå¹¶å®è§‚å˜é‡å: {df_merged.shape}")
        else:
            logger.warning("   âš ï¸ å®è§‚æ§åˆ¶å˜é‡æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
        
        # åˆå¹¶ç¾å›½äº§é‡å†²å‡»æ•°æ®
        if not df_prod_shock.empty:
            # äº§é‡å†²å‡»æ•°æ®é€šå¸¸åªæœ‰å¹´ä»½ç»´åº¦
            df_merged = df_merged.merge(df_prod_shock, on='year', how='left', suffixes=('', '_prod'))
            logger.info(f"   åˆå¹¶äº§é‡å†²å‡»å: {df_merged.shape}")
        else:
            logger.warning("   âš ï¸ ç¾å›½äº§é‡å†²å‡»æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
        
        # åˆå¹¶DLIæ•°æ®
        if not df_dli.empty:
            # DLIæ•°æ®å¯èƒ½æœ‰ä¸åŒçš„åˆ—åç»“æ„ï¼Œéœ€è¦é€‚é…
            dli_merge_keys = []
            if 'country' in df_dli.columns and 'country' in df_merged.columns:
                dli_merge_keys.extend(['country', 'year'])
            elif 'us_partner' in df_dli.columns:
                # DLIæ•°æ®ä½¿ç”¨us_partnerå­—æ®µ
                df_dli_adapted = df_dli.rename(columns={'us_partner': 'country'})
                dli_merge_keys = ['country', 'year']
                df_merged = df_merged.merge(df_dli_adapted, on=dli_merge_keys, how='left', suffixes=('', '_dli'))
            else:
                dli_merge_keys = ['year']
                df_merged = df_merged.merge(df_dli, on=dli_merge_keys, how='left', suffixes=('', '_dli'))
            
            logger.info(f"   åˆå¹¶DLIæ•°æ®å: {df_merged.shape}")
            logger.info(f"   ä½¿ç”¨çš„DLIç‰ˆæœ¬: {df_dli.get('dli_version', ['æœªçŸ¥'])[0] if 'dli_version' in df_dli.columns else 'æœªçŸ¥'}")
        else:
            logger.warning("   âš ï¸ DLIæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
        
        logger.info(f"âœ… åŸºç¡€æ•°æ®ç»„ä»¶åˆå¹¶å®Œæˆ: {df_merged.shape}")
        
        # æ£€æŸ¥å…³é”®å˜é‡
        key_vars = ['year', 'country', 'ovi_gas']
        available_vars = [var for var in key_vars if var in df_merged.columns]
        missing_vars = [var for var in key_vars if var not in df_merged.columns]
        
        logger.info(f"   å¯ç”¨å…³é”®å˜é‡: {available_vars}")
        if missing_vars:
            logger.warning(f"   âš ï¸ ç¼ºå¤±å…³é”®å˜é‡: {missing_vars}")
        
        return df_merged
    
    def load_distance_data(self) -> Dict:
        """
        åŠ è½½åœ°ç†è·ç¦»æ•°æ®
        
        Returns:
            è·ç¦»æ•°æ®å­—å…¸ {country_code: distance_to_us}
        """
        logger.info("ğŸŒ åŠ è½½åœ°ç†è·ç¦»æ•°æ®...")
        
        if not self.distance_data_path.exists():
            logger.error(f"âŒ è·ç¦»æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.distance_data_path}")
            return {}
        
        try:
            with open(self.distance_data_path, 'r', encoding='utf-8') as f:
                distance_data = json.load(f)
            
            logger.info(f"âœ… è·ç¦»æ•°æ®åŠ è½½å®Œæˆ: {len(distance_data)} ä¸ªå›½å®¶")
            
            # æ˜¾ç¤ºç¤ºä¾‹æ•°æ®
            sample_countries = list(distance_data.items())[:5]
            logger.info(f"   ç¤ºä¾‹æ•°æ®: {sample_countries}")
            
            return distance_data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è·ç¦»æ•°æ®å¤±è´¥: {str(e)}")
            return {}
    
    def load_lng_data(self) -> pd.DataFrame:
        """
        åŠ è½½LNGè´¸æ˜“æ•°æ®
        
        Returns:
            LNGæ•°æ®DataFrame
        """
        logger.info("ğŸš¢ åŠ è½½LNGè´¸æ˜“æ•°æ®...")
        
        if not self.lng_data_path.exists():
            logger.error(f"âŒ LNGæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.lng_data_path}")
            return pd.DataFrame()
        
        try:
            df_lng = pd.read_csv(self.lng_data_path)
            logger.info(f"âœ… LNGæ•°æ®åŠ è½½å®Œæˆ: {df_lng.shape[0]} è¡Œ Ã— {df_lng.shape[1]} åˆ—")
            
            # æ£€æŸ¥å…³é”®åˆ—
            required_cols = ['reporterISO', 'refYear', 'primaryValue', 'netWgt']
            missing_cols = [col for col in required_cols if col not in df_lng.columns]
            
            if missing_cols:
                logger.warning(f"âš ï¸ LNGæ•°æ®ç¼ºå°‘åˆ—: {missing_cols}")
                return pd.DataFrame()
            
            # æ˜¾ç¤ºæ•°æ®èŒƒå›´
            if 'refYear' in df_lng.columns:
                year_range = f"{df_lng['refYear'].min()}-{df_lng['refYear'].max()}"
                logger.info(f"   æ—¶é—´èŒƒå›´: {year_range}")
            
            if 'reporterISO' in df_lng.columns:
                country_count = df_lng['reporterISO'].nunique()
                logger.info(f"   æ¶µç›–å›½å®¶: {country_count} ä¸ª")
            
            return df_lng
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½LNGæ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def merge_distance_data(self, df_panel: pd.DataFrame, distance_data: Dict) -> pd.DataFrame:
        """
        å°†åœ°ç†è·ç¦»æ•°æ®åˆå¹¶åˆ°åˆ†æé¢æ¿
        
        Args:
            df_panel: åŸºç¡€åˆ†æé¢æ¿
            distance_data: è·ç¦»æ•°æ®å­—å…¸
            
        Returns:
            åˆå¹¶äº†è·ç¦»æ•°æ®çš„DataFrame
        """
        logger.info("ğŸ”— åˆå¹¶åœ°ç†è·ç¦»æ•°æ®...")
        
        if df_panel.empty or not distance_data:
            logger.warning("âš ï¸ è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡è·ç¦»æ•°æ®åˆå¹¶")
            return df_panel
        
        try:
            df_with_distance = df_panel.copy()
            
            # æ·»åŠ è·ç¦»åˆ—
            df_with_distance['distance_to_us'] = df_with_distance['country'].map(distance_data)
            
            # ç»Ÿè®¡åˆå¹¶ç»“æœ
            matched_countries = df_with_distance['distance_to_us'].notna().sum()
            total_records = len(df_with_distance)
            match_rate = matched_countries / total_records if total_records > 0 else 0
            
            logger.info(f"âœ… è·ç¦»æ•°æ®åˆå¹¶å®Œæˆ:")
            logger.info(f"   â€¢ æˆåŠŸåŒ¹é…: {matched_countries}/{total_records} æ¡è®°å½• ({match_rate:.1%})")
            
            # æ˜¾ç¤ºæœªåŒ¹é…çš„å›½å®¶
            unmatched_countries = df_with_distance[df_with_distance['distance_to_us'].isna()]['country'].unique()
            if len(unmatched_countries) > 0:
                logger.info(f"   â€¢ æœªåŒ¹é…å›½å®¶: {list(unmatched_countries)[:10]}...")
            
            return df_with_distance
            
        except Exception as e:
            logger.error(f"âŒ è·ç¦»æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
            return df_panel
    
    def construct_lng_price(self, df_lng: pd.DataFrame) -> pd.DataFrame:
        """
        æ„å»ºçº¯å‡€LNGä»·æ ¼å˜é‡ P_it_lng
        
        Args:
            df_lng: LNGè´¸æ˜“æ•°æ®
            
        Returns:
            åŒ…å«P_it_lngçš„æ¸…æ´æ•°æ®
        """
        logger.info("ğŸ’° æ„å»ºçº¯å‡€LNGä»·æ ¼å˜é‡...")
        
        if df_lng.empty:
            logger.warning("âš ï¸ LNGæ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ„å»ºä»·æ ¼å˜é‡")
            return pd.DataFrame()
        
        try:
            # å¤åˆ¶æ•°æ®
            df_lng_clean = df_lng.copy()
            
            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'reporterISO': 'country',
                'refYear': 'year',
                'primaryValue': 'trade_value_usd',
                'netWgt': 'net_weight_kg'
            }
            
            for old_col, new_col in column_mapping.items():
                if old_col in df_lng_clean.columns:
                    df_lng_clean = df_lng_clean.rename(columns={old_col: new_col})
            
            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            required_cols = ['country', 'year', 'trade_value_usd', 'net_weight_kg']
            missing_cols = [col for col in required_cols if col not in df_lng_clean.columns]
            
            if missing_cols:
                logger.error(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                return pd.DataFrame()
            
            # è®¡ç®—å•ä»· P_it_lng = Trade Value (US$) / Net Weight (kg)
            logger.info("   è®¡ç®—åŸå§‹LNGå•ä»·...")
            df_lng_clean = df_lng_clean[
                (df_lng_clean['trade_value_usd'] > 0) & 
                (df_lng_clean['net_weight_kg'] > 0)
            ].copy()
            
            df_lng_clean['P_it_lng_raw'] = (
                df_lng_clean['trade_value_usd'] / df_lng_clean['net_weight_kg']
            )
            
            # ç§»é™¤æ˜æ˜¾å¼‚å¸¸å€¼
            valid_prices = df_lng_clean[df_lng_clean['P_it_lng_raw'] > 0]
            logger.info(f"   åŸå§‹ä»·æ ¼è§‚æµ‹æ•°: {len(valid_prices)}")
            
            # 1%å’Œ99%ç¼©å°¾å¤„ç†
            logger.info("   æ‰§è¡Œ1%å’Œ99%ç¼©å°¾å¤„ç†...")
            price_values = valid_prices['P_it_lng_raw'].values
            
            # ä½¿ç”¨scipyçš„mstats.winsorizeè¿›è¡Œç¼©å°¾
            winsorized_prices = mstats.winsorize(price_values, limits=[0.01, 0.01])
            
            # åˆ›å»ºæœ€ç»ˆä»·æ ¼æ•°æ®
            df_price_final = valid_prices.copy()
            df_price_final['P_it_lng'] = winsorized_prices
            
            # æŒ‰å›½å®¶-å¹´ä»½èšåˆï¼ˆå–å‡å€¼ï¼‰
            logger.info("   æŒ‰å›½å®¶-å¹´ä»½èšåˆä»·æ ¼æ•°æ®...")
            df_price_agg = df_price_final.groupby(['country', 'year']).agg({
                'P_it_lng': 'mean',
                'trade_value_usd': 'sum',
                'net_weight_kg': 'sum'
            }).reset_index()
            
            # ç»Ÿè®¡æœ€ç»ˆç»“æœ
            final_countries = df_price_agg['country'].nunique()
            final_records = len(df_price_agg)
            year_range = f"{df_price_agg['year'].min()}-{df_price_agg['year'].max()}"
            
            logger.info(f"âœ… LNGä»·æ ¼æ„å»ºå®Œæˆ:")
            logger.info(f"   â€¢ æœ€ç»ˆè®°å½•æ•°: {final_records}")
            logger.info(f"   â€¢ æ¶µç›–å›½å®¶: {final_countries} ä¸ª")
            logger.info(f"   â€¢ æ—¶é—´èŒƒå›´: {year_range}")
            logger.info(f"   â€¢ ä»·æ ¼èŒƒå›´: ${df_price_agg['P_it_lng'].min():.4f} - ${df_price_agg['P_it_lng'].max():.4f} /kg")
            
            return df_price_agg
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºLNGä»·æ ¼å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def merge_lng_price_data(self, df_panel: pd.DataFrame, df_lng_price: pd.DataFrame) -> pd.DataFrame:
        """
        å°†LNGä»·æ ¼æ•°æ®åˆå¹¶åˆ°ä¸»åˆ†æé¢æ¿
        
        Args:
            df_panel: ä¸»åˆ†æé¢æ¿ï¼ˆå·²å«è·ç¦»æ•°æ®ï¼‰
            df_lng_price: LNGä»·æ ¼æ•°æ®
            
        Returns:
            æœ€ç»ˆçš„å®Œæ•´åˆ†ææ•°æ®
        """
        logger.info("ğŸ”— åˆå¹¶LNGä»·æ ¼æ•°æ®...")
        
        if df_panel.empty:
            logger.warning("âš ï¸ ä¸»åˆ†æé¢æ¿ä¸ºç©º")
            return df_panel
        
        if df_lng_price.empty:
            logger.warning("âš ï¸ LNGä»·æ ¼æ•°æ®ä¸ºç©ºï¼Œå°†æ·»åŠ ç©ºçš„P_it_lngåˆ—")
            df_panel['P_it_lng'] = np.nan
            return df_panel
        
        try:
            # æ‰§è¡Œå·¦è¿æ¥åˆå¹¶
            df_final = df_panel.merge(
                df_lng_price[['country', 'year', 'P_it_lng']], 
                on=['country', 'year'], 
                how='left'
            )
            
            # ç»Ÿè®¡åˆå¹¶ç»“æœ
            total_records = len(df_final)
            lng_matched = df_final['P_it_lng'].notna().sum()
            lng_countries = df_final[df_final['P_it_lng'].notna()]['country'].nunique()
            match_rate = lng_matched / total_records if total_records > 0 else 0
            
            logger.info(f"âœ… LNGä»·æ ¼æ•°æ®åˆå¹¶å®Œæˆ:")
            logger.info(f"   â€¢ LNGä»·æ ¼è¦†ç›–: {lng_matched}/{total_records} æ¡è®°å½• ({match_rate:.1%})")
            logger.info(f"   â€¢ æœ‰LNGæ•°æ®çš„å›½å®¶: {lng_countries} ä¸ª")
            
            return df_final
            
        except Exception as e:
            logger.error(f"âŒ LNGä»·æ ¼æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
            return df_panel
    
    def load_price_quantity_data(self) -> pd.DataFrame:
        """
        åŠ è½½ä»·æ ¼æ•°é‡å˜é‡æ•°æ® (P_it, g_it)
        
        Returns:
            ä»·æ ¼æ•°é‡æ•°æ®DataFrame
        """
        logger.info("ğŸ“Š åŠ è½½ä»·æ ¼æ•°é‡å˜é‡æ•°æ®...")
        
        if not self.price_quantity_path.exists():
            logger.error(f"âŒ ä»·æ ¼æ•°é‡æ–‡ä»¶ä¸å­˜åœ¨: {self.price_quantity_path}")
            return pd.DataFrame()
        
        try:
            df_pq = pd.read_csv(self.price_quantity_path)
            logger.info(f"âœ… ä»·æ ¼æ•°é‡æ•°æ®åŠ è½½å®Œæˆ: {df_pq.shape[0]} è¡Œ Ã— {df_pq.shape[1]} åˆ—")
            
            # æ£€æŸ¥å…³é”®åˆ—
            required_cols = ['country', 'year', 'P_it', 'g_it']
            missing_cols = [col for col in required_cols if col not in df_pq.columns]
            
            if missing_cols:
                logger.warning(f"âš ï¸ ä»·æ ¼æ•°é‡æ•°æ®ç¼ºå°‘åˆ—: {missing_cols}")
                return pd.DataFrame()
            
            # æ˜¾ç¤ºæ•°æ®èŒƒå›´
            if 'year' in df_pq.columns:
                year_range = f"{df_pq['year'].min()}-{df_pq['year'].max()}"
                logger.info(f"   æ—¶é—´èŒƒå›´: {year_range}")
            
            if 'country' in df_pq.columns:
                country_count = df_pq['country'].nunique()
                logger.info(f"   æ¶µç›–å›½å®¶: {country_count} ä¸ª")
            
            # æ•°æ®è´¨é‡ç»Ÿè®¡
            p_it_valid = df_pq['P_it'].notna().sum()
            g_it_valid = df_pq['g_it'].notna().sum()
            logger.info(f"   P_itæœ‰æ•ˆè§‚æµ‹: {p_it_valid}")
            logger.info(f"   g_itæœ‰æ•ˆè§‚æµ‹: {g_it_valid}")
            
            return df_pq
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ä»·æ ¼æ•°é‡æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def merge_price_quantity_data(self, df_panel: pd.DataFrame, df_pq: pd.DataFrame) -> pd.DataFrame:
        """
        å°†ä»·æ ¼æ•°é‡æ•°æ®åˆå¹¶åˆ°ä¸»åˆ†æé¢æ¿
        
        Args:
            df_panel: ä¸»åˆ†æé¢æ¿
            df_pq: ä»·æ ¼æ•°é‡æ•°æ®
            
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        logger.info("ğŸ”— åˆå¹¶ä»·æ ¼æ•°é‡æ•°æ®...")
        
        if df_panel.empty:
            logger.warning("âš ï¸ ä¸»åˆ†æé¢æ¿ä¸ºç©º")
            return df_panel
        
        if df_pq.empty:
            logger.warning("âš ï¸ ä»·æ ¼æ•°é‡æ•°æ®ä¸ºç©ºï¼Œå°†æ·»åŠ ç©ºçš„P_itå’Œg_itåˆ—")
            df_panel['P_it'] = np.nan
            df_panel['g_it'] = np.nan
            return df_panel
        
        try:
            # æ‰§è¡Œå·¦è¿æ¥åˆå¹¶
            df_merged = df_panel.merge(
                df_pq[['country', 'year', 'P_it', 'g_it']], 
                on=['country', 'year'], 
                how='left'
            )
            
            # ç»Ÿè®¡åˆå¹¶ç»“æœ
            total_records = len(df_merged)
            p_it_matched = df_merged['P_it'].notna().sum()
            g_it_matched = df_merged['g_it'].notna().sum()
            pq_countries = df_merged[df_merged['P_it'].notna() | df_merged['g_it'].notna()]['country'].nunique()
            
            logger.info(f"âœ… ä»·æ ¼æ•°é‡æ•°æ®åˆå¹¶å®Œæˆ:")
            logger.info(f"   â€¢ P_itè¦†ç›–: {p_it_matched}/{total_records} æ¡è®°å½• ({p_it_matched/total_records:.1%})")
            logger.info(f"   â€¢ g_itè¦†ç›–: {g_it_matched}/{total_records} æ¡è®°å½• ({g_it_matched/total_records:.1%})")
            logger.info(f"   â€¢ æœ‰ä»·æ ¼æ•°é‡æ•°æ®çš„å›½å®¶: {pq_countries} ä¸ª")
            
            return df_merged
            
        except Exception as e:
            logger.error(f"âŒ ä»·æ ¼æ•°é‡æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
            return df_panel

    def load_clean_lng_price(self) -> pd.DataFrame:
        """
        åŠ è½½æ¸…ç†åçš„LNGä»·æ ¼æ•°æ®
        
        Returns:
            æ¸…ç†åçš„LNGä»·æ ¼DataFrame
        """
        logger.info("ğŸš¢ åŠ è½½æ¸…ç†åçš„LNGä»·æ ¼æ•°æ®...")
        
        clean_lng_path = Path("outputs/clean_lng_price_data.csv")
        
        if not clean_lng_path.exists():
            logger.warning(f"âš ï¸ æ¸…ç†åçš„LNGæ•°æ®ä¸å­˜åœ¨: {clean_lng_path}")
            logger.info("   è¯·å…ˆè¿è¡Œ clean_lng_data.py è„šæœ¬")
            return pd.DataFrame()
        
        try:
            df_lng = pd.read_csv(clean_lng_path)
            logger.info(f"âœ… æ¸…ç†åLNGæ•°æ®åŠ è½½å®Œæˆ: {df_lng.shape[0]} è¡Œ Ã— {df_lng.shape[1]} åˆ—")
            
            # æ£€æŸ¥å¿…è¦åˆ—
            required_cols = ['country', 'year', 'P_lng']
            missing_cols = [col for col in required_cols if col not in df_lng.columns]
            
            if missing_cols:
                logger.warning(f"âš ï¸ LNGæ•°æ®ç¼ºå°‘åˆ—: {missing_cols}")
                return pd.DataFrame()
            
            # æ•°æ®è´¨é‡ç»Ÿè®¡
            valid_prices = df_lng['P_lng'].notna().sum()
            logger.info(f"   æœ‰æ•ˆä»·æ ¼è®°å½•: {valid_prices}")
            logger.info(f"   æ¶µç›–å›½å®¶: {df_lng['country'].nunique()} ä¸ª")
            logger.info(f"   æ—¶é—´èŒƒå›´: {df_lng['year'].min()}-{df_lng['year'].max()}")
            logger.info(f"   ä»·æ ¼èŒƒå›´: ${df_lng['P_lng'].min():.4f} - ${df_lng['P_lng'].max():.4f} /kg")
            
            return df_lng
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¸…ç†åLNGæ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def merge_clean_lng_price(self, df_panel: pd.DataFrame, df_lng: pd.DataFrame) -> pd.DataFrame:
        """
        å°†æ¸…ç†åçš„LNGä»·æ ¼æ•°æ®åˆå¹¶åˆ°ä¸»åˆ†æé¢æ¿
        
        Args:
            df_panel: ä¸»åˆ†æé¢æ¿
            df_lng: æ¸…ç†åçš„LNGä»·æ ¼æ•°æ®
            
        Returns:
            æœ€ç»ˆçš„å®Œæ•´åˆ†ææ•°æ®
        """
        logger.info("ğŸ”— åˆå¹¶æ¸…ç†åçš„LNGä»·æ ¼æ•°æ®...")
        
        if df_panel.empty:
            logger.warning("âš ï¸ ä¸»åˆ†æé¢æ¿ä¸ºç©º")
            return df_panel
        
        if df_lng.empty:
            logger.warning("âš ï¸ LNGä»·æ ¼æ•°æ®ä¸ºç©ºï¼Œå°†æ·»åŠ ç©ºçš„P_lngåˆ—")
            df_panel['P_lng'] = np.nan
            return df_panel
        
        try:
            # æ‰§è¡Œå·¦è¿æ¥åˆå¹¶
            df_final = df_panel.merge(
                df_lng[['country', 'year', 'P_lng']], 
                on=['country', 'year'], 
                how='left'
            )
            
            # ç»Ÿè®¡åˆå¹¶ç»“æœ
            total_records = len(df_final)
            lng_matched = df_final['P_lng'].notna().sum()
            lng_countries = df_final[df_final['P_lng'].notna()]['country'].nunique()
            match_rate = lng_matched / total_records if total_records > 0 else 0
            
            logger.info(f"âœ… LNGä»·æ ¼æ•°æ®åˆå¹¶å®Œæˆ:")
            logger.info(f"   â€¢ P_lngè¦†ç›–: {lng_matched}/{total_records} æ¡è®°å½• ({match_rate:.1%})")
            logger.info(f"   â€¢ æœ‰LNGä»·æ ¼æ•°æ®çš„å›½å®¶: {lng_countries} ä¸ª")
            
            return df_final
            
        except Exception as e:
            logger.error(f"âŒ LNGä»·æ ¼æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
            return df_panel

    def load_complete_dataset(self) -> Tuple[pd.DataFrame, Dict]:
        """
        åŠ è½½å®Œæ•´çš„æœ€ç»ˆåˆ†ææ•°æ®é›†
        
        Returns:
            (å®Œæ•´æ•°æ®é›†, æ•°æ®ç»Ÿè®¡ä¿¡æ¯)
        """
        logger.info("ğŸš€ å¼€å§‹åŠ è½½å®Œæ•´çš„æœ€ç»ˆåˆ†ææ•°æ®é›†...")
        
        # æ­¥éª¤1: åˆ†åˆ«åŠ è½½å„ä¸ªæ•°æ®æºï¼ˆæ›¿ä»£åŸæœ‰çš„analytical_panelï¼‰
        logger.info("ğŸ“Š å¼€å§‹åˆ†åˆ«åŠ è½½å„ä¸ªæ•°æ®æº...")
        
        # åŠ è½½æ ¸å¿ƒOVIæ•°æ®ä½œä¸ºä¸»è¡¨
        df_main = self.load_ovi_gas_data()
        if df_main.empty:
            return pd.DataFrame(), {'status': 'failed', 'message': 'æ ¸å¿ƒOVIæ•°æ®åŠ è½½å¤±è´¥'}
        
        # åŠ è½½å…¶ä»–æ•°æ®æº
        df_hhi = self.load_hhi_data()
        df_macro = self.load_macro_controls()
        df_prod_shock = self.load_us_prod_shock_data()
        df_dli = self.load_dli_data()  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç‰ˆæœ¬
        
        # æ„å»ºåŸºç¡€é¢æ¿æ•°æ®é›†
        df_panel = self._merge_base_components(df_main, df_hhi, df_macro, df_prod_shock, df_dli)
        
        if df_panel.empty:
            return pd.DataFrame(), {'status': 'failed', 'message': 'åŸºç¡€æ•°æ®åˆå¹¶å¤±è´¥'}
        
        # æ­¥éª¤2: åŠ è½½ä»·æ ¼æ•°é‡æ•°æ® (P_it, g_it)
        df_pq = self.load_price_quantity_data()
        
        # æ­¥éª¤3: åˆå¹¶ä»·æ ¼æ•°é‡æ•°æ®
        df_with_pq = self.merge_price_quantity_data(df_panel, df_pq)
        
        # æ­¥éª¤4: åŠ è½½åœ°ç†è·ç¦»æ•°æ®
        distance_data = self.load_distance_data()
        
        # æ­¥éª¤5: åˆå¹¶è·ç¦»æ•°æ®
        df_with_distance = self.merge_distance_data(df_with_pq, distance_data)
        
        # æ­¥éª¤6: åŠ è½½æ¸…ç†åçš„LNGä»·æ ¼æ•°æ®
        df_lng_price = self.load_clean_lng_price()
        
        # æ­¥éª¤7: æœ€ç»ˆåˆå¹¶
        df_final = self.merge_clean_lng_price(df_with_distance, df_lng_price)
        
        # ç”Ÿæˆæ•°æ®ç»Ÿè®¡
        stats = self._generate_dataset_stats(df_final)
        
        logger.info(f"ğŸ‰ å®Œæ•´æ•°æ®é›†æ„å»ºå®Œæˆ:")
        logger.info(f"   â€¢ æœ€ç»ˆå½¢çŠ¶: {df_final.shape}")
        logger.info(f"   â€¢ æ ¸å¿ƒå˜é‡å®Œæ•´æ€§: {stats['core_variables_status']}")
        
        return df_final, stats
    
    def _generate_dataset_stats(self, df: pd.DataFrame) -> Dict:
        """
        ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            df: æœ€ç»ˆæ•°æ®é›†
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if df.empty:
            return {'status': 'empty', 'message': 'æ•°æ®é›†ä¸ºç©º'}
        
        # æ ¸å¿ƒå˜é‡æ£€æŸ¥
        core_vars = ['country', 'year', 'ovi_gas', 'us_prod_shock', 'distance_to_us', 'P_it', 'g_it', 'P_lng', 'log_gdp', 'log_population']
        core_status = {}
        
        for var in core_vars:
            if var in df.columns:
                non_null_count = df[var].notna().sum()
                total_count = len(df)
                core_status[var] = {
                    'available': True,
                    'coverage': f"{non_null_count}/{total_count} ({non_null_count/total_count:.1%})"
                }
            else:
                core_status[var] = {'available': False, 'coverage': '0/0 (0.0%)'}
        
        return {
            'status': 'success',
            'total_observations': len(df),
            'total_countries': df['country'].nunique() if 'country' in df.columns else 0,
            'year_range': f"{df['year'].min()}-{df['year'].max()}" if 'year' in df.columns and not df['year'].isna().all() else 'N/A',
            'core_variables_status': core_status,
            'columns': list(df.columns)
        }


def main():
    """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
    
    print("ğŸ”¬ 092_final_econometric_model æ•°æ®åŠ è½½å™¨æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    loader = FinalDataLoader()
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†
    df_final, stats = loader.load_complete_dataset()
    
    print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®é›†ç»Ÿè®¡:")
    print(f"   â€¢ æ•°æ®å½¢çŠ¶: {df_final.shape}")
    print(f"   â€¢ çŠ¶æ€: {stats.get('status', 'unknown')}")
    
    if stats['status'] == 'success':
        print(f"   â€¢ æ€»è§‚æµ‹æ•°: {stats['total_observations']}")
        print(f"   â€¢ å›½å®¶æ•°: {stats['total_countries']}")
        print(f"   â€¢ æ—¶é—´èŒƒå›´: {stats['year_range']}")
        
        print(f"\nğŸ“‹ æ ¸å¿ƒå˜é‡çŠ¶æ€:")
        for var, info in stats['core_variables_status'].items():
            status_icon = "âœ…" if info['available'] else "âŒ"
            print(f"   {status_icon} {var}: {info['coverage']}")
    
    print(f"\nğŸ‰ æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()