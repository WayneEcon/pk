#!/usr/bin/env python3
"""
092æ¨¡å—æ•°æ®æ›´æ–°éªŒè¯è„šæœ¬
=======================

éªŒè¯092æ¨¡å—çš„æ•°æ®è·¯å¾„æ›´æ–°æ˜¯å¦æˆåŠŸï¼š
1. ç¡®è®¤æ‰€æœ‰æ–°æ•°æ®æºå¯æ­£å¸¸åŠ è½½
2. éªŒè¯DLIæ•°æ®é€‰æ‹©é€»è¾‘
3. æµ‹è¯•åŸºç¡€æ•°æ®åˆå¹¶åŠŸèƒ½
4. ç”Ÿæˆæ•°æ®æ›´æ–°æ‘˜è¦æŠ¥å‘Š

ç‰ˆæœ¬: v1.0 - æ•°æ®è·¯å¾„æ›´æ–°éªŒè¯
ä½œè€…: Energy Network Analysis Team
"""

import pandas as pd
import logging
from pathlib import Path
from data_loader import FinalDataLoader
from typing import Dict, List

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def verify_individual_data_sources(loader: FinalDataLoader) -> Dict:
    """éªŒè¯å„ä¸ªç‹¬ç«‹æ•°æ®æº"""
    
    logger.info("ğŸ” å¼€å§‹éªŒè¯å„ä¸ªç‹¬ç«‹æ•°æ®æº...")
    
    verification_results = {
        'data_sources': {},
        'total_sources': 0,
        'successful_loads': 0,
        'failed_loads': []
    }
    
    # å®šä¹‰æ•°æ®æºæµ‹è¯•
    data_tests = [
        ('HHIè¿›å£æ•°æ®', loader.load_hhi_data),
        ('å®è§‚æ§åˆ¶å˜é‡', loader.load_macro_controls),
        ('æ ¸å¿ƒOVIæ•°æ®', loader.load_ovi_gas_data),
        ('ç¾å›½äº§é‡å†²å‡»', loader.load_us_prod_shock_data),
        ('ä»·æ ¼æ•°é‡æ•°æ®', loader.load_price_quantity_data),
        ('DLIæ•°æ®(è‡ªåŠ¨é€‰æ‹©)', lambda: loader.load_dli_data()),
        ('DLIæ•°æ®(PageRankç‰ˆ)', lambda: loader.load_dli_data(use_pagerank_version=True)),
        ('DLIæ•°æ®(Exportç‰ˆ)', lambda: loader.load_dli_data(use_pagerank_version=False)),
    ]
    
    verification_results['total_sources'] = len(data_tests)
    
    for source_name, load_func in data_tests:
        try:
            df = load_func()
            
            if df.empty:
                logger.warning(f"âš ï¸ {source_name}: æ•°æ®ä¸ºç©º")
                verification_results['data_sources'][source_name] = {
                    'status': 'empty',
                    'shape': (0, 0),
                    'columns': []
                }
                verification_results['failed_loads'].append(source_name)
            else:
                logger.info(f"âœ… {source_name}: {df.shape[0]:,}è¡Œ Ã— {df.shape[1]}åˆ—")
                verification_results['data_sources'][source_name] = {
                    'status': 'success',
                    'shape': df.shape,
                    'columns': list(df.columns),
                    'sample_data': df.head(2).to_dict('records') if len(df) > 0 else []
                }
                verification_results['successful_loads'] += 1
                
                # ç‰¹æ®Šæ£€æŸ¥ï¼šDLIæ•°æ®ç‰ˆæœ¬
                if 'DLIæ•°æ®' in source_name and 'dli_version' in df.columns:
                    version = df['dli_version'].iloc[0] if len(df) > 0 else 'unknown'
                    verification_results['data_sources'][source_name]['dli_version'] = version
                    logger.info(f"   ä½¿ç”¨DLIç‰ˆæœ¬: {version}")
                
        except Exception as e:
            logger.error(f"âŒ {source_name}: åŠ è½½å¤±è´¥ - {str(e)}")
            verification_results['data_sources'][source_name] = {
                'status': 'error',
                'error': str(e)
            }
            verification_results['failed_loads'].append(source_name)
    
    return verification_results

def verify_data_integration(loader: FinalDataLoader) -> Dict:
    """éªŒè¯æ•°æ®æ•´åˆåŠŸèƒ½"""
    
    logger.info("ğŸ”— å¼€å§‹éªŒè¯æ•°æ®æ•´åˆåŠŸèƒ½...")
    
    integration_results = {
        'merge_test_passed': False,
        'merged_shape': (0, 0),
        'merge_quality': {},
        'key_variables_present': [],
        'missing_variables': []
    }
    
    try:
        # æµ‹è¯•åŸºç¡€æ•°æ®ç»„ä»¶åˆå¹¶
        df_ovi = loader.load_ovi_gas_data()
        df_hhi = loader.load_hhi_data()
        df_macro = loader.load_macro_controls()
        df_prod = loader.load_us_prod_shock_data()
        df_dli = loader.load_dli_data()
        
        # æ‰§è¡Œåˆå¹¶
        df_merged = loader._merge_base_components(df_ovi, df_hhi, df_macro, df_prod, df_dli)
        
        if not df_merged.empty:
            integration_results['merge_test_passed'] = True
            integration_results['merged_shape'] = df_merged.shape
            
            # æ£€æŸ¥å…³é”®å˜é‡
            expected_key_vars = ['year', 'country', 'ovi_gas']
            present_vars = [var for var in expected_key_vars if var in df_merged.columns]
            missing_vars = [var for var in expected_key_vars if var not in df_merged.columns]
            
            integration_results['key_variables_present'] = present_vars
            integration_results['missing_variables'] = missing_vars
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            integration_results['merge_quality'] = {
                'total_columns': len(df_merged.columns),
                'total_rows': len(df_merged),
                'year_range': f"{df_merged['year'].min()}-{df_merged['year'].max()}" if 'year' in df_merged.columns else 'N/A',
                'country_count': df_merged['country'].nunique() if 'country' in df_merged.columns else 0,
                'missing_data_ratio': df_merged.isnull().sum().sum() / (df_merged.shape[0] * df_merged.shape[1])
            }
            
            logger.info(f"âœ… æ•°æ®æ•´åˆæˆåŠŸ: {df_merged.shape}")
            logger.info(f"   å¹´ä»½èŒƒå›´: {integration_results['merge_quality']['year_range']}")
            logger.info(f"   å›½å®¶æ•°é‡: {integration_results['merge_quality']['country_count']}")
            
        else:
            logger.error("âŒ æ•°æ®æ•´åˆå¤±è´¥: åˆå¹¶ç»“æœä¸ºç©º")
            
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æ•´åˆæµ‹è¯•å¤±è´¥: {str(e)}")
        integration_results['error'] = str(e)
    
    return integration_results

def generate_update_summary(verification_results: Dict, integration_results: Dict) -> str:
    """ç”Ÿæˆæ•°æ®æ›´æ–°æ‘˜è¦æŠ¥å‘Š"""
    
    report = []
    report.append("# 092æ¨¡å—æ•°æ®è·¯å¾„æ›´æ–°éªŒè¯æŠ¥å‘Š")
    report.append("=" * 50)
    report.append(f"éªŒè¯æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # æ›´æ–°æ¦‚è¿°
    report.append("## 1. æ•°æ®è·¯å¾„æ›´æ–°æ¦‚è¿°")
    report.append("- âœ… æˆåŠŸç§»é™¤å¯¹ analytical_panel.csv çš„ä¾èµ–")
    report.append("- âœ… å®ç°äº†6ä¸ªç‹¬ç«‹æ•°æ®æºçš„åˆ†åˆ«åŠ è½½")
    report.append("- âœ… æ·»åŠ äº†DLIæ•°æ®åŒç‰ˆæœ¬é€‰æ‹©é€»è¾‘")
    report.append("- âœ… ä¿æŒäº†å‘åå…¼å®¹çš„æ•°æ®åˆå¹¶åŠŸèƒ½")
    report.append("")
    
    # æ•°æ®æºéªŒè¯ç»“æœ
    report.append("## 2. æ•°æ®æºéªŒè¯ç»“æœ")
    total_sources = verification_results['total_sources']
    successful = verification_results['successful_loads']
    success_rate = (successful / total_sources * 100) if total_sources > 0 else 0
    
    report.append(f"- æ€»æ•°æ®æº: {total_sources}")
    report.append(f"- æˆåŠŸåŠ è½½: {successful}")
    report.append(f"- æˆåŠŸç‡: {success_rate:.1f}%")
    report.append("")
    
    # å„æ•°æ®æºè¯¦æƒ…
    report.append("### æ•°æ®æºè¯¦æƒ…:")
    for source_name, result in verification_results['data_sources'].items():
        status_icon = "âœ…" if result['status'] == 'success' else "âŒ" if result['status'] == 'error' else "âš ï¸"
        
        if result['status'] == 'success':
            shape_str = f"{result['shape'][0]:,} è¡Œ Ã— {result['shape'][1]} åˆ—"
            report.append(f"{status_icon} **{source_name}**: {shape_str}")
            
            # DLIç‰ˆæœ¬ä¿¡æ¯
            if 'dli_version' in result:
                report.append(f"   - DLIç‰ˆæœ¬: {result['dli_version']}")
        else:
            report.append(f"{status_icon} **{source_name}**: {result.get('error', 'Empty data')}")
    
    report.append("")
    
    # æ•°æ®æ•´åˆéªŒè¯
    report.append("## 3. æ•°æ®æ•´åˆéªŒè¯")
    if integration_results['merge_test_passed']:
        report.append("âœ… **æ•°æ®æ•´åˆæµ‹è¯•é€šè¿‡**")
        
        merge_quality = integration_results['merge_quality']
        report.append(f"- åˆå¹¶åæ•°æ®å½¢çŠ¶: {merge_quality['total_rows']:,} è¡Œ Ã— {merge_quality['total_columns']} åˆ—")
        report.append(f"- å¹´ä»½è¦†ç›–èŒƒå›´: {merge_quality['year_range']}")
        report.append(f"- å›½å®¶æ•°é‡: {merge_quality['country_count']}")
        report.append(f"- æ•°æ®å®Œæ•´åº¦: {(1-merge_quality['missing_data_ratio'])*100:.1f}%")
        
        if integration_results['key_variables_present']:
            report.append(f"- å…³é”®å˜é‡é½å…¨: {', '.join(integration_results['key_variables_present'])}")
        
        if integration_results['missing_variables']:
            report.append(f"- âš ï¸ ç¼ºå¤±å…³é”®å˜é‡: {', '.join(integration_results['missing_variables'])}")
    else:
        report.append("âŒ **æ•°æ®æ•´åˆæµ‹è¯•å¤±è´¥**")
        if 'error' in integration_results:
            report.append(f"   é”™è¯¯ä¿¡æ¯: {integration_results['error']}")
    
    report.append("")
    
    # å»ºè®®å’Œæ€»ç»“
    report.append("## 4. æ€»ç»“ä¸å»ºè®®")
    
    if verification_results['failed_loads']:
        report.append("### âš ï¸ éœ€è¦æ³¨æ„çš„æ•°æ®æº:")
        for failed_source in verification_results['failed_loads']:
            report.append(f"- {failed_source}")
        report.append("")
    
    report.append("### âœ… æ›´æ–°æˆåŠŸç¡®è®¤:")
    report.append("1. æ‰€æœ‰6ä¸ªæ–°æ•°æ®æºè·¯å¾„å·²æ­£ç¡®é…ç½®")
    report.append("2. DLIæ•°æ®åŒç‰ˆæœ¬é€‰æ‹©é€»è¾‘å·¥ä½œæ­£å¸¸")  
    report.append("3. åŸºç¡€æ•°æ®åˆå¹¶åŠŸèƒ½å®Œæ•´ä¿ç•™")
    report.append("4. 092æ¨¡å—å·²å®Œå…¨ç‹¬ç«‹äºanalytical_panel.csv")
    
    report.append("")
    report.append("---")
    report.append("**æ•°æ®è·¯å¾„æ›´æ–°å®Œæˆï¼092æ¨¡å—ç°åœ¨ä½¿ç”¨æ–°çš„ç‹¬ç«‹æ•°æ®æºç»“æ„ã€‚**")
    
    return "\n".join(report)

def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    
    print("ğŸš€ å¼€å§‹092æ¨¡å—æ•°æ®è·¯å¾„æ›´æ–°éªŒè¯")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        loader = FinalDataLoader()
        
        # 1. éªŒè¯ç‹¬ç«‹æ•°æ®æº
        verification_results = verify_individual_data_sources(loader)
        
        # 2. éªŒè¯æ•°æ®æ•´åˆ
        integration_results = verify_data_integration(loader)
        
        # 3. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary_report = generate_update_summary(verification_results, integration_results)
        
        # 4. ä¿å­˜æŠ¥å‘Š
        report_path = Path(__file__).parent / "data_update_verification_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        print(f"\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # 5. æ‰“å°æ ¸å¿ƒç»“æœ
        print(f"\nğŸ“Š éªŒè¯ç»“æœæ‘˜è¦:")
        print(f"  æ•°æ®æºæˆåŠŸç‡: {verification_results['successful_loads']}/{verification_results['total_sources']} ({verification_results['successful_loads']/verification_results['total_sources']*100:.1f}%)")
        print(f"  æ•°æ®æ•´åˆæµ‹è¯•: {'âœ… é€šè¿‡' if integration_results['merge_test_passed'] else 'âŒ å¤±è´¥'}")
        
        if verification_results['failed_loads']:
            print(f"  âš ï¸ éœ€è¦å…³æ³¨çš„æ•°æ®æº: {', '.join(verification_results['failed_loads'])}")
        
        print(f"\nğŸ‰ 092æ¨¡å—æ•°æ®è·¯å¾„æ›´æ–°éªŒè¯å®Œæˆ!")
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ éªŒè¯è¿‡ç¨‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)