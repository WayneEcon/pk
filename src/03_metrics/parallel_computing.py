#!/usr/bin/env python3
"""
å¹¶è¡Œè®¡ç®—æ¨¡å—
ä¸ºç½‘ç»œæŒ‡æ ‡è®¡ç®—æä¾›å¤šè¿›ç¨‹å¹¶è¡Œæ”¯æŒ
"""

import multiprocessing as mp
import pandas as pd
import networkx as nx
from typing import Dict, Any, List, Tuple, Callable
import logging
from functools import partial
import time

from utils import setup_logger

logger = setup_logger(__name__)

def _calculate_metrics_worker(args: Tuple[nx.DiGraph, int, str]) -> Tuple[int, pd.DataFrame]:
    """
    å¹¶è¡Œè®¡ç®—å·¥ä½œå‡½æ•°
    
    Args:
        args: (å›¾å¯¹è±¡, å¹´ä»½, è®¡ç®—ç±»å‹)
        
    Returns:
        (å¹´ä»½, ç»“æœDataFrame)
    """
    G, year, metric_type = args
    
    try:
        if metric_type == 'all':
            from . import calculate_all_metrics_for_year
            result = calculate_all_metrics_for_year(G, year)
        elif metric_type == 'node':
            from .node_metrics import calculate_all_node_centralities
            result = calculate_all_node_centralities(G, year)
        elif metric_type == 'global':
            from .global_metrics import calculate_all_global_metrics
            global_dict = calculate_all_global_metrics(G, year)
            result = pd.DataFrame([global_dict])
        else:
            raise ValueError(f"æœªçŸ¥çš„è®¡ç®—ç±»å‹: {metric_type}")
            
        return year, result
        
    except Exception as e:
        logger.error(f"å·¥ä½œè¿›ç¨‹è®¡ç®— {year} å¹´æŒ‡æ ‡å¤±è´¥: {e}")
        return year, pd.DataFrame()

def calculate_metrics_parallel(annual_networks: Dict[int, nx.DiGraph], 
                             metric_type: str = 'all',
                             n_processes: int = None) -> pd.DataFrame:
    """
    å¹¶è¡Œè®¡ç®—å¤šå¹´ä»½ç½‘ç»œæŒ‡æ ‡
    
    Args:
        annual_networks: å¹´åº¦ç½‘ç»œå­—å…¸
        metric_type: è®¡ç®—ç±»å‹ ('all', 'node', 'global')
        n_processes: è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
        
    Returns:
        åŒ…å«æ‰€æœ‰å¹´ä»½æŒ‡æ ‡çš„DataFrame
        
    Example:
        >>> networks = {2020: G2020, 2021: G2021, 2022: G2022}
        >>> result_df = calculate_metrics_parallel(networks, 'all', 4)
    """
    if not annual_networks:
        logger.warning("æ²¡æœ‰ç½‘ç»œæ•°æ®ï¼Œè¿”å›ç©ºDataFrame")
        return pd.DataFrame()
    
    if n_processes is None:
        n_processes = min(mp.cpu_count(), len(annual_networks))
    
    logger.info(f"ğŸš€ å¯åŠ¨å¹¶è¡Œè®¡ç®— - {len(annual_networks)} ä¸ªå¹´ä»½ï¼Œ{n_processes} ä¸ªè¿›ç¨‹")
    
    start_time = time.time()
    
    # å‡†å¤‡å‚æ•°
    args_list = [(G, year, metric_type) for year, G in annual_networks.items()]
    
    try:
        # å¯åŠ¨å¤šè¿›ç¨‹è®¡ç®—
        with mp.Pool(processes=n_processes) as pool:
            results = pool.map(_calculate_metrics_worker, args_list)
        
        # åˆå¹¶ç»“æœ
        all_dataframes = []
        successful_years = []
        
        for year, df in results:
            if not df.empty:
                all_dataframes.append(df)
                successful_years.append(year)
            else:
                logger.warning(f"âŒ {year} å¹´è®¡ç®—å¤±è´¥æˆ–è¿”å›ç©ºç»“æœ")
        
        if all_dataframes:
            combined_df = pd.concat(all_dataframes, ignore_index=True)
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… å¹¶è¡Œè®¡ç®—å®Œæˆ - {len(successful_years)}/{len(annual_networks)} å¹´ä»½æˆåŠŸï¼Œ"
                       f"ç”¨æ—¶ {elapsed:.2f} ç§’ï¼Œé€Ÿåº¦æå‡çº¦ {len(successful_years)/elapsed:.1f}x")
            
            return combined_df
        else:
            logger.error("âŒ æ‰€æœ‰å¹´ä»½è®¡ç®—éƒ½å¤±è´¥äº†")
            return pd.DataFrame()
            
    except Exception as e:
        logger.error(f"âŒ å¹¶è¡Œè®¡ç®—å¤±è´¥: {e}")
        # å›é€€åˆ°ä¸²è¡Œè®¡ç®—
        logger.info("ğŸ”„ å›é€€åˆ°ä¸²è¡Œè®¡ç®—...")
        return _fallback_serial_calculation(annual_networks, metric_type)

def _fallback_serial_calculation(annual_networks: Dict[int, nx.DiGraph], 
                                metric_type: str) -> pd.DataFrame:
    """
    å¹¶è¡Œè®¡ç®—å¤±è´¥æ—¶çš„ä¸²è¡Œå›é€€æ–¹æ¡ˆ
    
    Args:
        annual_networks: å¹´åº¦ç½‘ç»œå­—å…¸
        metric_type: è®¡ç®—ç±»å‹
        
    Returns:
        ç»“æœDataFrame
    """
    logger.info("ä½¿ç”¨ä¸²è¡Œè®¡ç®—æ¨¡å¼...")
    
    all_results = []
    
    for year in sorted(annual_networks.keys()):
        G = annual_networks[year]
        try:
            year, result = _calculate_metrics_worker((G, year, metric_type))
            if not result.empty:
                all_results.append(result)
        except Exception as e:
            logger.error(f"ä¸²è¡Œè®¡ç®— {year} å¹´å¤±è´¥: {e}")
            continue
    
    if all_results:
        return pd.concat(all_results, ignore_index=True)
    else:
        return pd.DataFrame()

def calculate_centralities_batch(graphs_and_years: List[Tuple[nx.DiGraph, int]], 
                               centrality_functions: List[Callable],
                               n_processes: int = None) -> Dict[str, pd.DataFrame]:
    """
    æ‰¹é‡å¹¶è¡Œè®¡ç®—ç‰¹å®šä¸­å¿ƒæ€§æŒ‡æ ‡
    
    Args:
        graphs_and_years: (å›¾, å¹´ä»½) å…ƒç»„åˆ—è¡¨
        centrality_functions: ä¸­å¿ƒæ€§è®¡ç®—å‡½æ•°åˆ—è¡¨
        n_processes: è¿›ç¨‹æ•°
        
    Returns:
        {å‡½æ•°å: ç»“æœDataFrame} å­—å…¸
        
    Example:
        >>> from .node_metrics import calculate_degree_centrality, calculate_pagerank_centrality
        >>> functions = [calculate_degree_centrality, calculate_pagerank_centrality]
        >>> results = calculate_centralities_batch(graphs_years, functions)
    """
    if n_processes is None:
        n_processes = min(mp.cpu_count(), len(graphs_and_years))
    
    logger.info(f"æ‰¹é‡å¹¶è¡Œè®¡ç®— {len(centrality_functions)} ç§ä¸­å¿ƒæ€§æŒ‡æ ‡ï¼Œ"
               f"{len(graphs_and_years)} ä¸ªç½‘ç»œï¼Œ{n_processes} ä¸ªè¿›ç¨‹")
    
    results = {}
    
    for func in centrality_functions:
        func_name = func.__name__
        logger.info(f"  è®¡ç®— {func_name}...")
        
        start_time = time.time()
        
        try:
            # åˆ›å»ºéƒ¨åˆ†å‡½æ•°
            worker_func = partial(_centrality_worker, func=func)
            
            with mp.Pool(processes=n_processes) as pool:
                func_results = pool.map(worker_func, graphs_and_years)
            
            # åˆå¹¶ç»“æœ
            valid_results = [df for df in func_results if not df.empty]
            if valid_results:
                results[func_name] = pd.concat(valid_results, ignore_index=True)
            else:
                results[func_name] = pd.DataFrame()
            
            elapsed = time.time() - start_time
            logger.info(f"  {func_name} å®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f} ç§’")
            
        except Exception as e:
            logger.error(f"  {func_name} è®¡ç®—å¤±è´¥: {e}")
            results[func_name] = pd.DataFrame()
    
    return results

def _centrality_worker(graph_year: Tuple[nx.DiGraph, int], func: Callable) -> pd.DataFrame:
    """
    ä¸­å¿ƒæ€§è®¡ç®—å·¥ä½œå‡½æ•°
    
    Args:
        graph_year: (å›¾, å¹´ä»½) å…ƒç»„
        func: ä¸­å¿ƒæ€§è®¡ç®—å‡½æ•°
        
    Returns:
        ç»“æœDataFrame
    """
    G, year = graph_year
    try:
        return func(G, year)
    except Exception as e:
        logger.error(f"è®¡ç®— {year} å¹´ {func.__name__} å¤±è´¥: {e}")
        return pd.DataFrame()

def estimate_computation_time(annual_networks: Dict[int, nx.DiGraph], 
                            metric_type: str = 'all',
                            sample_size: int = 3) -> Dict[str, float]:
    """
    ä¼°ç®—è®¡ç®—æ—¶é—´
    
    Args:
        annual_networks: å¹´åº¦ç½‘ç»œå­—å…¸
        metric_type: è®¡ç®—ç±»å‹
        sample_size: ç”¨äºä¼°ç®—çš„æ ·æœ¬æ•°é‡
        
    Returns:
        æ—¶é—´ä¼°ç®—å­—å…¸
    """
    if len(annual_networks) == 0:
        return {'serial_time': 0, 'parallel_time': 0, 'speedup': 1}
    
    # é€‰æ‹©æ ·æœ¬å¹´ä»½
    years = sorted(annual_networks.keys())
    sample_years = years[:min(sample_size, len(years))]
    
    logger.info(f"ä½¿ç”¨ {len(sample_years)} ä¸ªå¹´ä»½æ ·æœ¬ä¼°ç®—è®¡ç®—æ—¶é—´...")
    
    total_sample_time = 0
    
    for year in sample_years:
        G = annual_networks[year]
        
        start_time = time.time()
        try:
            _calculate_metrics_worker((G, year, metric_type))
        except:
            pass
        elapsed = time.time() - start_time
        
        total_sample_time += elapsed
        logger.debug(f"  {year} å¹´æ ·æœ¬ç”¨æ—¶: {elapsed:.2f} ç§’")
    
    # ä¼°ç®—æ€»æ—¶é—´
    avg_time_per_year = total_sample_time / len(sample_years)
    total_years = len(annual_networks)
    
    estimated_serial_time = avg_time_per_year * total_years
    
    # ä¼°ç®—å¹¶è¡Œæ—¶é—´ï¼ˆè€ƒè™‘å¹¶è¡Œå¼€é”€ï¼‰
    n_processes = min(mp.cpu_count(), total_years)
    parallel_efficiency = 0.8  # å‡è®¾80%çš„å¹¶è¡Œæ•ˆç‡
    estimated_parallel_time = (estimated_serial_time / n_processes) / parallel_efficiency
    
    speedup = estimated_serial_time / estimated_parallel_time if estimated_parallel_time > 0 else 1
    
    estimates = {
        'serial_time': estimated_serial_time,
        'parallel_time': estimated_parallel_time,
        'speedup': speedup,
        'recommended_parallel': speedup > 1.5 and total_years >= 4
    }
    
    logger.info(f"æ—¶é—´ä¼°ç®— - ä¸²è¡Œ: {estimated_serial_time:.1f}s, "
               f"å¹¶è¡Œ: {estimated_parallel_time:.1f}s, "
               f"åŠ é€Ÿæ¯”: {speedup:.1f}x")
    
    return estimates

def get_optimal_process_count(annual_networks: Dict[int, nx.DiGraph]) -> int:
    """
    è·å–æœ€ä¼˜è¿›ç¨‹æ•°
    
    Args:
        annual_networks: å¹´åº¦ç½‘ç»œå­—å…¸
        
    Returns:
        æ¨èçš„è¿›ç¨‹æ•°
    """
    n_years = len(annual_networks)
    n_cpus = mp.cpu_count()
    
    # åŸºäºç½‘ç»œè§„æ¨¡è°ƒæ•´
    avg_nodes = sum(G.number_of_nodes() for G in annual_networks.values()) / n_years if n_years > 0 else 0
    avg_edges = sum(G.number_of_edges() for G in annual_networks.values()) / n_years if n_years > 0 else 0
    
    # å¤§ç½‘ç»œéœ€è¦æ›´å°‘çš„å¹¶è¡Œè¿›ç¨‹ï¼ˆå› ä¸ºæ¯ä¸ªè¿›ç¨‹å ç”¨æ›´å¤šå†…å­˜ï¼‰
    if avg_nodes > 200 or avg_edges > 2000:
        optimal_processes = min(n_cpus // 2, n_years, 4)
    elif avg_nodes > 100 or avg_edges > 1000:
        optimal_processes = min(n_cpus - 1, n_years, 6)
    else:
        optimal_processes = min(n_cpus, n_years, 8)
    
    logger.info(f"æ¨èè¿›ç¨‹æ•°: {optimal_processes} "
               f"(åŸºäº {n_years} ä¸ªç½‘ç»œï¼Œå¹³å‡ {avg_nodes:.0f} èŠ‚ç‚¹ {avg_edges:.0f} è¾¹)")
    
    return max(1, optimal_processes)