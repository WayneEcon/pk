#!/usr/bin/env python3
"""
ä¸ªæ€§åŒ–PageRankè®¡ç®—æ¨¡å— (Personalized PageRank Calculator)
==========================================================

è®¡ç®—ç¾å›½åœ¨å…¨çƒèƒ½æºè´¸æ˜“ç½‘ç»œä¸­çš„æ–¹å‘æ€§PageRankå½±å“åŠ›ï¼Œä¸ºDLIåˆ†ææä¾›æ–°çš„ç½‘ç»œä¸­å¿ƒæ€§ç»´åº¦ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å‡ºå£é”å®šå½±å“åŠ›ï¼šä»¥ç¾å›½ä¸ºç§å­èŠ‚ç‚¹ï¼Œè®¡ç®—ç¾å›½å¯¹å…¶ä»–å›½å®¶çš„ç½‘ç»œå½±å“åŠ›
2. è¿›å£é”å®šå½±å“åŠ›ï¼šä»¥å…¶ä»–å›½å®¶ä¸ºç§å­èŠ‚ç‚¹ï¼Œè®¡ç®—å®ƒä»¬å¯¹ç¾å›½çš„ç½‘ç»œå½±å“åŠ›

è¾“å‡ºæ–‡ä»¶ï¼špersonalized_pagerank_panel.csv
- year: å¹´ä»½
- country_name: å›½å®¶åç§°  
- ppr_us_export_influence: ç¾å›½å¯¹è¯¥å›½çš„å‡ºå£é”å®šç½‘ç»œå½±å“åŠ›
- ppr_influence_on_us: è¯¥å›½å¯¹ç¾å›½çš„è¿›å£é”å®šç½‘ç»œå½±å“åŠ›

ä½œè€…ï¼šEnergy Network Analysis Team
ç‰ˆæœ¬ï¼šv1.0
"""

import pandas as pd
import networkx as nx
import numpy as np
from pathlib import Path
import logging
import argparse
import sys
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PersonalizedPageRankCalculator:
    """ä¸ªæ€§åŒ–PageRankè®¡ç®—å™¨"""
    
    def __init__(self, networks_dir: Path, output_dir: Path):
        """
        åˆå§‹åŒ–è®¡ç®—å™¨
        
        Args:
            networks_dir: ç½‘ç»œæ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        self.networks_dir = Path(networks_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ“ ç½‘ç»œæ•°æ®ç›®å½•: {self.networks_dir}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_annual_networks(self) -> Dict[int, nx.Graph]:
        """
        åŠ è½½å¹´åº¦ç½‘ç»œæ•°æ®
        
        Returns:
            Dict[int, nx.Graph]: å¹´ä»½åˆ°ç½‘ç»œå›¾çš„æ˜ å°„
        """
        logger.info("ğŸ“‚ å¼€å§‹åŠ è½½å¹´åº¦ç½‘ç»œæ•°æ®...")
        
        networks = {}
        
        # å°è¯•å¤šç§å¯èƒ½çš„æ–‡ä»¶æ ¼å¼å’Œå‘½å
        potential_patterns = [
            "network_*.graphml",
            "network_*.gexf", 
            "*_network.graphml",
            "*.graphml",
            "*.gexf"
        ]
        
        for pattern in potential_patterns:
            network_files = list(self.networks_dir.glob(pattern))
            if network_files:
                logger.info(f"   æ‰¾åˆ°ç½‘ç»œæ–‡ä»¶æ¨¡å¼: {pattern}")
                break
        else:
            # å°è¯•ä»pickleæ–‡ä»¶åŠ è½½
            pkl_files = list(self.networks_dir.glob("*.pkl"))
            if pkl_files:
                logger.info("   å°è¯•ä»pickleæ–‡ä»¶åŠ è½½ç½‘ç»œæ•°æ®...")
                import pickle
                for pkl_file in pkl_files:
                    try:
                        with open(pkl_file, 'rb') as f:
                            annual_networks = pickle.load(f)
                        if isinstance(annual_networks, dict):
                            networks.update(annual_networks)
                            logger.info(f"   âœ… ä»{pkl_file.name}åŠ è½½äº†{len(annual_networks)}ä¸ªå¹´åº¦ç½‘ç»œ")
                            return networks
                    except Exception as e:
                        logger.warning(f"   âš ï¸ åŠ è½½{pkl_file}å¤±è´¥: {e}")
            
            raise FileNotFoundError(f"åœ¨{self.networks_dir}ä¸­æœªæ‰¾åˆ°ç½‘ç»œæ–‡ä»¶")
        
        # ä»GraphML/GEXFæ–‡ä»¶åŠ è½½
        for network_file in network_files:
            try:
                # ä»æ–‡ä»¶åæå–å¹´ä»½
                filename = network_file.stem
                year = None
                
                # å°è¯•å¤šç§å¹´ä»½æå–æ–¹å¼
                for part in filename.split('_'):
                    if part.isdigit() and len(part) == 4:
                        potential_year = int(part)
                        if 2000 <= potential_year <= 2030:
                            year = potential_year
                            break
                
                if year is None:
                    logger.warning(f"   âš ï¸ æ— æ³•ä»æ–‡ä»¶å{filename}æå–å¹´ä»½ï¼Œè·³è¿‡")
                    continue
                
                # åŠ è½½ç½‘ç»œ
                if network_file.suffix == '.graphml':
                    G = nx.read_graphml(network_file)
                elif network_file.suffix == '.gexf':
                    G = nx.read_gexf(network_file)
                else:
                    continue
                
                if G.number_of_nodes() > 0:
                    networks[year] = G
                    logger.info(f"   âœ… {year}: {G.number_of_nodes()}èŠ‚ç‚¹, {G.number_of_edges()}è¾¹")
                
            except Exception as e:
                logger.warning(f"   âš ï¸ åŠ è½½{network_file}å¤±è´¥: {e}")
                continue
        
        if not networks:
            raise ValueError("æœªæˆåŠŸåŠ è½½ä»»ä½•ç½‘ç»œæ•°æ®")
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½{len(networks)}ä¸ªå¹´åº¦ç½‘ç»œ ({min(networks.keys())}-{max(networks.keys())})")
        return networks
    
    def calculate_us_export_influence(self, G: nx.Graph) -> Dict[str, float]:
        """
        è®¡ç®—ç¾å›½çš„å‡ºå£é”å®šå½±å“åŠ›
        
        ä»¥ç¾å›½ä¸ºç§å­èŠ‚ç‚¹ï¼Œè®¡ç®—ç¾å›½å¯¹å…¶ä»–å›½å®¶çš„ç½‘ç»œå½±å“åŠ›
        
        Args:
            G: ç½‘ç»œå›¾
            
        Returns:
            Dict[str, float]: å›½å®¶åˆ°å½±å“åŠ›åˆ†æ•°çš„æ˜ å°„
        """
        if 'USA' not in G.nodes():
            logger.warning("   âš ï¸ ç½‘ç»œä¸­æœªæ‰¾åˆ°USAèŠ‚ç‚¹")
            return {}
        
        # ä»¥ç¾å›½ä¸ºå”¯ä¸€ç§å­èŠ‚ç‚¹
        personalization = {node: 1.0 if node == 'USA' else 0.0 for node in G.nodes()}
        
        try:
            # è®¡ç®—ä¸ªæ€§åŒ–PageRank
            pagerank_scores = nx.pagerank(
                G, 
                personalization=personalization,
                max_iter=1000,
                tol=1e-6
            )
            
            return pagerank_scores
            
        except Exception as e:
            logger.warning(f"   âš ï¸ è®¡ç®—ç¾å›½å‡ºå£å½±å“åŠ›å¤±è´¥: {e}")
            return {}
    
    def calculate_influence_on_us(self, G: nx.Graph) -> Dict[str, float]:
        """
        è®¡ç®—å…¶ä»–å›½å®¶å¯¹ç¾å›½çš„è¿›å£é”å®šå½±å“åŠ›
        
        éå†æ¯ä¸ªå›½å®¶ä½œä¸ºç§å­èŠ‚ç‚¹ï¼Œè®¡ç®—å…¶å¯¹ç¾å›½çš„ç½‘ç»œå½±å“åŠ›
        
        Args:
            G: ç½‘ç»œå›¾
            
        Returns:
            Dict[str, float]: å›½å®¶åˆ°å¯¹ç¾å½±å“åŠ›åˆ†æ•°çš„æ˜ å°„
        """
        if 'USA' not in G.nodes():
            logger.warning("   âš ï¸ ç½‘ç»œä¸­æœªæ‰¾åˆ°USAèŠ‚ç‚¹")
            return {}
        
        influence_on_us = {}
        non_us_nodes = [node for node in G.nodes() if node != 'USA']
        
        for country in non_us_nodes:
            try:
                # ä»¥å½“å‰å›½å®¶ä¸ºå”¯ä¸€ç§å­èŠ‚ç‚¹
                personalization = {node: 1.0 if node == country else 0.0 for node in G.nodes()}
                
                # è®¡ç®—ä¸ªæ€§åŒ–PageRank
                pagerank_scores = nx.pagerank(
                    G,
                    personalization=personalization,
                    max_iter=1000,
                    tol=1e-6
                )
                
                # æå–è¯¥å›½å®¶å¯¹ç¾å›½çš„å½±å“åŠ›åˆ†æ•°
                influence_on_us[country] = pagerank_scores.get('USA', 0.0)
                
            except Exception as e:
                logger.warning(f"   âš ï¸ è®¡ç®—{country}å¯¹ç¾å›½å½±å“åŠ›å¤±è´¥: {e}")
                influence_on_us[country] = 0.0
        
        return influence_on_us
    
    def calculate_annual_personalized_pagerank(self, year: int, G: nx.Graph) -> pd.DataFrame:
        """
        è®¡ç®—å•å¹´åº¦çš„ä¸ªæ€§åŒ–PageRankæŒ‡æ ‡
        
        Args:
            year: å¹´ä»½
            G: ç½‘ç»œå›¾
            
        Returns:
            pd.DataFrame: åŒ…å«è¯¥å¹´æ‰€æœ‰å›½å®¶ä¸ªæ€§åŒ–PageRankåˆ†æ•°çš„DataFrame
        """
        logger.info(f"ğŸ“Š è®¡ç®—{year}å¹´ä¸ªæ€§åŒ–PageRank...")
        
        # 1. è®¡ç®—ç¾å›½å‡ºå£å½±å“åŠ›
        logger.info(f"   è®¡ç®—ç¾å›½å‡ºå£å½±å“åŠ›...")
        us_export_influence = self.calculate_us_export_influence(G)
        
        # 2. è®¡ç®—å„å›½å¯¹ç¾å›½è¿›å£å½±å“åŠ›  
        logger.info(f"   è®¡ç®—å„å›½å¯¹ç¾è¿›å£å½±å“åŠ›...")
        influence_on_us = self.calculate_influence_on_us(G)
        
        # 3. æ„å»ºç»“æœDataFrame
        results = []
        all_countries = set(us_export_influence.keys()) | set(influence_on_us.keys())
        
        for country in all_countries:
            if country == 'USA':  # ç¾å›½è‡ªå·±çš„æ•°æ®ç‰¹æ®Šå¤„ç†
                results.append({
                    'year': year,
                    'country_name': country,
                    'ppr_us_export_influence': us_export_influence.get(country, 0.0),
                    'ppr_influence_on_us': 0.0  # ç¾å›½å¯¹è‡ªå·±çš„å½±å“åŠ›è®¾ä¸º0
                })
            else:
                results.append({
                    'year': year,
                    'country_name': country,
                    'ppr_us_export_influence': us_export_influence.get(country, 0.0),
                    'ppr_influence_on_us': influence_on_us.get(country, 0.0)
                })
        
        year_df = pd.DataFrame(results)
        logger.info(f"   âœ… {year}å¹´å®Œæˆ: {len(year_df)}ä¸ªå›½å®¶")
        
        return year_df
    
    def calculate_all_years(self, networks: Dict[int, nx.Graph]) -> pd.DataFrame:
        """
        è®¡ç®—æ‰€æœ‰å¹´ä»½çš„ä¸ªæ€§åŒ–PageRank
        
        Args:
            networks: å¹´åº¦ç½‘ç»œå­—å…¸
            
        Returns:
            pd.DataFrame: åŒ…å«æ‰€æœ‰å¹´ä»½æ‰€æœ‰å›½å®¶çš„å®Œæ•´é¢æ¿æ•°æ®
        """
        logger.info(f"ğŸš€ å¼€å§‹è®¡ç®—{len(networks)}ä¸ªå¹´ä»½çš„ä¸ªæ€§åŒ–PageRank...")
        
        all_results = []
        
        for year in sorted(networks.keys()):
            G = networks[year]
            try:
                year_results = self.calculate_annual_personalized_pagerank(year, G)
                all_results.append(year_results)
            except Exception as e:
                logger.error(f"   âŒ {year}å¹´è®¡ç®—å¤±è´¥: {e}")
                continue
        
        if not all_results:
            raise ValueError("æ‰€æœ‰å¹´ä»½çš„è®¡ç®—éƒ½å¤±è´¥äº†")
        
        # åˆå¹¶æ‰€æœ‰å¹´ä»½æ•°æ®
        complete_df = pd.concat(all_results, ignore_index=True)
        
        logger.info(f"âœ… ä¸ªæ€§åŒ–PageRankè®¡ç®—å®Œæˆ: {len(complete_df)}æ¡è®°å½•ï¼Œè¦†ç›–{len(complete_df['year'].unique())}å¹´")
        
        return complete_df
    
    def generate_summary_stats(self, df: pd.DataFrame) -> Dict:
        """
        ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡
        
        Args:
            df: ä¸ªæ€§åŒ–PageRankç»“æœDataFrame
            
        Returns:
            Dict: æ‘˜è¦ç»Ÿè®¡å­—å…¸
        """
        stats = {
            'total_records': len(df),
            'years_covered': sorted(df['year'].unique().tolist()),
            'countries_count': df['country_name'].nunique(),
            'year_range': f"{df['year'].min()}-{df['year'].max()}",
            
            # ç¾å›½å‡ºå£å½±å“åŠ›ç»Ÿè®¡
            'us_export_influence': {
                'mean': df['ppr_us_export_influence'].mean(),
                'std': df['ppr_us_export_influence'].std(),
                'max': df['ppr_us_export_influence'].max(),
                'min': df['ppr_us_export_influence'].min()
            },
            
            # å¯¹ç¾è¿›å£å½±å“åŠ›ç»Ÿè®¡
            'influence_on_us': {
                'mean': df['ppr_influence_on_us'].mean(),
                'std': df['ppr_influence_on_us'].std(), 
                'max': df['ppr_influence_on_us'].max(),
                'min': df['ppr_influence_on_us'].min()
            }
        }
        
        # ç¾å›½å‡ºå£å½±å“åŠ›æœ€é«˜çš„5ä¸ªå›½å®¶ï¼ˆæœ€æ–°å¹´ä»½ï¼‰
        latest_year = df['year'].max()
        latest_data = df[df['year'] == latest_year]
        
        top_us_export_targets = latest_data.nlargest(5, 'ppr_us_export_influence')[
            ['country_name', 'ppr_us_export_influence']
        ].to_dict('records')
        
        top_influence_on_us = latest_data.nlargest(5, 'ppr_influence_on_us')[
            ['country_name', 'ppr_influence_on_us']  
        ].to_dict('records')
        
        stats['top_rankings'] = {
            'latest_year': latest_year,
            'top_us_export_influence': top_us_export_targets,
            'top_influence_on_us': top_influence_on_us
        }
        
        return stats
    
    def save_results(self, df: pd.DataFrame) -> Tuple[Path, Path]:
        """
        ä¿å­˜è®¡ç®—ç»“æœ
        
        Args:
            df: ç»“æœDataFrame
            
        Returns:
            Tuple[Path, Path]: CSVæ–‡ä»¶è·¯å¾„å’Œæ‘˜è¦JSONæ–‡ä»¶è·¯å¾„
        """
        # ä¿å­˜ä¸»è¦æ•°æ®æ–‡ä»¶
        csv_path = self.output_dir / "personalized_pagerank_panel.csv"
        df.to_csv(csv_path, index=False)
        logger.info(f"ğŸ’¾ ä¸»æ•°æ®æ–‡ä»¶å·²ä¿å­˜: {csv_path}")
        
        # ç”Ÿæˆå¹¶ä¿å­˜æ‘˜è¦ç»Ÿè®¡
        stats = self.generate_summary_stats(df)
        
        import json
        json_path = self.output_dir / "personalized_pagerank_summary.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False, default=str)
        logger.info(f"ğŸ“Š æ‘˜è¦ç»Ÿè®¡å·²ä¿å­˜: {json_path}")
        
        return csv_path, json_path
    
    def run_full_calculation(self) -> Tuple[pd.DataFrame, Path, Path]:
        """
        è¿è¡Œå®Œæ•´çš„ä¸ªæ€§åŒ–PageRankè®¡ç®—æµç¨‹
        
        Returns:
            Tuple[pd.DataFrame, Path, Path]: ç»“æœDataFrameã€CSVè·¯å¾„ã€JSONè·¯å¾„
        """
        logger.info("=" * 60)
        logger.info("ğŸŒŸ ä¸ªæ€§åŒ–PageRankè®¡ç®—ç³»ç»Ÿå¯åŠ¨")
        logger.info("=" * 60)
        
        try:
            # 1. åŠ è½½ç½‘ç»œæ•°æ®
            networks = self.load_annual_networks()
            
            # 2. è®¡ç®—ä¸ªæ€§åŒ–PageRank
            results_df = self.calculate_all_years(networks)
            
            # 3. ä¿å­˜ç»“æœ
            csv_path, json_path = self.save_results(results_df)
            
            # 4. è¾“å‡ºå®Œæˆä¿¡æ¯
            logger.info("=" * 60)
            logger.info("ğŸ‰ ä¸ªæ€§åŒ–PageRankè®¡ç®—å®Œæˆ!")
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {len(results_df):,}")
            logger.info(f"ğŸ“… è¦†ç›–å¹´ä»½: {results_df['year'].min()}-{results_df['year'].max()}")
            logger.info(f"ğŸŒ è¦†ç›–å›½å®¶: {results_df['country_name'].nunique()}")
            logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {csv_path.name}, {json_path.name}")
            
            return results_df, csv_path, json_path
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—è¿‡ç¨‹å¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸ªæ€§åŒ–PageRankè®¡ç®—ç³»ç»Ÿ v1.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python personalized_pagerank.py                                              # ä½¿ç”¨é»˜è®¤è·¯å¾„
  python personalized_pagerank.py --networks-dir ../02_net_analysis/outputs/networks  # æŒ‡å®šç½‘ç»œæ•°æ®ç›®å½•
  python personalized_pagerank.py --output-dir ./outputs                       # æŒ‡å®šè¾“å‡ºç›®å½•
        """
    )
    
    # è®¾ç½®é»˜è®¤è·¯å¾„
    current_dir = Path(__file__).parent
    default_networks_dir = current_dir.parent / "02_net_analysis" / "outputs" / "networks"
    default_output_dir = current_dir / "outputs"
    
    parser.add_argument(
        '--networks-dir', 
        type=str, 
        default=str(default_networks_dir),
        help=f'ç½‘ç»œæ•°æ®ç›®å½• (é»˜è®¤: {default_networks_dir})'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str, 
        default=str(default_output_dir),
        help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {default_output_dir})'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # åˆ›å»ºè®¡ç®—å™¨å¹¶æ‰§è¡Œ
        calculator = PersonalizedPageRankCalculator(
            networks_dir=Path(args.networks_dir),
            output_dir=Path(args.output_dir)
        )
        
        results_df, csv_path, json_path = calculator.run_full_calculation()
        
        print(f"\nâœ… ä¸ªæ€§åŒ–PageRankè®¡ç®—æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“Š ç»“æœæ–‡ä»¶: {csv_path}")
        print(f"ğŸ“ˆ æ‘˜è¦æ–‡ä»¶: {json_path}")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ è®¡ç®—å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)