#!/usr/bin/env python3
"""
ç½‘ç»œæŒ‡æ ‡è®¡ç®—æ¨¡å— (03_metrics)
====================================

è¿™æ˜¯ä¸€ä¸ªå…¨é¢é‡æ„çš„ç½‘ç»œæŒ‡æ ‡è®¡ç®—æ¨¡å—ï¼Œæä¾›èŠ‚ç‚¹çº§åˆ«å’Œå…¨å±€çº§åˆ«çš„ç½‘ç»œåˆ†æåŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- èŠ‚ç‚¹çº§åˆ«æŒ‡æ ‡ (node_metrics): åº¦ä¸­å¿ƒæ€§ã€å¼ºåº¦ä¸­å¿ƒæ€§ã€ä¸­ä»‹ä¸­å¿ƒæ€§ã€PageRankã€ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
- å…¨å±€çº§åˆ«æŒ‡æ ‡ (global_metrics): ç½‘ç»œå¯†åº¦ã€è¿é€šæ€§ã€è·¯å¾„é•¿åº¦ã€èšç±»ç³»æ•°ã€ç½‘ç»œæ•ˆç‡
- ç»Ÿä¸€å·¥å…·å‡½æ•° (utils): æ•°æ®éªŒè¯ã€ç¼“å­˜ã€é”™è¯¯å¤„ç†

æ ¸å¿ƒä¿®æ­£ï¼š
- ä¿®æ­£äº†åŠ æƒæœ€çŸ­è·¯å¾„è®¡ç®—é€»è¾‘ï¼ˆä½¿ç”¨distance = 1/weightï¼‰
- æ¶ˆé™¤äº†ä»£ç é‡å¤ï¼Œæä¾›ç»Ÿä¸€çš„å·¥å…·å‡½æ•°
- å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ•°æ®éªŒè¯æœºåˆ¶
- æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜æœºåˆ¶ã€é‡‡æ ·ç­–ç•¥ã€è®¡æ—¶è£…é¥°å™¨

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from 03_metrics import calculate_all_metrics_for_year
    
    # è®¡ç®—å•ä¸ªå¹´ä»½çš„æ‰€æœ‰æŒ‡æ ‡
    all_metrics_df = calculate_all_metrics_for_year(G, 2020)
    
    # æˆ–åˆ†åˆ«è®¡ç®—
    from 03_metrics import calculate_all_node_centralities, calculate_all_global_metrics
    
    node_metrics_df = calculate_all_node_centralities(G, 2020)
    global_metrics_dict = calculate_all_global_metrics(G, 2020)
"""

import pandas as pd
import networkx as nx
from typing import Dict, Any, List, Tuple

# å¯¼å…¥æ ¸å¿ƒåŠŸèƒ½
from node_metrics import (
    calculate_all_node_centralities,
    get_node_centrality_rankings,
    get_node_centrality_summary
)

from global_metrics import (
    calculate_all_global_metrics,
    get_global_metrics_summary
)

from utils import (
    setup_logger, validate_graph, safe_divide, timer_decorator,
    merge_metric_dataframes, create_metrics_summary, validate_metrics_result
)

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = '3.0.0'
__author__ = 'Energy Network Analysis Team'

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__)

def calculate_all_metrics_for_year(G: nx.DiGraph, year: int) -> pd.DataFrame:
    """
    è®¡ç®—å•ä¸ªå¹´ä»½ç½‘ç»œçš„æ‰€æœ‰æŒ‡æ ‡ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
    
    Args:
        G: NetworkXæœ‰å‘å›¾
        year: å¹´ä»½
        
    Returns:
        åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„DataFrameï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªèŠ‚ç‚¹åŠå…¶æŒ‡æ ‡
        
    Raises:
        ValueError: å½“è¾“å…¥å›¾æ— æ•ˆæ—¶
        
    Example:
        >>> G = nx.DiGraph()
        >>> G.add_edge('A', 'B', weight=100)
        >>> metrics_df = calculate_all_metrics_for_year(G, 2020)
        >>> print(metrics_df.columns)
    """
    logger.info(f"ğŸš€ {year}: å¼€å§‹è®¡ç®—æ‰€æœ‰ç½‘ç»œæŒ‡æ ‡...")
    
    # éªŒè¯è¾“å…¥
    validate_graph(G, "calculate_all_metrics_for_year")
    
    try:
        # 1. è®¡ç®—èŠ‚ç‚¹çº§åˆ«æŒ‡æ ‡
        node_metrics_df = calculate_all_node_centralities(G, year)
        
        # 2. è®¡ç®—å…¨å±€çº§åˆ«æŒ‡æ ‡ï¼ˆç°åœ¨è¿”å›å•è¡ŒDataFrameï¼‰
        global_metrics_df = calculate_all_global_metrics(G, year)
        
        # 3. ä½¿ç”¨pandas mergeè¿›è¡Œæ•°æ®åˆå¹¶ï¼ˆåŸºäºå¹´ä»½ï¼‰
        # å…¨å±€æŒ‡æ ‡ä¼šè‡ªåŠ¨å¹¿æ’­åˆ°æ¯ä¸€è¡ŒèŠ‚ç‚¹æ•°æ®
        combined_df = pd.merge(node_metrics_df, global_metrics_df, on='year', how='left')
        
        # ä¸ºå…¨å±€æŒ‡æ ‡æ·»åŠ global_å‰ç¼€ï¼ˆé™¤äº†yearåˆ—ï¼‰
        global_cols_to_rename = {col: f'global_{col}' for col in global_metrics_df.columns if col != 'year'}
        if global_cols_to_rename:
            combined_df = combined_df.rename(columns=global_cols_to_rename)
        
        logger.info(f"ğŸ¯ {year}: æ‰€æœ‰ç½‘ç»œæŒ‡æ ‡è®¡ç®—å®Œæˆ - {len(combined_df)} ä¸ªèŠ‚ç‚¹ï¼Œ{len(combined_df.columns)} ä¸ªæŒ‡æ ‡")
        
        return combined_df
        
    except Exception as e:
        logger.error(f"âŒ {year}: æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        raise

def calculate_metrics_for_multiple_years(annual_networks: Dict[int, nx.DiGraph]) -> pd.DataFrame:
    """
    å¹¶è¡Œè®¡ç®—å¤šä¸ªå¹´ä»½çš„ç½‘ç»œæŒ‡æ ‡
    
    Args:
        annual_networks: å¹´åº¦ç½‘ç»œå­—å…¸ {year: Graph}
        
    Returns:
        åŒ…å«æ‰€æœ‰å¹´ä»½æ‰€æœ‰æŒ‡æ ‡çš„å®Œæ•´DataFrame
        
    Example:
        >>> networks = {2020: G2020, 2021: G2021}
        >>> all_metrics_df = calculate_metrics_for_multiple_years(networks)
    """
    logger.info(f"ğŸŒŸ å¼€å§‹è®¡ç®—å¤šå¹´ä»½ç½‘ç»œæŒ‡æ ‡ - {len(annual_networks)} ä¸ªå¹´ä»½")
    
    if not annual_networks:
        logger.warning("æ²¡æœ‰ç½‘ç»œæ•°æ®ï¼Œè¿”å›ç©ºDataFrame")
        return pd.DataFrame()
    
    all_metrics_list = []
    
    for year in sorted(annual_networks.keys()):
        G = annual_networks[year]
        try:
            year_metrics = calculate_all_metrics_for_year(G, year)
            all_metrics_list.append(year_metrics)
        except Exception as e:
            logger.error(f"âŒ {year}å¹´æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            continue
    
    if all_metrics_list:
        # åˆå¹¶æ‰€æœ‰å¹´ä»½çš„æ•°æ®
        combined_df = pd.concat(all_metrics_list, ignore_index=True)
        logger.info(f"âœ… å¤šå¹´ä»½æŒ‡æ ‡è®¡ç®—å®Œæˆ - æ€»è®¡ {len(combined_df)} æ¡è®°å½•")
        return combined_df
    else:
        logger.error("æ‰€æœ‰å¹´ä»½è®¡ç®—éƒ½å¤±è´¥äº†")
        return pd.DataFrame()

def get_metrics_summary_report(metrics_df: pd.DataFrame) -> Dict[str, Any]:
    """
    ç”ŸæˆæŒ‡æ ‡è®¡ç®—çš„è¯¦ç»†æ‘˜è¦æŠ¥å‘Š
    
    Args:
        metrics_df: åŒ…å«æŒ‡æ ‡çš„DataFrame
        
    Returns:
        æ‘˜è¦æŠ¥å‘Šå­—å…¸
        
    Example:
        >>> summary = get_metrics_summary_report(metrics_df)
        >>> print(summary['total_records'])
    """
    if metrics_df.empty:
        return {'error': 'æ²¡æœ‰æ•°æ®å¯ç”¨äºç”Ÿæˆæ‘˜è¦'}
    
    years = sorted(metrics_df['year'].unique())
    
    summary = {
        'report_generated': pd.Timestamp.now().isoformat(),
        'total_records': len(metrics_df),
        'years_covered': len(years),
        'year_range': f"{min(years)} - {max(years)}" if years else "æ— æ•°æ®",
        'total_countries': metrics_df['country_code'].nunique(),
    }
    
    # æŒ‰å¹´ä»½ç»Ÿè®¡
    yearly_stats = []
    for year in years:
        year_data = metrics_df[metrics_df['year'] == year]
        
        # èŠ‚ç‚¹ä¸­å¿ƒæ€§æ’å
        top_by_strength = year_data.nlargest(5, 'total_strength')[['country_code', 'total_strength']].to_dict('records')
        top_by_pagerank = year_data.nlargest(5, 'pagerank_centrality')[['country_code', 'pagerank_centrality']].to_dict('records')
        
        yearly_stats.append({
            'year': year,
            'nodes': len(year_data),
            'avg_total_strength': year_data['total_strength'].mean(),
            'network_density': year_data['global_density'].iloc[0] if 'global_density' in year_data.columns else 0,
            'avg_path_length': year_data['global_avg_path_length'].iloc[0] if 'global_avg_path_length' in year_data.columns else 0,
            'top_countries_by_strength': top_by_strength,
            'top_countries_by_pagerank': top_by_pagerank
        })
    
    summary['yearly_statistics'] = yearly_stats
    
    # æ•´ä½“è¶‹åŠ¿åˆ†æ
    if len(years) > 1:
        # ç½‘ç»œè§„æ¨¡è¶‹åŠ¿
        network_sizes = [len(metrics_df[metrics_df['year'] == y]) for y in years]
        summary['network_growth'] = {
            'initial_size': network_sizes[0],
            'final_size': network_sizes[-1],
            'growth_rate': (network_sizes[-1] - network_sizes[0]) / network_sizes[0] if network_sizes[0] > 0 else 0
        }
        
        # å¯†åº¦è¶‹åŠ¿ï¼ˆå¦‚æœæœ‰å…¨å±€æŒ‡æ ‡ï¼‰
        if 'global_density' in metrics_df.columns:
            densities = [metrics_df[metrics_df['year'] == y]['global_density'].iloc[0] for y in years]
            summary['density_trend'] = {
                'initial_density': densities[0],
                'final_density': densities[-1],
                'density_change': densities[-1] - densities[0]
            }
    
    return summary

def export_metrics_to_files(metrics_df: pd.DataFrame, output_dir: str = None) -> Dict[str, str]:
    """
    å°†æŒ‡æ ‡ç»“æœå¯¼å‡ºåˆ°æ–‡ä»¶
    
    Args:
        metrics_df: åŒ…å«æŒ‡æ ‡çš„DataFrame
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        å¯¼å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        
    Example:
        >>> file_paths = export_metrics_to_files(metrics_df, "./results")
    """
    from pathlib import Path
    import os
    
    if output_dir is None:
        output_dir = Path(__file__).parent
        
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    exported_files = {}
    
    try:
        # 1. å®Œæ•´æ•°æ®CSV
        full_csv_path = output_path / "all_metrics.csv"
        metrics_df.to_csv(full_csv_path, index=False)
        exported_files['full_data'] = str(full_csv_path)
        
        # 2. èŠ‚ç‚¹ä¸­å¿ƒæ€§æ±‡æ€»
        node_centrality_cols = [col for col in metrics_df.columns 
                               if any(c in col for c in ['degree', 'strength', 'centrality'])]
        centrality_cols = ['year', 'country_code', 'country_name'] + node_centrality_cols
        centrality_df = metrics_df[centrality_cols]
        
        centrality_csv_path = output_path / "node_centrality_metrics.csv"
        centrality_df.to_csv(centrality_csv_path, index=False)
        exported_files['node_centrality'] = str(centrality_csv_path)
        
        # 3. å…¨å±€æŒ‡æ ‡æ±‡æ€»
        global_cols = [col for col in metrics_df.columns if col.startswith('global_')]
        if global_cols:
            global_cols = ['year'] + global_cols
            global_df = metrics_df[global_cols].drop_duplicates()
            
            global_csv_path = output_path / "global_network_metrics.csv"
            global_df.to_csv(global_csv_path, index=False)
            exported_files['global_metrics'] = str(global_csv_path)
        
        # 4. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary_report = get_metrics_summary_report(metrics_df)
        summary_json_path = output_path / "metrics_summary_report.json"
        
        import json
        with open(summary_json_path, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, ensure_ascii=False, indent=2, default=str)
        exported_files['summary_report'] = str(summary_json_path)
        
        logger.info(f"ğŸ“Š æŒ‡æ ‡æ•°æ®å·²å¯¼å‡ºåˆ° {output_dir}")
        
        return exported_files
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return {}

def run_full_metrics_calculation() -> bool:
    """
    è¿è¡Œå®Œæ•´çš„æŒ‡æ ‡è®¡ç®—æµç¨‹çš„ä¾¿æ·å‡½æ•°
    
    è‡ªåŠ¨åŠ è½½ç½‘ç»œæ•°æ®ï¼Œè®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼Œå¹¶å¯¼å‡ºç»“æœæ–‡ä»¶
    
    Returns:
        bool: è®¡ç®—æ˜¯å¦æˆåŠŸ
        
    Example:
        >>> from src.metrics_03 import run_full_metrics_calculation
        >>> success = run_full_metrics_calculation()
    """
    import pickle
    from pathlib import Path
    
    logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„æŒ‡æ ‡è®¡ç®—æµç¨‹...")
    
    try:
        # è®¾ç½®è·¯å¾„
        base_dir = Path(__file__).parent.parent.parent
        networks_file = base_dir / "data/processed_data/networks/annual_networks_2001_2024.pkl"
        
        # æ£€æŸ¥ç½‘ç»œæ–‡ä»¶
        if not networks_file.exists():
            logger.error(f"âŒ ç½‘ç»œæ–‡ä»¶ä¸å­˜åœ¨: {networks_file}")
            return False
        
        # åŠ è½½ç½‘ç»œæ•°æ®
        logger.info("ğŸ“‚ åŠ è½½å¹´åº¦ç½‘ç»œæ•°æ®...")
        with open(networks_file, 'rb') as f:
            annual_networks = pickle.load(f)
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(annual_networks)} ä¸ªå¹´åº¦ç½‘ç»œ")
        
        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        logger.info("âš™ï¸ å¼€å§‹è®¡ç®—æŒ‡æ ‡...")
        all_metrics_df = calculate_metrics_for_multiple_years(annual_networks)
        
        if all_metrics_df.empty:
            logger.error("âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥")
            return False
        
        logger.info(f"âœ… æŒ‡æ ‡è®¡ç®—å®Œæˆ: {len(all_metrics_df):,} æ¡è®°å½•")
        
        # å¯¼å‡ºç»“æœ
        logger.info("ğŸ’¾ å¯¼å‡ºç»“æœæ–‡ä»¶...")
        output_dir = Path(__file__).parent
        exported_files = export_metrics_to_files(all_metrics_df, str(output_dir))
        
        if exported_files:
            logger.info(f"âœ… ç»“æœå·²å¯¼å‡º: {list(exported_files.keys())}")
            return True
        else:
            logger.error("âŒ ç»“æœå¯¼å‡ºå¤±è´¥")
            return False
            
    except Exception as e:
        logger.error(f"âŒ æŒ‡æ ‡è®¡ç®—æµç¨‹å¤±è´¥: {e}")
        return False

# å¯¼å‡ºçš„ä¸»è¦å‡½æ•°
__all__ = [
    # ä¸»è¦è®¡ç®—å‡½æ•°
    'calculate_all_metrics_for_year',
    'calculate_metrics_for_multiple_years',
    'run_full_metrics_calculation',
    
    # èŠ‚ç‚¹æŒ‡æ ‡
    'calculate_all_node_centralities',
    'get_node_centrality_rankings', 
    'get_node_centrality_summary',
    
    # å…¨å±€æŒ‡æ ‡
    'calculate_all_global_metrics',
    'get_global_metrics_summary',
    
    # è¾…åŠ©åŠŸèƒ½
    'get_metrics_summary_report',
    'export_metrics_to_files',
    
    # å·¥å…·å‡½æ•°
    'setup_logger',
    'validate_graph',
    'safe_divide'
]