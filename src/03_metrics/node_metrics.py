#!/usr/bin/env python3
"""
èŠ‚ç‚¹çº§åˆ«æŒ‡æ ‡è®¡ç®—æ¨¡å—

è´Ÿè´£è®¡ç®—ç½‘ç»œä¸­å„ä¸ªèŠ‚ç‚¹çš„ä¸­å¿ƒæ€§æŒ‡æ ‡ï¼š
- åº¦ä¸­å¿ƒæ€§ (Degree Centrality)
- å¼ºåº¦ä¸­å¿ƒæ€§ (Strength Centrality) 
- ä¸­ä»‹ä¸­å¿ƒæ€§ (Betweenness Centrality)
- PageRank ä¸­å¿ƒæ€§
- ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§ (Eigenvector Centrality)
"""

import networkx as nx
import pandas as pd
import numpy as np
from typing import Dict, Any, List
from utils import (
    setup_logger, validate_graph, safe_divide, timer_decorator,
    handle_computation_error, validate_metrics_result, add_distance_weights
)

logger = setup_logger(__name__)

@timer_decorator
def calculate_degree_centrality(G: nx.DiGraph, year: int) -> pd.DataFrame:
    """
    è®¡ç®—åº¦ä¸­å¿ƒæ€§æŒ‡æ ‡
    
    Args:
        G: NetworkXæœ‰å‘å›¾
        year: å¹´ä»½
    
    Returns:
        åŒ…å«åº¦ä¸­å¿ƒæ€§æŒ‡æ ‡çš„DataFrame
        
    Raises:
        ValueError: å½“è¾“å…¥å›¾æ— æ•ˆæ—¶
    """
    validate_graph(G, "calculate_degree_centrality")
    logger.info(f"     {year}: è®¡ç®—åº¦ä¸­å¿ƒæ€§...")
    
    try:
        results = []
        n_nodes = G.number_of_nodes()
        
        for node in G.nodes():
            in_degree = G.in_degree(node)
            out_degree = G.out_degree(node)
            total_degree = in_degree + out_degree
            
            # å½’ä¸€åŒ–åº¦ä¸­å¿ƒæ€§ (é™¤ä»¥å¯èƒ½çš„æœ€å¤§è¿æ¥æ•°)
            norm_in_degree = safe_divide(in_degree, n_nodes - 1)
            norm_out_degree = safe_divide(out_degree, n_nodes - 1)
            norm_total_degree = safe_divide(total_degree, 2 * (n_nodes - 1))
            
            results.append({
                'year': year,
                'country_code': node,
                'country_name': G.nodes[node].get('name', node),
                'in_degree': in_degree,
                'out_degree': out_degree,
                'total_degree': total_degree,
                'norm_in_degree': norm_in_degree,
                'norm_out_degree': norm_out_degree,
                'norm_total_degree': norm_total_degree
            })
        
        df = pd.DataFrame(results)
        
        # éªŒè¯ç»“æœ
        expected_cols = ['year', 'country_code', 'in_degree', 'out_degree', 'total_degree']
        validate_metrics_result(df, expected_cols, year, "åº¦ä¸­å¿ƒæ€§")
        
        return df
        
    except Exception as e:
        return handle_computation_error("calculate_degree_centrality", year, e, 
                                      pd.DataFrame(columns=['year', 'country_code', 'in_degree', 'out_degree']))

@timer_decorator  
def calculate_strength_centrality(G: nx.DiGraph, year: int) -> pd.DataFrame:
    """
    è®¡ç®—å¼ºåº¦ä¸­å¿ƒæ€§æŒ‡æ ‡ (åŠ æƒåº¦)
    
    Args:
        G: NetworkXæœ‰å‘å›¾
        year: å¹´ä»½
    
    Returns:
        åŒ…å«å¼ºåº¦ä¸­å¿ƒæ€§æŒ‡æ ‡çš„DataFrame
    """
    validate_graph(G, "calculate_strength_centrality")
    logger.info(f"     {year}: è®¡ç®—å¼ºåº¦ä¸­å¿ƒæ€§...")
    
    try:
        results = []
        
        # è®¡ç®—æ€»è´¸æ˜“é¢ç”¨äºå½’ä¸€åŒ–
        total_trade = sum(data.get('weight', 0) for _, _, data in G.edges(data=True))
        
        for node in G.nodes():
            in_strength = G.in_degree(node, weight='weight')
            out_strength = G.out_degree(node, weight='weight')
            total_strength = in_strength + out_strength
            
            # å½’ä¸€åŒ–å¼ºåº¦ä¸­å¿ƒæ€§
            norm_in_strength = safe_divide(in_strength, total_trade)
            norm_out_strength = safe_divide(out_strength, total_trade)
            norm_total_strength = safe_divide(total_strength, total_trade)
            
            results.append({
                'year': year,
                'country_code': node,
                'country_name': G.nodes[node].get('name', node),
                'in_strength': in_strength,
                'out_strength': out_strength,
                'total_strength': total_strength,
                'norm_in_strength': norm_in_strength,
                'norm_out_strength': norm_out_strength,
                'norm_total_strength': norm_total_strength
            })
        
        df = pd.DataFrame(results)
        
        # éªŒè¯ç»“æœ
        expected_cols = ['year', 'country_code', 'in_strength', 'out_strength', 'total_strength']
        validate_metrics_result(df, expected_cols, year, "å¼ºåº¦ä¸­å¿ƒæ€§")
        
        return df
        
    except Exception as e:
        return handle_computation_error("calculate_strength_centrality", year, e,
                                      pd.DataFrame(columns=['year', 'country_code', 'in_strength', 'out_strength']))

@timer_decorator
def calculate_betweenness_centrality(G: nx.DiGraph, year: int) -> pd.DataFrame:
    """
    è®¡ç®—ä¸­ä»‹ä¸­å¿ƒæ€§æŒ‡æ ‡
    
    Args:
        G: NetworkXæœ‰å‘å›¾
        year: å¹´ä»½
    
    Returns:
        åŒ…å«ä¸­ä»‹ä¸­å¿ƒæ€§æŒ‡æ ‡çš„DataFrame
    """
    validate_graph(G, "calculate_betweenness_centrality")
    logger.info(f"     {year}: è®¡ç®—ä¸­ä»‹ä¸­å¿ƒæ€§...")
    
    try:
        # ä¿®æ­£ï¼šä¸ºæ­£ç¡®è®¡ç®—åŠ æƒä¸­ä»‹ä¸­å¿ƒæ€§ï¼Œéœ€ä½¿ç”¨è·ç¦»ä½œä¸ºæƒé‡ (distance = 1/weight)
        G_with_distance = add_distance_weights(G)
        
        # ä½¿ç”¨ä¿®æ­£åçš„è·ç¦»æƒé‡è¿›è¡Œè®¡ç®—
        betweenness = nx.betweenness_centrality(G_with_distance, weight='distance', normalized=True)
        
        results = []
        for node in G.nodes():
            results.append({
                'year': year,
                'country_code': node,
                'country_name': G.nodes[node].get('name', node),
                'betweenness_centrality': betweenness.get(node, 0)
            })
        
        df = pd.DataFrame(results)
        
        # éªŒè¯ç»“æœ
        expected_cols = ['year', 'country_code', 'betweenness_centrality']
        validate_metrics_result(df, expected_cols, year, "ä¸­ä»‹ä¸­å¿ƒæ€§")
        
        return df
        
    except Exception as e:
        # å¯¹äºè®¡ç®—å¤±è´¥çš„æƒ…å†µï¼Œè¿”å›é›¶å€¼ç»“æœ
        results = []
        for node in G.nodes():
            results.append({
                'year': year,
                'country_code': node,
                'country_name': G.nodes[node].get('name', node),
                'betweenness_centrality': 0.0
            })
        return pd.DataFrame(results)

@timer_decorator
def calculate_pagerank_centrality(G: nx.DiGraph, year: int, 
                                alpha: float = 0.85, max_iter: int = 100) -> pd.DataFrame:
    """
    è®¡ç®—PageRankä¸­å¿ƒæ€§æŒ‡æ ‡
    
    Args:
        G: NetworkXæœ‰å‘å›¾
        year: å¹´ä»½
        alpha: é˜»å°¼å‚æ•°
        max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    Returns:
        åŒ…å«PageRankä¸­å¿ƒæ€§æŒ‡æ ‡çš„DataFrame
    """
    validate_graph(G, "calculate_pagerank_centrality")
    logger.info(f"     {year}: è®¡ç®—PageRankä¸­å¿ƒæ€§...")
    
    try:
        # ä½¿ç”¨æƒé‡è®¡ç®—PageRank
        pagerank = nx.pagerank(G, alpha=alpha, max_iter=max_iter, weight='weight')
        
        results = []
        for node in G.nodes():
            results.append({
                'year': year,
                'country_code': node,
                'country_name': G.nodes[node].get('name', node),
                'pagerank_centrality': pagerank.get(node, 0)
            })
        
        df = pd.DataFrame(results)
        
        # éªŒè¯ç»“æœ
        expected_cols = ['year', 'country_code', 'pagerank_centrality']
        validate_metrics_result(df, expected_cols, year, "PageRankä¸­å¿ƒæ€§")
        
        return df
        
    except Exception as e:
        # è¿”å›å‡åŒ€åˆ†å¸ƒçš„ç»“æœ
        n_nodes = G.number_of_nodes()
        uniform_value = safe_divide(1.0, n_nodes)
        
        results = []
        for node in G.nodes():
            results.append({
                'year': year,
                'country_code': node,
                'country_name': G.nodes[node].get('name', node),
                'pagerank_centrality': uniform_value
            })
        return pd.DataFrame(results)

@timer_decorator
def calculate_eigenvector_centrality(G: nx.DiGraph, year: int, 
                                   max_iter: int = 100, tolerance: float = 1e-6) -> pd.DataFrame:
    """
    è®¡ç®—ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§æŒ‡æ ‡
    
    Args:
        G: NetworkXæœ‰å‘å›¾
        year: å¹´ä»½
        max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
        tolerance: æ”¶æ•›å®¹å·®
    
    Returns:
        åŒ…å«ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§æŒ‡æ ‡çš„DataFrame
    """
    validate_graph(G, "calculate_eigenvector_centrality")
    logger.info(f"     {year}: è®¡ç®—ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§...")
    
    try:
        # ä½¿ç”¨æƒé‡è®¡ç®—ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
        eigenvector = nx.eigenvector_centrality(G, max_iter=max_iter, tol=tolerance, weight='weight')
        
        results = []
        for node in G.nodes():
            results.append({
                'year': year,
                'country_code': node,
                'country_name': G.nodes[node].get('name', node),
                'eigenvector_centrality': eigenvector.get(node, 0)
            })
        
        df = pd.DataFrame(results)
        
        # éªŒè¯ç»“æœ
        expected_cols = ['year', 'country_code', 'eigenvector_centrality']
        validate_metrics_result(df, expected_cols, year, "ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§")
        
        return df
        
    except Exception as e:
        # å¯¹äºä¸æ”¶æ•›æˆ–å…¶ä»–é”™è¯¯ï¼Œè¿”å›é›¶å€¼ç»“æœ
        logger.warning(f"     {year}: ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§è®¡ç®—å¤±è´¥ï¼Œè¿”å›é›¶å€¼: {e}")
        results = []
        for node in G.nodes():
            results.append({
                'year': year,
                'country_code': node,
                'country_name': G.nodes[node].get('name', node),
                'eigenvector_centrality': 0.0
            })
        return pd.DataFrame(results)

def calculate_all_node_centralities(G: nx.DiGraph, year: int) -> pd.DataFrame:
    """
    è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹ä¸­å¿ƒæ€§æŒ‡æ ‡
    
    Args:
        G: NetworkXæœ‰å‘å›¾
        year: å¹´ä»½
    
    Returns:
        åŒ…å«æ‰€æœ‰èŠ‚ç‚¹ä¸­å¿ƒæ€§æŒ‡æ ‡çš„å®Œæ•´DataFrame
    """
    logger.info(f"ğŸ“Š {year}: å¼€å§‹è®¡ç®—èŠ‚ç‚¹ä¸­å¿ƒæ€§æŒ‡æ ‡...")
    
    # è®¡ç®—å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡
    degree_df = calculate_degree_centrality(G, year)
    strength_df = calculate_strength_centrality(G, year)
    betweenness_df = calculate_betweenness_centrality(G, year)
    pagerank_df = calculate_pagerank_centrality(G, year)
    eigenvector_df = calculate_eigenvector_centrality(G, year)
    
    # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
    result_df = degree_df.copy()
    
    # æ·»åŠ å¼ºåº¦æŒ‡æ ‡
    strength_cols = ['in_strength', 'out_strength', 'total_strength', 
                     'norm_in_strength', 'norm_out_strength', 'norm_total_strength']
    for col in strength_cols:
        if col in strength_df.columns:
            result_df[col] = strength_df[col]
    
    # æ·»åŠ ä¸­ä»‹ä¸­å¿ƒæ€§
    if 'betweenness_centrality' in betweenness_df.columns:
        result_df['betweenness_centrality'] = betweenness_df['betweenness_centrality']
    
    # æ·»åŠ PageRank
    if 'pagerank_centrality' in pagerank_df.columns:
        result_df['pagerank_centrality'] = pagerank_df['pagerank_centrality']
        
    # æ·»åŠ ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
    if 'eigenvector_centrality' in eigenvector_df.columns:
        result_df['eigenvector_centrality'] = eigenvector_df['eigenvector_centrality']
    
    logger.info(f"âœ… {year}: èŠ‚ç‚¹ä¸­å¿ƒæ€§æŒ‡æ ‡è®¡ç®—å®Œæˆ - {len(result_df)} ä¸ªèŠ‚ç‚¹")
    
    return result_df

def get_node_centrality_rankings(df: pd.DataFrame, year: int, top_k: int = 10) -> Dict[str, List[Dict[str, Any]]]:
    """
    è·å–å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡çš„æ’å
    
    Args:
        df: åŒ…å«ä¸­å¿ƒæ€§æŒ‡æ ‡çš„DataFrame
        year: å¹´ä»½
        top_k: è¿”å›å‰kä¸ªèŠ‚ç‚¹
    
    Returns:
        åŒ…å«å„ç§æ’åçš„å­—å…¸
    """
    rankings = {}
    
    centrality_metrics = [
        'total_degree', 'total_strength', 'betweenness_centrality', 
        'pagerank_centrality', 'eigenvector_centrality'
    ]
    
    for metric in centrality_metrics:
        if metric in df.columns:
            top_nodes = df.nlargest(top_k, metric)[['country_code', 'country_name', metric]].to_dict('records')
            rankings[f'top_{metric}'] = top_nodes
    
    return rankings

def get_node_centrality_summary(df: pd.DataFrame, year: int) -> Dict[str, Any]:
    """
    ç”ŸæˆèŠ‚ç‚¹ä¸­å¿ƒæ€§æŒ‡æ ‡çš„ç»Ÿè®¡æ‘˜è¦
    
    Args:
        df: åŒ…å«ä¸­å¿ƒæ€§æŒ‡æ ‡çš„DataFrame  
        year: å¹´ä»½
    
    Returns:
        ç»Ÿè®¡æ‘˜è¦å­—å…¸
    """
    summary = {
        'year': year,
        'total_nodes': len(df)
    }
    
    # è®¡ç®—å„æŒ‡æ ‡çš„åŸºæœ¬ç»Ÿè®¡é‡
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        if col != 'year':  # æ’é™¤å¹´ä»½åˆ—
            summary.update({
                f'{col}_mean': df[col].mean(),
                f'{col}_std': df[col].std(),
                f'{col}_max': df[col].max(),
                f'{col}_min': df[col].min(),
                f'{col}_median': df[col].median()
            })
            
            # æ‰¾åˆ°æœ€å¤§å€¼å¯¹åº”çš„å›½å®¶
            max_idx = df[col].idxmax()
            if not pd.isna(max_idx):
                summary[f'{col}_max_country'] = df.loc[max_idx, 'country_code']
    
    return summary