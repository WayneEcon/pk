#!/usr/bin/env python3
"""
03_metricsæ¨¡å—æµ‹è¯•è¿è¡Œå™¨

è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•ï¼Œç”Ÿæˆæµ‹è¯•æŠ¥å‘Šã€‚

ä½¿ç”¨æ–¹æ³•:
    python run_all_tests.py
    python run_all_tests.py --verbose    # è¯¦ç»†è¾“å‡º
    python run_all_tests.py --module utils  # åªæµ‹è¯•ç‰¹å®šæ¨¡å—
"""

import unittest
import sys
import time
import argparse
from pathlib import Path
from io import StringIO

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å¯¼å…¥æ‰€æœ‰æµ‹è¯•æ¨¡å—
from tests.test_utils import *
from tests.test_node_metrics import *
from tests.test_global_metrics import *
from tests.test_integration import *


class ColoredTextTestResult(unittest.TextTestResult):
    """å¸¦é¢œè‰²è¾“å‡ºçš„æµ‹è¯•ç»“æœç±»"""
    
    def __init__(self, stream, descriptions, verbosity):
        super().__init__(stream, descriptions, verbosity)
        self.success_count = 0
    
    def addSuccess(self, test):
        super().addSuccess(test)
        self.success_count += 1
        if self.verbosity > 1:
            self.stream.writeln(f"âœ… {test._testMethodName}")
    
    def addError(self, test, err):
        super().addError(test, err)
        if self.verbosity > 1:
            self.stream.writeln(f"âŒ {test._testMethodName} - ERROR")
    
    def addFailure(self, test, err):
        super().addFailure(test, err)
        if self.verbosity > 1:
            self.stream.writeln(f"âŒ {test._testMethodName} - FAIL")
    
    def addSkip(self, test, reason):
        super().addSkip(test, reason)
        if self.verbosity > 1:
            self.stream.writeln(f"â­ï¸  {test._testMethodName} - SKIPPED")


class TestSuite:
    """æµ‹è¯•å¥—ä»¶ç®¡ç†å™¨"""
    
    def __init__(self):
        self.modules = {
            'utils': ['test_utils'],
            'node_metrics': ['test_node_metrics'],
            'global_metrics': ['test_global_metrics'],
            'integration': ['test_integration'],
            'all': ['test_utils', 'test_node_metrics', 'test_global_metrics', 'test_integration']
        }
    
    def create_suite(self, module_name='all'):
        """åˆ›å»ºæµ‹è¯•å¥—ä»¶"""
        if module_name not in self.modules:
            raise ValueError(f"æœªçŸ¥æ¨¡å—: {module_name}. å¯ç”¨æ¨¡å—: {list(self.modules.keys())}")
        
        loader = unittest.TestLoader()
        suite = unittest.TestSuite()
        
        test_modules = self.modules[module_name]
        
        for module in test_modules:
            if module == 'test_utils':
                # utilsæ¨¡å—çš„æµ‹è¯•ç±»
                suite.addTest(loader.loadTestsFromTestCase(TestSetupLogger))
                suite.addTest(loader.loadTestsFromTestCase(TestValidateGraph))
                suite.addTest(loader.loadTestsFromTestCase(TestAddDistanceAttribute))
                suite.addTest(loader.loadTestsFromTestCase(TestSafeDivide))
                suite.addTest(loader.loadTestsFromTestCase(TestGetGraphBasicStats))
                suite.addTest(loader.loadTestsFromTestCase(TestCreateEmptyDataFrames))
                suite.addTest(loader.loadTestsFromTestCase(TestTimerContext))
                suite.addTest(loader.loadTestsFromTestCase(TestValidateYear))
                suite.addTest(loader.loadTestsFromTestCase(TestLogFunctions))
                
            elif module == 'test_node_metrics':
                # node_metricsæ¨¡å—çš„æµ‹è¯•ç±»
                suite.addTest(loader.loadTestsFromTestCase(TestNodeMetricsCalculation))
                suite.addTest(loader.loadTestsFromTestCase(TestNodeMetricsEdgeCases))
                suite.addTest(loader.loadTestsFromTestCase(TestNodeMetricsUtility))
                suite.addTest(loader.loadTestsFromTestCase(TestNodeMetricsValidation))
                
            elif module == 'test_global_metrics':
                # global_metricsæ¨¡å—çš„æµ‹è¯•ç±»
                suite.addTest(loader.loadTestsFromTestCase(TestGlobalMetricsCalculation))
                suite.addTest(loader.loadTestsFromTestCase(TestGlobalMetricsEdgeCases))
                suite.addTest(loader.loadTestsFromTestCase(TestGlobalMetricsUtilities))
                suite.addTest(loader.loadTestsFromTestCase(TestGlobalMetricsValidation))
                suite.addTest(loader.loadTestsFromTestCase(TestGlobalMetricsCorrectness))
                
            elif module == 'test_integration':
                # integrationæµ‹è¯•ç±»
                suite.addTest(loader.loadTestsFromTestCase(TestMainInterface))
                suite.addTest(loader.loadTestsFromTestCase(TestDataProcessingAndExport))
                suite.addTest(loader.loadTestsFromTestCase(TestErrorHandlingAndRobustness))
                suite.addTest(loader.loadTestsFromTestCase(TestPerformanceAndCaching))
                suite.addTest(loader.loadTestsFromTestCase(TestDataConsistencyAndIntegrity))
        
        return suite
    
    def run_tests(self, module_name='all', verbosity=1):
        """è¿è¡Œæµ‹è¯•å¹¶è¿”å›ç»“æœ"""
        print(f"\nğŸ§ª è¿è¡Œ 03_metrics æ¨¡å—æµ‹è¯• - {module_name}")
        print("=" * 60)
        
        suite = self.create_suite(module_name)
        
        # è‡ªå®šä¹‰æµ‹è¯•è¿è¡Œå™¨
        stream = StringIO() if verbosity == 0 else sys.stdout
        runner = unittest.TextTestRunner(
            stream=stream,
            verbosity=verbosity,
            resultclass=ColoredTextTestResult
        )
        
        start_time = time.time()
        result = runner.run(suite)
        end_time = time.time()
        
        # æ‰“å°æµ‹è¯•æ€»ç»“
        self.print_summary(result, end_time - start_time, module_name)
        
        return result
    
    def print_summary(self, result, duration, module_name):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "=" * 60)
        print(f"ğŸ“Š æµ‹è¯•æ€»ç»“ - {module_name} æ¨¡å—")
        print("=" * 60)
        
        total_tests = result.testsRun
        success_count = getattr(result, 'success_count', total_tests - len(result.errors) - len(result.failures))
        error_count = len(result.errors)
        failure_count = len(result.failures)
        skip_count = len(result.skipped)
        
        print(f"æ€»è®¡æµ‹è¯•: {total_tests}")
        print(f"âœ… é€šè¿‡: {success_count}")
        print(f"âŒ å¤±è´¥: {failure_count}")
        print(f"ğŸ’¥ é”™è¯¯: {error_count}")
        print(f"â­ï¸  è·³è¿‡: {skip_count}")
        print(f"â±ï¸  ç”¨æ—¶: {duration:.2f} ç§’")
        
        # æˆåŠŸç‡
        success_rate = (success_count / total_tests) * 100 if total_tests > 0 else 0
        print(f"âœ¨ æˆåŠŸç‡: {success_rate:.1f}%")
        
        # è¯¦ç»†é”™è¯¯ä¿¡æ¯
        if result.errors:
            print(f"\nğŸš« é”™è¯¯è¯¦æƒ… ({len(result.errors)} ä¸ª):")
            for i, (test, error) in enumerate(result.errors, 1):
                error_msg = error.split('\n')[-2] if error else 'æœªçŸ¥é”™è¯¯'
                print(f"  {i}. {test}: {error_msg}")
        
        if result.failures:
            print(f"\nğŸ”´ å¤±è´¥è¯¦æƒ… ({len(result.failures)} ä¸ª):")
            for i, (test, failure) in enumerate(result.failures, 1):
                failure_msg = failure.split('\n')[-2] if failure else 'æœªçŸ¥å¤±è´¥'
                print(f"  {i}. {test}: {failure_msg}")
        
        # æ•´ä½“è¯„ä¼°
        if error_count == 0 and failure_count == 0:
            print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼{module_name} æ¨¡å—è¿è¡Œæ­£å¸¸ã€‚")
        elif success_rate >= 90:
            print(f"\nâš ï¸  å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œä½†å­˜åœ¨å°‘é‡é—®é¢˜éœ€è¦å¤„ç†ã€‚")
        else:
            print(f"\nğŸš¨ å­˜åœ¨è¾ƒå¤šæµ‹è¯•é—®é¢˜ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨ã€‚")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='03_metricsæ¨¡å—æµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument('--module', '-m', 
                       choices=['utils', 'node_metrics', 'global_metrics', 'integration', 'all'],
                       default='all',
                       help='æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å—')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='é™é»˜è¾“å‡ºï¼ˆåªæ˜¾ç¤ºæ€»ç»“ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    import logging
    if args.quiet:
        logging.getLogger().setLevel(logging.CRITICAL)
    else:
        logging.getLogger().setLevel(logging.WARNING)
    
    # ç¡®å®šverbosityçº§åˆ«
    if args.quiet:
        verbosity = 0
    elif args.verbose:
        verbosity = 2
    else:
        verbosity = 1
    
    # è¿è¡Œæµ‹è¯•
    test_suite = TestSuite()
    
    try:
        result = test_suite.run_tests(args.module, verbosity)
        
        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
        if result.errors or result.failures:
            sys.exit(1)  # æœ‰é”™è¯¯æˆ–å¤±è´¥
        else:
            sys.exit(0)  # æ‰€æœ‰æµ‹è¯•é€šè¿‡
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(2)
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿è¡Œå¼‚å¸¸: {e}")
        sys.exit(3)


if __name__ == '__main__':
    main()