#!/usr/bin/env python3
"""
åŒå‘DLIç»Ÿè®¡éªŒè¯æ¨¡å— v2.0 (Bidirectional DLI Statistical Verification Module)
============================================================================

æœ¬æ¨¡å—ä¸“ä¸ºåŒå‘DLIåˆ†æç³»ç»Ÿè®¾è®¡ï¼Œä½¿ç”¨åŒé‡å·®åˆ†æ³•(DID)ç­‰å‡†å®éªŒæ–¹æ³•ï¼Œ
å¯¹"é¡µå²©é©å‘½æ˜¯å¦æ˜¾è‘—æ”¹å˜äº†ç¾å›½èƒ½æºè´¸æ˜“çš„åŒå‘é”å®šæ ¼å±€"è¿›è¡Œä¸¥è°¨çš„ç»Ÿè®¡éªŒè¯ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è¿›å£é”å®šDIDåˆ†æï¼šéªŒè¯ç¾å›½è¢«ä¾›åº”å•†é”å®šç¨‹åº¦çš„å˜åŒ–
2. å‡ºå£é”å®šDIDåˆ†æï¼šéªŒè¯ç¾å›½é”å®šå…¶ä»–å›½å®¶ç¨‹åº¦çš„å˜åŒ–
3. åŒå‘å¯¹æ¯”åˆ†æï¼šé‡åŒ–æƒåŠ›å…³ç³»åè½¬æ•ˆåº”

DIDå®éªŒè®¾è®¡ï¼š
- å¤„ç†ç»„ï¼šç¾-åŠ ã€ç¾-å¢¨çš„ç®¡é“è´¸æ˜“å…³ç³»ï¼ˆé«˜ä¸“ç”¨æ€§åŸºç¡€è®¾æ–½ï¼‰
- æ§åˆ¶ç»„ï¼šä¸æ²™ç‰¹ã€å¡å¡”å°”ç­‰çš„æµ·è¿è´¸æ˜“å…³ç³»ï¼ˆä½ä¸“ç”¨æ€§ï¼‰
- æ”¿ç­–å†²å‡»æ—¶ç‚¹ï¼š2011å¹´ï¼ˆé¡µå²©é©å‘½æ˜¾è‘—äº§å‡ºæ•ˆåº”å¹´ä»½ï¼‰

ä½œè€…ï¼šEnergy Network Analysis Team
ç‰ˆæœ¬ï¼š2.0
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import Dict, List, Tuple, Optional, Union
import warnings
import json
warnings.filterwarnings('ignore')

# å¯¼å…¥statsmodels
try:
    import statsmodels.api as sm
    import statsmodels.formula.api as smf
    from statsmodels.stats.diagnostic import het_breuschpagan
    from statsmodels.stats.stattools import durbin_watson
    HAS_STATSMODELS = True
except ImportError:
    HAS_STATSMODELS = False
    logging.warning("statsmodels not available, using simplified regression")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# DIDå®éªŒè®¾è®¡å¸¸é‡
TREATMENT_COUNTRIES = ['CAN', 'MEX']  # å¤„ç†ç»„ï¼šç®¡é“è´¸æ˜“å›½å®¶
CONTROL_COUNTRIES = ['SAU', 'QAT', 'VEN', 'NOR', 'GBR', 'RUS', 'ARE']  # æ§åˆ¶ç»„
PIPELINE_PRODUCTS = ['Crude_Oil', 'Natural_Gas']  # ç®¡é“è¿è¾“äº§å“
POLICY_SHOCK_YEAR = 2011  # é¡µå²©é©å‘½å†²å‡»å¹´ä»½
PRE_PERIOD = (2001, 2010)  # æ”¿ç­–å‰æœŸé—´
POST_PERIOD = (2011, 2024)  # æ”¿ç­–åæœŸé—´

def load_bidirectional_dli_data(data_file_path: str = None) -> pd.DataFrame:
    """
    åŠ è½½åŒå‘DLIé¢æ¿æ•°æ®
    
    Args:
        data_file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨v2æ•°æ®æ–‡ä»¶
        
    Returns:
        åŒå‘DLIé¢æ¿æ•°æ®DataFrame
    """
    
    if data_file_path is None:
        base_dir = Path(__file__).parent.parent.parent
        data_file_path = Path(__file__).parent / "dli_panel_data_v2.csv"
    
    if not Path(data_file_path).exists():
        raise FileNotFoundError(f"åŒå‘DLIæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file_path}")
    
    df = pd.read_csv(data_file_path)
    logger.info(f"ğŸ“‚ æˆåŠŸåŠ è½½åŒå‘DLIæ•°æ®: {len(df):,} æ¡è®°å½•")
    
    # éªŒè¯æ•°æ®ç»“æ„
    required_columns = ['year', 'us_partner', 'energy_product', 'locking_dimension_type', 'dli_score']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
    
    # æ•°æ®æ¦‚è§ˆ
    locking_stats = df.groupby('locking_dimension_type').agg({
        'dli_score': ['count', 'mean', 'std']
    }).round(4)
    logger.info("ğŸ“Š åŒå‘é”å®šæ•°æ®åˆ†å¸ƒ:")
    print(locking_stats)
    
    return df

def prepare_did_dataset_v2(df: pd.DataFrame, locking_type: str) -> pd.DataFrame:
    """
    ä¸ºæŒ‡å®šé”å®šç±»å‹å‡†å¤‡DIDåˆ†ææ•°æ®é›†
    
    Args:
        df: åŒå‘DLIé¢æ¿æ•°æ®
        locking_type: é”å®šç±»å‹ ('import_locking' æˆ– 'export_locking')
        
    Returns:
        å‡†å¤‡å¥½çš„DIDåˆ†ææ•°æ®é›†
    """
    
    logger.info(f"ğŸ¯ å‡†å¤‡{locking_type}çš„DIDåˆ†ææ•°æ®é›†...")
    
    # ç­›é€‰æŒ‡å®šé”å®šç±»å‹çš„æ•°æ®
    df_filtered = df[df['locking_dimension_type'] == locking_type].copy()
    logger.info(f"ç­›é€‰{locking_type}æ•°æ®: {len(df_filtered):,} æ¡è®°å½•")
    
    if len(df_filtered) == 0:
        raise ValueError(f"æœªæ‰¾åˆ°{locking_type}ç±»å‹çš„æ•°æ®")
    
    # å®šä¹‰å¤„ç†ç»„å’Œæ§åˆ¶ç»„
    if locking_type == 'import_locking':
        # è¿›å£é”å®šï¼šå¤„ç†ç»„ä¸ºç®¡é“è´¸æ˜“ï¼Œæ§åˆ¶ç»„ä¸ºæµ·è¿è´¸æ˜“
        treatment_condition = (
            df_filtered['us_partner'].isin(TREATMENT_COUNTRIES) & 
            df_filtered['energy_product'].isin(PIPELINE_PRODUCTS)
        )
        control_condition = (
            df_filtered['us_partner'].isin(CONTROL_COUNTRIES) & 
            ~df_filtered['energy_product'].isin(['Coal'])
        )
    else:
        # å‡ºå£é”å®šï¼šå¤„ç†ç»„ä¸ºå¯¹é‚»å›½å‡ºå£ï¼Œæ§åˆ¶ç»„ä¸ºå¯¹è¿œè·ç¦»å›½å®¶å‡ºå£
        treatment_condition = (
            df_filtered['us_partner'].isin(TREATMENT_COUNTRIES)
        )
        control_condition = (
            df_filtered['us_partner'].isin(CONTROL_COUNTRIES)
        )
    
    # åˆ›å»ºDIDæ ·æœ¬
    did_sample = df_filtered[treatment_condition | control_condition].copy()
    
    if len(did_sample) == 0:
        raise ValueError(f"{locking_type}çš„DIDæ ·æœ¬ä¸ºç©º")
    
    # åˆ›å»ºDIDå˜é‡
    did_sample['treatment'] = treatment_condition[treatment_condition | control_condition].astype(int)
    did_sample['post'] = (did_sample['year'] >= POLICY_SHOCK_YEAR).astype(int)
    did_sample['treatment_post'] = did_sample['treatment'] * did_sample['post']
    did_sample['period'] = did_sample['year'].apply(
        lambda x: 'pre' if x < POLICY_SHOCK_YEAR else 'post'
    )
    
    # åˆ›å»ºæ§åˆ¶å˜é‡
    did_sample['log_trade_value'] = np.log(did_sample['trade_value_usd'] + 1)
    if 'distance_km' in did_sample.columns:
        did_sample['log_distance'] = np.log(did_sample['distance_km'])
    did_sample['year_trend'] = did_sample['year'] - 2001
    did_sample['country_product'] = did_sample['us_partner'] + '_' + did_sample['energy_product']
    
    # å®éªŒè®¾è®¡éªŒè¯
    logger.info("ğŸ” DIDå®éªŒè®¾è®¡éªŒè¯:")
    
    # æŒ‰ç»„å’Œæ—¶æœŸç»Ÿè®¡
    group_stats = did_sample.groupby(['treatment', 'period']).agg({
        'us_partner': 'nunique',
        'energy_product': 'nunique',
        'dli_score': ['count', 'mean', 'std']
    }).round(4)
    
    logger.info("å®éªŒç»„æ„æˆç»Ÿè®¡:")
    print(group_stats)
    
    # å¤„ç†ç»„å’Œæ§åˆ¶ç»„å›½å®¶
    treatment_countries = did_sample[did_sample['treatment'] == 1]['us_partner'].unique()
    control_countries = did_sample[did_sample['treatment'] == 0]['us_partner'].unique()
    
    logger.info(f"  å¤„ç†ç»„å›½å®¶: {sorted(treatment_countries)}")
    logger.info(f"  æ§åˆ¶ç»„å›½å®¶: {sorted(control_countries)}")
    
    logger.info(f"âœ… {locking_type} DIDæ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(did_sample):,} è§‚æµ‹")
    
    return did_sample

def run_did_regression_v2(did_data: pd.DataFrame, 
                         outcome_vars: List[str] = None,
                         control_vars: List[str] = None,
                         locking_type: str = 'import_locking') -> Dict[str, Dict]:
    """
    æ‰§è¡ŒDIDå›å½’åˆ†æï¼ˆä½¿ç”¨èšç±»ç¨³å¥æ ‡å‡†è¯¯ï¼‰
    
    Args:
        did_data: DIDåˆ†ææ•°æ®é›†
        outcome_vars: ç»“æœå˜é‡åˆ—è¡¨
        control_vars: æ§åˆ¶å˜é‡åˆ—è¡¨
        locking_type: é”å®šç±»å‹æ ‡è¯†
        
    Returns:
        DIDåˆ†æç»“æœå­—å…¸
    """
    
    logger.info(f"ğŸ§® å¼€å§‹æ‰§è¡Œ{locking_type}çš„DIDå›å½’åˆ†æ...")
    
    if not HAS_STATSMODELS:
        raise ImportError("éœ€è¦å®‰è£…statsmodelsåº“è¿›è¡Œå›å½’åˆ†æ")
    
    # é»˜è®¤ç»“æœå˜é‡
    if outcome_vars is None:
        potential_outcomes = ['dli_score', 'continuity', 'infrastructure', 'stability', 'market_locking_power']
        outcome_vars = [var for var in potential_outcomes if var in did_data.columns]
    
    # é»˜è®¤æ§åˆ¶å˜é‡
    if control_vars is None:
        potential_controls = ['log_trade_value', 'log_distance', 'year_trend']
        control_vars = [var for var in potential_controls if var in did_data.columns]
    
    logger.info(f"ç»“æœå˜é‡: {outcome_vars}")
    logger.info(f"æ§åˆ¶å˜é‡: {control_vars}")
    
    results = {}
    
    for outcome_var in outcome_vars:
        logger.info(f"åˆ†æ {outcome_var}...")
        
        # æ„å»ºå›å½’å…¬å¼
        formula = f"{outcome_var} ~ treatment + post + treatment_post"
        if control_vars:
            formula += " + " + " + ".join(control_vars)
        
        logger.info(f"å›å½’å…¬å¼: {formula}")
        
        # å‡†å¤‡å›å½’æ•°æ®ï¼ˆç§»é™¤ç¼ºå¤±å€¼ï¼‰
        reg_vars = [outcome_var, 'treatment', 'post', 'treatment_post'] + control_vars
        cluster_vars = ['us_partner']  # èšç±»å˜é‡
        all_vars = reg_vars + cluster_vars
        
        reg_data = did_data[all_vars].dropna()
        
        if len(reg_data) < 50:  # æœ€å°‘æ ·æœ¬é‡æ£€æŸ¥
            logger.warning(f"âš ï¸ {outcome_var}çš„æœ‰æ•ˆæ ·æœ¬é‡è¿‡å°‘: {len(reg_data)}")
            continue
        
        try:
            # è¿è¡Œå›å½’ - ä½¿ç”¨èšç±»ç¨³å¥æ ‡å‡†è¯¯
            model = smf.ols(formula, data=reg_data).fit(
                cov_type='cluster', 
                cov_kwds={'groups': reg_data['us_partner']}
            )
            
            # æå–DIDç³»æ•°åŠç»Ÿè®¡é‡
            did_coef = model.params['treatment_post']
            did_se = model.bse['treatment_post']
            did_tstat = model.tvalues['treatment_post']
            did_pvalue = model.pvalues['treatment_post']
            did_ci = model.conf_int().loc['treatment_post'].tolist()
            
            # åˆ¤æ–­æ˜¾è‘—æ€§
            significant_5pct = did_pvalue < 0.05
            significant_10pct = did_pvalue < 0.10
            
            # ä¿å­˜ç»“æœ
            results[outcome_var] = {
                'did_coefficient': did_coef,
                'did_std_error': did_se,
                'did_t_statistic': did_tstat,
                'did_p_value': did_pvalue,
                'significant_5pct': significant_5pct,
                'significant_10pct': significant_10pct,
                'r_squared': model.rsquared,
                'n_observations': len(reg_data),
                'ci_lower': did_ci[0],
                'ci_upper': did_ci[1],
                'locking_type': locking_type,
                'formula': formula
            }
            
            # è¾“å‡ºç»“æœ
            significance = "***" if did_pvalue < 0.01 else "**" if did_pvalue < 0.05 else "*" if did_pvalue < 0.10 else ""
            direction = "â†‘" if did_coef > 0 else "â†“"
            
            logger.info(f"  {outcome_var}: {did_coef:+.4f} {significance} (p={did_pvalue:.4f}) {direction}")
            
        except Exception as e:
            logger.error(f"âŒ {outcome_var}å›å½’åˆ†æå¤±è´¥: {e}")
            continue
    
    logger.info(f"âœ… {locking_type} DIDå›å½’åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ {len(results)} ä¸ªæŒ‡æ ‡")
    return results

def run_full_bidirectional_did_analysis(dli_data: pd.DataFrame = None) -> Dict[str, Dict]:
    """
    æ‰§è¡Œå®Œæ•´çš„åŒå‘DIDåˆ†æ
    
    Args:
        dli_data: åŒå‘DLIé¢æ¿æ•°æ®ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åŠ è½½
        
    Returns:
        åŒ…å«è¿›å£é”å®šå’Œå‡ºå£é”å®šDIDåˆ†æç»“æœçš„å­—å…¸
    """
    
    logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„åŒå‘DIDåˆ†æ...")
    
    # åŠ è½½æ•°æ®
    if dli_data is None:
        dli_data = load_bidirectional_dli_data()
    
    results = {}
    
    # 1. è¿›å£é”å®šDIDåˆ†æ
    logger.info("ğŸ“¥ æ‰§è¡Œè¿›å£é”å®šDIDåˆ†æ...")
    try:
        import_data = prepare_did_dataset_v2(dli_data, 'import_locking')
        results['import_locking'] = run_did_regression_v2(
            import_data, locking_type='import_locking'
        )
        logger.info(f"âœ… è¿›å£é”å®šDIDåˆ†æå®Œæˆ: {len(results['import_locking'])} ä¸ªæŒ‡æ ‡")
    except Exception as e:
        logger.error(f"âŒ è¿›å£é”å®šDIDåˆ†æå¤±è´¥: {e}")
    
    # 2. å‡ºå£é”å®šDIDåˆ†æ  
    logger.info("ğŸ“¤ æ‰§è¡Œå‡ºå£é”å®šDIDåˆ†æ...")
    try:
        export_data = prepare_did_dataset_v2(dli_data, 'export_locking')
        results['export_locking'] = run_did_regression_v2(
            export_data, locking_type='export_locking'
        )
        logger.info(f"âœ… å‡ºå£é”å®šDIDåˆ†æå®Œæˆ: {len(results['export_locking'])} ä¸ªæŒ‡æ ‡")
    except Exception as e:
        logger.error(f"âŒ å‡ºå£é”å®šDIDåˆ†æå¤±è´¥: {e}")
    
    # 3. æ€»ç»“åˆ†æç»“æœ
    logger.info("ğŸ“Š åŒå‘DIDåˆ†ææ€»ç»“:")
    for locking_type, type_results in results.items():
        significant_vars = [var for var, res in type_results.items() 
                          if res.get('significant_5pct', False)]
        logger.info(f"  {locking_type}: {len(significant_vars)}/{len(type_results)} ä¸ªæŒ‡æ ‡åœ¨5%æ°´å¹³æ˜¾è‘—")
        for var in significant_vars:
            coef = type_results[var]['did_coefficient']
            p_val = type_results[var]['did_p_value']
            direction = "å¢å¼º" if coef > 0 else "å‡å¼±"
            logger.info(f"    {var}: {coef:+.4f} (p={p_val:.4f}) - é”å®šæ•ˆåº”{direction}")
    
    logger.info(f"ğŸ‰ å®Œæ•´åŒå‘DIDåˆ†æå®Œæˆï¼æˆåŠŸåˆ†æ {len(results)} ä¸ªé”å®šç»´åº¦")
    return results

def save_bidirectional_results(results: Dict[str, Dict], 
                              output_dir: str = None) -> Dict[str, str]:
    """
    ä¿å­˜åŒå‘DIDåˆ†æç»“æœ
    
    Args:
        results: åŒå‘DIDåˆ†æç»“æœ
        output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†è·¯å¾„
        
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
    """
    
    if output_dir is None:
        base_dir = Path(__file__).parent.parent.parent
        output_dir = Path(__file__).parent
        output_dir.mkdir(parents=True, exist_ok=True)
    
    output_paths = {}
    
    # 1. ä¿å­˜è¯¦ç»†ç»“æœä¸ºCSV
    all_results = []
    for locking_type, type_results in results.items():
        for variable, result in type_results.items():
            result_row = {
                'locking_type': locking_type,
                'variable': variable,
                **result
            }
            all_results.append(result_row)
    
    results_df = pd.DataFrame(all_results)
    csv_path = Path(output_dir) / "dli_verification_results_v2.csv"
    results_df.to_csv(csv_path, index=False)
    output_paths['results_csv'] = str(csv_path)
    logger.info(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {csv_path}")
    
    # 2. ä¿å­˜JSONæ ¼å¼ç»“æœ
    json_path = Path(output_dir) / "dli_verification_results_v2.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        # å¤„ç†numpyç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.float64):
                return float(obj)
            elif isinstance(obj, np.int64):
                return int(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            return obj
        
        serializable_results = {}
        for locking_type, type_results in results.items():
            serializable_results[locking_type] = {}
            for var, result in type_results.items():
                serializable_results[locking_type][var] = {
                    k: convert_numpy(v) for k, v in result.items()
                }
        
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    output_paths['results_json'] = str(json_path)
    logger.info(f"ğŸ’¾ JSONç»“æœå·²ä¿å­˜è‡³: {json_path}")
    
    return output_paths

def generate_verification_report_v2(results: Dict[str, Dict], 
                                   output_dir: str = None) -> str:
    """
    ç”ŸæˆåŒå‘DLIéªŒè¯æŠ¥å‘Š
    
    Args:
        results: åŒå‘DIDåˆ†æç»“æœ
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    
    if output_dir is None:
        base_dir = Path(__file__).parent.parent.parent
        output_dir = Path(__file__).parent
    
    report_path = Path(output_dir) / "dli_verification_report_v2.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# åŒå‘åŠ¨æ€é”å®šæŒ‡æ•°(DLI)ç»Ÿè®¡éªŒè¯æŠ¥å‘Š v2.0\\n\\n")
        f.write("**ç”Ÿæˆæ—¶é—´**: " + pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S') + "\\n")
        f.write("**åˆ†ææ–¹æ³•**: åŒé‡å·®åˆ†æ³•(DID)ï¼Œèšç±»ç¨³å¥æ ‡å‡†è¯¯\\n")
        f.write("**æ”¿ç­–å†²å‡»**: é¡µå²©é©å‘½(2011å¹´)\\n\\n")
        
        f.write("---\\n\\n")
        
        # åˆ†ææ¦‚è¿°
        f.write("## ğŸ“Š åˆ†ææ¦‚è¿°\\n\\n")
        f.write("æœ¬æŠ¥å‘ŠåŸºäºåŒå‘DLIåˆ†æç³»ç»Ÿï¼Œä½¿ç”¨DIDæ–¹æ³•éªŒè¯é¡µå²©é©å‘½å¯¹ç¾å›½èƒ½æºè´¸æ˜“é”å®šå…³ç³»çš„åŒå‘å½±å“ï¼š\\n\\n")
        f.write("- **è¿›å£é”å®š**: ç¾å›½è¢«ä¾›åº”å•†é”å®šçš„ç¨‹åº¦å˜åŒ–\\n")
        f.write("- **å‡ºå£é”å®š**: ç¾å›½é”å®šå…¶ä»–å›½å®¶çš„ç¨‹åº¦å˜åŒ–\\n\\n")
        
        # å®éªŒè®¾è®¡
        f.write("## ğŸ§ª å®éªŒè®¾è®¡\\n\\n")
        f.write("### å¤„ç†ç»„ä¸æ§åˆ¶ç»„\\n")
        f.write(f"- **å¤„ç†ç»„**: {', '.join(TREATMENT_COUNTRIES)}ï¼ˆç®¡é“è´¸æ˜“ï¼Œé«˜ä¸“ç”¨æ€§åŸºç¡€è®¾æ–½ï¼‰\\n")
        f.write(f"- **æ§åˆ¶ç»„**: {', '.join(CONTROL_COUNTRIES)}ï¼ˆæµ·è¿è´¸æ˜“ï¼Œä½ä¸“ç”¨æ€§åŸºç¡€è®¾æ–½ï¼‰\\n")
        f.write(f"- **æ”¿ç­–å†²å‡»æ—¶ç‚¹**: {POLICY_SHOCK_YEAR}å¹´\\n\\n")
        
        # ä¸»è¦å‘ç°
        f.write("## ğŸ” ä¸»è¦å‘ç°\\n\\n")
        
        for locking_type, type_results in results.items():
            type_name = "è¿›å£é”å®š" if locking_type == "import_locking" else "å‡ºå£é”å®š"
            f.write(f"### {type_name}åˆ†æç»“æœ\\n\\n")
            
            # åˆ›å»ºç»“æœè¡¨æ ¼
            f.write("| æŒ‡æ ‡ | DIDç³»æ•° | æ ‡å‡†è¯¯ | tç»Ÿè®¡é‡ | på€¼ | æ˜¾è‘—æ€§ | 95%ç½®ä¿¡åŒºé—´ |\\n")
            f.write("|------|---------|--------|---------|-----|--------|-------------|\\n")
            
            for variable, result in type_results.items():
                coef = result['did_coefficient']
                se = result['did_std_error']
                t_stat = result['did_t_statistic'] 
                p_val = result['did_p_value']
                ci_lower = result['ci_lower']
                ci_upper = result['ci_upper']
                
                # æ˜¾è‘—æ€§æ ‡è®°
                if p_val < 0.01:
                    sig = "***"
                elif p_val < 0.05:
                    sig = "**"
                elif p_val < 0.10:
                    sig = "*"
                else:
                    sig = ""
                
                f.write(f"| {variable} | {coef:+.4f} | {se:.4f} | {t_stat:+.2f} | {p_val:.4f} | {sig} | [{ci_lower:+.4f}, {ci_upper:+.4f}] |\\n")
            
            f.write("\\n")
            
            # æ˜¾è‘—æ€§è§£é‡Š
            significant_vars = [var for var, res in type_results.items() if res.get('significant_5pct', False)]
            if significant_vars:
                f.write(f"**{type_name}å…³é”®å‘ç°**ï¼š\\n")
                for var in significant_vars:
                    coef = type_results[var]['did_coefficient']
                    direction = "å¢å¼º" if coef > 0 else "å‡å¼±"
                    f.write(f"- {var}: é”å®šæ•ˆåº”æ˜¾è‘—{direction} ({coef:+.4f})\\n")
                f.write("\\n")
        
        # ç»Ÿè®¡è¯´æ˜
        f.write("## ğŸ“ ç»Ÿè®¡è¯´æ˜\\n\\n")
        f.write("- **èšç±»ç¨³å¥æ ‡å‡†è¯¯**: æŒ‰å›½å®¶èšç±»æ ¡æ­£é¢æ¿æ•°æ®åºåˆ—ç›¸å…³æ€§\\n")
        f.write("- **æ˜¾è‘—æ€§æ°´å¹³**: *** p<0.01, ** p<0.05, * p<0.10\\n")
        f.write("- **DIDç³»æ•°**: treatment_postäº¤äº’é¡¹ç³»æ•°ï¼Œè¡¨ç¤ºæ”¿ç­–å¯¹å¤„ç†ç»„çš„å‡€æ•ˆåº”\\n")
        f.write("- **æ­£ç³»æ•°**: é”å®šæ•ˆåº”å¢å¼ºï¼›è´Ÿç³»æ•°: é”å®šæ•ˆåº”å‡å¼±\\n\\n")
        
        f.write("---\\n\\n")
        f.write("*æœ¬æŠ¥å‘Šç”±åŒå‘DLIç»Ÿè®¡éªŒè¯æ¨¡å—v2.0è‡ªåŠ¨ç”Ÿæˆ*\\n")
    
    logger.info(f"ğŸ“„ åŒå‘DLIéªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return str(report_path)

if __name__ == "__main__":
    # æµ‹è¯•åŒå‘DIDåˆ†æ
    try:
        logger.info("ğŸš€ å¼€å§‹åŒå‘DLIç»Ÿè®¡éªŒè¯æµ‹è¯•...")
        
        # æ‰§è¡Œå®Œæ•´åˆ†æ
        results = run_full_bidirectional_did_analysis()
        
        # ä¿å­˜ç»“æœ
        output_paths = save_bidirectional_results(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = generate_verification_report_v2(results)
        
        print("ğŸ‰ åŒå‘DLIç»Ÿè®¡éªŒè¯å®Œæˆï¼")
        print("ğŸ“Š è¾“å‡ºæ–‡ä»¶:")
        for desc, path in output_paths.items():
            print(f"  {desc}: {path}")
        print(f"  verification_report: {report_path}")
        
    except Exception as e:
        logger.error(f"âŒ åŒå‘DLIç»Ÿè®¡éªŒè¯å¤±è´¥: {e}")
        raise