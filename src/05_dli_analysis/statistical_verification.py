#!/usr/bin/env python3
"""
ç»Ÿè®¡éªŒè¯æ¨¡å— (Statistical Verification Module)
==============================================

æœ¬æ¨¡å—ä½¿ç”¨åŒé‡å·®åˆ†æ³•(DID)ç­‰å‡†å®éªŒæ–¹æ³•ï¼Œå¯¹"é¡µå²©é©å‘½æ˜¯å¦æ˜¾è‘—æ”¹å˜äº†DLIæ ¼å±€"
è¿™ä¸€æ ¸å¿ƒå‡è¯´è¿›è¡Œä¸¥è°¨çš„ç»Ÿè®¡éªŒè¯ã€‚

DIDæ¨¡å‹è®¾å®šï¼š
- å¤„ç†ç»„ (Treatment Group): é€šè¿‡ç®¡é“è¿›è¡ŒåŸæ²¹å’Œå¤©ç„¶æ°”è´¸æ˜“çš„ç¾-åŠ ã€ç¾-å¢¨å…³ç³»
  è¿™äº›å…³ç³»å—é«˜æ²‰æ²¡æˆæœ¬çš„ä¸“ç”¨æ€§åŸºç¡€è®¾æ–½é”å®šï¼Œæ˜¯æ”¿ç­–å†²å‡»æœ€ç›´æ¥çš„ä¼ å¯¼æ¸ é“
- æ§åˆ¶ç»„ (Control Group): é€šè¿‡æµ·è¿è¿›è¡ŒLNGã€åŸæ²¹åŠæˆå“æ²¹è´¸æ˜“çš„å…³ç³»
  å¦‚ä¸æ²™ç‰¹ã€å¡å¡”å°”ã€å§”å†…ç‘æ‹‰ç­‰ï¼ŒåŸºç¡€è®¾æ–½ä¸“ç”¨æ€§è¾ƒä½ï¼Œè½¬æ¢æˆæœ¬æ›´çµæ´»
- æ”¿ç­–å†²å‡»æ—¶é—´ç‚¹: é¡µå²©é©å‘½äº§ç”Ÿæ˜¾è‘—äº§å‡ºæ•ˆåº”çš„å¹´ä»½ï¼ˆ2011å¹´æˆ–ä¹‹åï¼‰

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import Dict, List, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥statsmodelsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
try:
    import statsmodels.api as sm
    import statsmodels.formula.api as smf
    from statsmodels.stats.diagnostic import het_breuschpagan
    from statsmodels.stats.stattools import durbin_watson
    HAS_STATSMODELS = True
except ImportError:
    HAS_STATSMODELS = False
    logging.warning("statsmodels not available, using simplified regression")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# DIDå®éªŒè®¾è®¡å¸¸é‡
TREATMENT_COUNTRIES = ['CAN', 'MEX']  # å¤„ç†ç»„ï¼šç®¡é“è´¸æ˜“å›½å®¶
CONTROL_COUNTRIES = ['SAU', 'QAT', 'VEN', 'NOR', 'GBR', 'RUS', 'ARE']  # æ§åˆ¶ç»„ï¼šæµ·è¿è´¸æ˜“ä¸»è¦å›½å®¶
PIPELINE_PRODUCTS = ['Crude_Oil', 'Natural_Gas']  # ç®¡é“è¿è¾“çš„ä¸»è¦äº§å“
POLICY_SHOCK_YEAR = 2011  # é¡µå²©é©å‘½æ˜¾è‘—äº§å‡ºæ•ˆåº”å¹´ä»½
PRE_PERIOD = (2001, 2010)  # æ”¿ç­–å‰æœŸé—´
POST_PERIOD = (2011, 2024)  # æ”¿ç­–åæœŸé—´

def prepare_did_dataset(dli_data: pd.DataFrame = None, 
                       data_file_path: str = None) -> pd.DataFrame:
    """
    å‡†å¤‡DIDåˆ†ææ•°æ®é›†
    
    Args:
        dli_data: DLIé¢æ¿æ•°æ®ï¼Œå¦‚æœä¸ºNoneåˆ™ä»æ–‡ä»¶åŠ è½½
        data_file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        å‡†å¤‡å¥½çš„DIDåˆ†ææ•°æ®é›†
        
    åŒ…å«åˆ—ï¼š
        - åŸºç¡€æ ‡è¯†ï¼šyear, us_partner, energy_product, us_role
        - DLIæŒ‡æ ‡ï¼šdli_composite_adjusted + å››ä¸ªç»´åº¦
        - DIDå˜é‡ï¼štreatment, post, treatment_post
        - æ§åˆ¶å˜é‡ï¼štrade_value_usd, distance_kmç­‰
    """
    
    logger.info("ğŸ¯ å¼€å§‹å‡†å¤‡DIDåˆ†ææ•°æ®é›†...")
    
    # ç¬¬1æ­¥ï¼šåŠ è½½DLIæ•°æ®
    if dli_data is not None:
        df = dli_data.copy()
        logger.info(f"ä½¿ç”¨æä¾›çš„DLIæ•°æ®: {len(df)} æ¡è®°å½•")
    else:
        if data_file_path is None:
            base_dir = Path(__file__).parent.parent.parent
            data_file_path = base_dir / "outputs" / "tables" / "dli_panel_data.csv"
        
        if not Path(data_file_path).exists():
            raise FileNotFoundError(f"DLIæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file_path}")
        
        df = pd.read_csv(data_file_path)
        logger.info(f"ä»æ–‡ä»¶åŠ è½½DLIæ•°æ®: {data_file_path}, {len(df)} æ¡è®°å½•")
    
    # ç¬¬2æ­¥ï¼šå®šä¹‰å¤„ç†ç»„å’Œæ§åˆ¶ç»„
    logger.info("ğŸ” å®šä¹‰å¤„ç†ç»„å’Œæ§åˆ¶ç»„...")
    
    # å¤„ç†ç»„ï¼šç®¡é“è´¸æ˜“å…³ç³»ï¼ˆç¾-åŠ ã€ç¾-å¢¨çš„åŸæ²¹å’Œå¤©ç„¶æ°”ï¼‰
    treatment_condition = (
        df['us_partner'].isin(TREATMENT_COUNTRIES) & 
        df['energy_product'].isin(PIPELINE_PRODUCTS)
    )
    
    # æ§åˆ¶ç»„ï¼šæµ·è¿è´¸æ˜“å…³ç³»ï¼ˆå…¶ä»–ä¸»è¦è´¸æ˜“ä¼™ä¼´ï¼‰
    control_condition = (
        df['us_partner'].isin(CONTROL_COUNTRIES) & 
        ~df['energy_product'].isin(['Coal'])  # æ’é™¤ç…¤ç‚­ï¼Œå› ä¸ºä¸»è¦æ˜¯æµ·è¿ä½†æ€§è´¨ä¸åŒ
    )
    
    # ç­›é€‰å®éªŒæ ·æœ¬
    did_sample = df[treatment_condition | control_condition].copy()
    
    if len(did_sample) == 0:
        raise ValueError("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„DIDåˆ†ææ ·æœ¬")
    
    logger.info(f"ğŸ“Š DIDæ ·æœ¬æ„æˆ:")
    logger.info(f"  æ€»æ ·æœ¬: {len(did_sample)} æ¡è®°å½•")
    
    # ç¬¬3æ­¥ï¼šåˆ›å»ºDIDå˜é‡
    logger.info("âš™ï¸ åˆ›å»ºDIDå®éªŒå˜é‡...")
    
    # å¤„ç†ç»„æŒ‡ç¤ºå˜é‡ (Treatment)
    did_sample['treatment'] = treatment_condition[treatment_condition | control_condition].astype(int)
    
    # æ”¿ç­–åæ—¶æœŸæŒ‡ç¤ºå˜é‡ (Post)
    did_sample['post'] = (did_sample['year'] >= POLICY_SHOCK_YEAR).astype(int)
    
    # DIDäº¤äº’é¡¹ (Treatment Ã— Post)
    did_sample['treatment_post'] = did_sample['treatment'] * did_sample['post']
    
    # ç¬¬4æ­¥ï¼šåˆ›å»ºæ—¶æœŸå˜é‡
    def assign_period(year):
        if year < POLICY_SHOCK_YEAR:
            return 'pre'
        else:
            return 'post'
    
    did_sample['period'] = did_sample['year'].apply(assign_period)
    
    # ç¬¬5æ­¥ï¼šæ•°æ®éªŒè¯å’Œç»Ÿè®¡
    logger.info("ğŸ” DIDå®éªŒè®¾è®¡éªŒè¯:")
    
    # æŒ‰ç»„å’Œæ—¶æœŸç»Ÿè®¡
    group_period_stats = did_sample.groupby(['treatment', 'period']).agg({
        'us_partner': 'nunique',
        'energy_product': 'nunique', 
        'dli_composite_adjusted': ['count', 'mean', 'std']
    }).round(4)
    
    logger.info("å®éªŒç»„æ„æˆç»Ÿè®¡:")
    print(group_period_stats)
    
    # å¤„ç†ç»„å›½å®¶ç»Ÿè®¡
    treatment_countries_actual = did_sample[did_sample['treatment'] == 1]['us_partner'].unique()
    control_countries_actual = did_sample[did_sample['treatment'] == 0]['us_partner'].unique()
    
    logger.info(f"  å®é™…å¤„ç†ç»„å›½å®¶: {sorted(treatment_countries_actual)}")
    logger.info(f"  å®é™…æ§åˆ¶ç»„å›½å®¶: {sorted(control_countries_actual)}")
    
    # äº§å“åˆ†å¸ƒç»Ÿè®¡
    product_by_group = did_sample.groupby(['treatment', 'energy_product']).size().unstack(fill_value=0)
    logger.info("äº§å“åˆ†å¸ƒç»Ÿè®¡:")
    print(product_by_group)
    
    # æ—¶é—´å¹³è¡¡æ€§æ£€æŸ¥
    time_balance = did_sample.groupby(['treatment', 'year']).size().unstack(fill_value=0)
    logger.info(f"æ—¶é—´è·¨åº¦: {did_sample['year'].min()}-{did_sample['year'].max()}")
    logger.info(f"æ”¿ç­–å†²å‡»å¹´ä»½: {POLICY_SHOCK_YEAR}")
    
    # ç¬¬6æ­¥ï¼šåˆ›å»ºæ§åˆ¶å˜é‡
    logger.info("ğŸ“ˆ åˆ›å»ºæ§åˆ¶å˜é‡...")
    
    # å¯¹æ•°åŒ–è´¸æ˜“å€¼ï¼ˆå¤„ç†æå€¼ï¼‰
    did_sample['log_trade_value'] = np.log(did_sample['trade_value_usd'] + 1)
    
    # å¯¹æ•°åŒ–è·ç¦»
    did_sample['log_distance'] = np.log(did_sample['distance_km'])
    
    # å¹´ä»½è¶‹åŠ¿å˜é‡
    did_sample['year_trend'] = did_sample['year'] - 2001
    
    # åˆ›å»ºå›½å®¶å’Œäº§å“å›ºå®šæ•ˆåº”å˜é‡
    did_sample['country_product'] = did_sample['us_partner'] + '_' + did_sample['energy_product']
    
    # ç¬¬7æ­¥ï¼šæœ€ç»ˆæ•°æ®éªŒè¯
    logger.info("âœ… DIDæ•°æ®é›†éªŒè¯:")
    logger.info(f"  æœ€ç»ˆæ ·æœ¬é‡: {len(did_sample):,} è§‚æµ‹")
    logger.info(f"  å›½å®¶æ•°: {did_sample['us_partner'].nunique()}")
    logger.info(f"  äº§å“æ•°: {did_sample['energy_product'].nunique()}")
    logger.info(f"  å¹´ä»½æ•°: {did_sample['year'].nunique()}")
    logger.info(f"  å¤„ç†ç»„è§‚æµ‹: {did_sample['treatment'].sum():,} ({did_sample['treatment'].mean()*100:.1f}%)")
    logger.info(f"  æ”¿ç­–åè§‚æµ‹: {did_sample['post'].sum():,} ({did_sample['post'].mean()*100:.1f}%)")
    
    # æ£€æŸ¥å…³é”®å˜é‡çš„ç¼ºå¤±å€¼
    key_variables = ['dli_composite_adjusted', 'treatment', 'post', 'treatment_post', 
                    'log_trade_value', 'log_distance']
    missing_summary = did_sample[key_variables].isnull().sum()
    if missing_summary.any():
        logger.warning("å‘ç°ç¼ºå¤±å€¼:")
        for var, count in missing_summary[missing_summary > 0].items():
            logger.warning(f"  {var}: {count} ä¸ªç¼ºå¤±å€¼")
    else:
        logger.info("âœ… å…³é”®å˜é‡æ— ç¼ºå¤±å€¼")
    
    logger.info("âœ… DIDæ•°æ®é›†å‡†å¤‡å®Œæˆ!")
    return did_sample

def run_did_analysis(did_data: pd.DataFrame = None,
                    outcome_vars: List[str] = None,
                    control_vars: List[str] = None,
                    use_fixed_effects: bool = True) -> Dict[str, Dict]:
    """
    æ‰§è¡ŒåŒé‡å·®åˆ†(DID)åˆ†æ
    
    åŸºæœ¬æ¨¡å‹ï¼š
    Y_ijt = Î± + Î²â‚Ã—Treatment_ij + Î²â‚‚Ã—Post_t + Î²â‚ƒÃ—(Treatment_ij Ã— Post_t) + Î³Ã—X_ijt + Îµ_ijt
    
    å…¶ä¸­ï¼š
    - Y_ijt: DLIç›¸å…³ç»“æœå˜é‡
    - Treatment_ij: å¤„ç†ç»„æŒ‡ç¤ºå˜é‡ï¼ˆ1=ç®¡é“è´¸æ˜“å›½å®¶ï¼Œ0=æµ·è¿è´¸æ˜“å›½å®¶ï¼‰
    - Post_t: æ”¿ç­–åæ—¶æœŸæŒ‡ç¤ºå˜é‡ï¼ˆ1=2011å¹´åŠä»¥åï¼Œ0=2010å¹´åŠä»¥å‰ï¼‰
    - Î²â‚ƒ: DIDä¼°è®¡é‡ï¼Œè¡¨ç¤ºæ”¿ç­–å¯¹å¤„ç†ç»„çš„å‡€å½±å“
    - X_ijt: æ§åˆ¶å˜é‡
    
    Args:
        did_data: DIDåˆ†ææ•°æ®é›†
        outcome_vars: ç»“æœå˜é‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºDLIç›¸å…³æŒ‡æ ‡
        control_vars: æ§åˆ¶å˜é‡åˆ—è¡¨
        use_fixed_effects: æ˜¯å¦ä½¿ç”¨å›ºå®šæ•ˆåº”
        
    Returns:
        åŒ…å«æ‰€æœ‰æ¨¡å‹ç»“æœçš„å­—å…¸
    """
    
    logger.info("ğŸ“Š å¼€å§‹æ‰§è¡ŒDIDåˆ†æ...")
    
    # æ•°æ®å‡†å¤‡
    if did_data is None:
        did_data = prepare_did_dataset()
    
    if outcome_vars is None:
        outcome_vars = [
            'dli_composite_adjusted',
            'continuity', 
            'infrastructure', 
            'stability', 
            'market_locking_power'
        ]
    
    if control_vars is None:
        control_vars = ['log_trade_value', 'log_distance', 'year_trend']
    
    # éªŒè¯å˜é‡å­˜åœ¨
    all_vars = outcome_vars + control_vars + ['treatment', 'post', 'treatment_post']
    missing_vars = [var for var in all_vars if var not in did_data.columns]
    if missing_vars:
        raise ValueError(f"æ•°æ®ä¸­ç¼ºå°‘å˜é‡: {missing_vars}")
    
    results = {}
    
    # ä¸ºæ¯ä¸ªç»“æœå˜é‡è¿è¡ŒDIDå›å½’
    for outcome_var in outcome_vars:
        logger.info(f"ğŸ” åˆ†æç»“æœå˜é‡: {outcome_var}")
        
        try:
            # å‡†å¤‡å›å½’æ•°æ®ï¼ˆç§»é™¤ç¼ºå¤±å€¼ï¼‰
            reg_vars = [outcome_var, 'treatment', 'post', 'treatment_post'] + control_vars
            cluster_vars = ['us_partner']  # èšç±»å˜é‡
            all_vars = reg_vars + cluster_vars
            reg_data = did_data[all_vars].dropna()
            
            if len(reg_data) == 0:
                logger.warning(f"  {outcome_var}: æ— æœ‰æ•ˆè§‚æµ‹ï¼Œè·³è¿‡åˆ†æ")
                continue
            
            logger.info(f"  æœ‰æ•ˆè§‚æµ‹æ•°: {len(reg_data):,}")
            
            if HAS_STATSMODELS:
                # ä½¿ç”¨statsmodelsè¿›è¡Œä¸“ä¸šå›å½’åˆ†æ
                
                # æ„å»ºå›å½’å…¬å¼
                formula = f"{outcome_var} ~ treatment + post + treatment_post"
                if control_vars:
                    formula += " + " + " + ".join(control_vars)
                
                logger.info(f"  å›å½’å…¬å¼: {formula}")
                
                # è¿è¡Œå›å½’ - ä½¿ç”¨èšç±»ç¨³å¥æ ‡å‡†è¯¯
                # è¿™æ˜¯é¢æ¿æ•°æ®DIDåˆ†æçš„æ ‡å‡†åšæ³•ï¼Œé¿å…åŒä¸€å®ä½“è§‚æµ‹å€¼çš„åºåˆ—ç›¸å…³æ€§
                model = smf.ols(formula, data=reg_data).fit(
                    cov_type='cluster', 
                    cov_kwds={'groups': reg_data['us_partner']}
                )
                
                # æå–å…³é”®ç»“æœ
                did_coef = model.params['treatment_post']
                did_pvalue = model.pvalues['treatment_post']
                did_stderr = model.bse['treatment_post']
                did_tstat = model.tvalues['treatment_post']
                
                # è®¡ç®—ç½®ä¿¡åŒºé—´
                conf_int = model.conf_int().loc['treatment_post']
                did_ci_lower = conf_int[0]
                did_ci_upper = conf_int[1]
                
                # æ¨¡å‹è¯Šæ–­ç»Ÿè®¡
                r_squared = model.rsquared
                adj_r_squared = model.rsquared_adj
                f_statistic = model.fvalue
                f_pvalue = model.f_pvalue
                
                # å¼‚æ–¹å·®æ£€éªŒï¼ˆBreusch-Paganï¼‰
                try:
                    bp_stat, bp_pvalue, _, _ = het_breuschpagan(model.resid, model.model.exog)
                except:
                    bp_stat, bp_pvalue = None, None
                
                # Durbin-Watsonç»Ÿè®¡é‡ï¼ˆåºåˆ—ç›¸å…³æ£€éªŒï¼‰
                try:
                    dw_stat = durbin_watson(model.resid)
                except:
                    dw_stat = None
                
                # ä¿å­˜è¯¦ç»†ç»“æœ
                var_results = {
                    # DIDæ ¸å¿ƒç»“æœ
                    'did_coefficient': did_coef,
                    'did_std_error': did_stderr,
                    'did_t_statistic': did_tstat,
                    'did_p_value': did_pvalue,
                    'did_ci_lower': did_ci_lower,
                    'did_ci_upper': did_ci_upper,
                    'is_significant_5pct': did_pvalue < 0.05,
                    'is_significant_10pct': did_pvalue < 0.10,
                    
                    # å…¶ä»–ç³»æ•°
                    'treatment_coef': model.params.get('treatment', None),
                    'post_coef': model.params.get('post', None),
                    'treatment_pvalue': model.pvalues.get('treatment', None),
                    'post_pvalue': model.pvalues.get('post', None),
                    
                    # æ¨¡å‹æ‹Ÿåˆç»Ÿè®¡
                    'r_squared': r_squared,
                    'adj_r_squared': adj_r_squared,
                    'f_statistic': f_statistic,
                    'f_pvalue': f_pvalue,
                    'n_observations': len(reg_data),
                    
                    # è¯Šæ–­ç»Ÿè®¡
                    'breusch_pagan_stat': bp_stat,
                    'breusch_pagan_pvalue': bp_pvalue,
                    'durbin_watson_stat': dw_stat,
                    
                    # å®Œæ•´æ¨¡å‹å¯¹è±¡ï¼ˆç”¨äºåç»­åˆ†æï¼‰
                    'full_model': model
                }
                
            else:
                # ç®€åŒ–ç‰ˆå›å½’åˆ†æï¼ˆä½¿ç”¨numpyï¼‰
                logger.info("  ä½¿ç”¨ç®€åŒ–å›å½’æ–¹æ³•ï¼ˆå»ºè®®å®‰è£…statsmodelsä»¥è·å¾—å®Œæ•´ç»Ÿè®¡ï¼‰")
                
                # å‡†å¤‡è®¾è®¡çŸ©é˜µ
                X = reg_data[['treatment', 'post', 'treatment_post'] + control_vars].values
                X = np.column_stack([np.ones(len(X)), X])  # æ·»åŠ å¸¸æ•°é¡¹
                y = reg_data[outcome_var].values
                
                # OLSä¼°è®¡
                beta = np.linalg.inv(X.T @ X) @ X.T @ y
                y_pred = X @ beta
                residuals = y - y_pred
                
                # æ ‡å‡†è¯¯è®¡ç®—
                mse = np.sum(residuals**2) / (len(y) - X.shape[1])
                var_cov_matrix = mse * np.linalg.inv(X.T @ X)
                std_errors = np.sqrt(np.diag(var_cov_matrix))
                
                # DIDç³»æ•°æ˜¯ç¬¬4ä¸ªç³»æ•°ï¼ˆtreatment_postï¼‰
                did_coef = beta[3]
                did_stderr = std_errors[3]
                did_tstat = did_coef / did_stderr
                did_pvalue = 2 * (1 - stats.t.cdf(abs(did_tstat), len(y) - X.shape[1]))
                
                # Rå¹³æ–¹
                ss_res = np.sum(residuals**2)
                ss_tot = np.sum((y - np.mean(y))**2)
                r_squared = 1 - (ss_res / ss_tot)
                adj_r_squared = 1 - (1 - r_squared) * (len(y) - 1) / (len(y) - X.shape[1])
                
                var_results = {
                    'did_coefficient': did_coef,
                    'did_std_error': did_stderr,
                    'did_t_statistic': did_tstat,
                    'did_p_value': did_pvalue,
                    'is_significant_5pct': did_pvalue < 0.05,
                    'is_significant_10pct': did_pvalue < 0.10,
                    'r_squared': r_squared,
                    'adj_r_squared': adj_r_squared,
                    'n_observations': len(reg_data),
                    'method': 'simplified_ols'
                }
            
            results[outcome_var] = var_results
            
            # æ‰“å°ä¸»è¦ç»“æœ
            logger.info(f"  âœ… {outcome_var} DIDç»“æœ:")
            logger.info(f"    ç³»æ•°: {did_coef:.6f}")
            logger.info(f"    æ ‡å‡†è¯¯: {did_stderr:.6f}")
            logger.info(f"    tç»Ÿè®¡é‡: {did_tstat:.4f}")
            logger.info(f"    på€¼: {did_pvalue:.6f}")
            logger.info(f"    5%æ˜¾è‘—æ€§: {'æ˜¯' if did_pvalue < 0.05 else 'å¦'}")
            if HAS_STATSMODELS:
                logger.info(f"    95%ç½®ä¿¡åŒºé—´: [{did_ci_lower:.6f}, {did_ci_upper:.6f}]")
            logger.info(f"    RÂ²: {r_squared:.4f}")
            
        except Exception as e:
            logger.error(f"  âŒ {outcome_var} åˆ†æå¤±è´¥: {e}")
            results[outcome_var] = {'error': str(e)}
            continue
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    successful_analyses = [k for k, v in results.items() if 'error' not in v]
    significant_5pct = [k for k, v in results.items() 
                       if 'error' not in v and v.get('is_significant_5pct', False)]
    significant_10pct = [k for k, v in results.items() 
                        if 'error' not in v and v.get('is_significant_10pct', False)]
    
    # æ·»åŠ æ±‡æ€»ä¿¡æ¯
    results['_summary'] = {
        'total_variables_analyzed': len(outcome_vars),
        'successful_analyses': len(successful_analyses),
        'significant_5pct': len(significant_5pct),
        'significant_10pct': len(significant_10pct),
        'significant_variables_5pct': significant_5pct,
        'significant_variables_10pct': significant_10pct,
        'policy_shock_year': POLICY_SHOCK_YEAR,
        'treatment_countries': TREATMENT_COUNTRIES,
        'control_countries': CONTROL_COUNTRIES,
        'analysis_timestamp': pd.Timestamp.now().isoformat()
    }
    
    logger.info("ğŸ“Š DIDåˆ†ææ±‡æ€»:")
    logger.info(f"  æˆåŠŸåˆ†æå˜é‡: {len(successful_analyses)}/{len(outcome_vars)}")
    logger.info(f"  5%æ°´å¹³æ˜¾è‘—: {len(significant_5pct)} ä¸ªå˜é‡ {significant_5pct}")
    logger.info(f"  10%æ°´å¹³æ˜¾è‘—: {len(significant_10pct)} ä¸ªå˜é‡ {significant_10pct}")
    
    logger.info("âœ… DIDåˆ†æå®Œæˆ!")
    return results

def generate_verification_report(did_results: Dict = None,
                                did_data: pd.DataFrame = None,
                                output_dir: str = None) -> str:
    """
    ç”ŸæˆDIDéªŒè¯æŠ¥å‘Š
    
    Args:
        did_results: DIDåˆ†æç»“æœ
        did_data: DIDæ•°æ®é›†
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    
    logger.info("ğŸ“ å¼€å§‹ç”ŸæˆDIDéªŒè¯æŠ¥å‘Š...")
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if output_dir is None:
        base_dir = Path(__file__).parent.parent.parent
        output_dir = base_dir / "outputs" / "tables"
        output_dir.mkdir(parents=True, exist_ok=True)
    else:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¦‚æœæ²¡æœ‰æä¾›ç»“æœï¼Œåˆ™è¿è¡Œåˆ†æ
    if did_results is None:
        did_results = run_did_analysis(did_data)
    
    # ç”ŸæˆæŠ¥å‘Šæ—¶é—´æˆ³
    timestamp = pd.Timestamp.now()
    
    # åˆ›å»ºMarkdownæŠ¥å‘Š
    md_report_path = output_dir / "dli_verification_report.md"
    
    with open(md_report_path, 'w', encoding='utf-8') as f:
        # æŠ¥å‘Šæ ‡é¢˜å’Œæ¦‚è¿°
        f.write("# DLIåŠ¨æ€é”å®šæŒ‡æ•°ç»Ÿè®¡éªŒè¯æŠ¥å‘Š\n\n")
        f.write(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## 1. ç ”ç©¶å‡è¯´ä¸å®éªŒè®¾è®¡\n\n")
        f.write("### 1.1 æ ¸å¿ƒå‡è¯´\n")
        f.write("é¡µå²©é©å‘½æ˜¯å¦æ˜¾è‘—æ”¹å˜äº†ç¾å›½ä¸è´¸æ˜“ä¼™ä¼´ä¹‹é—´çš„èƒ½æºè´¸æ˜“é”å®šæ ¼å±€ï¼ˆDLIï¼‰ï¼Ÿ\n\n")
        
        f.write("### 1.2 å®éªŒè®¾è®¡ (åŒé‡å·®åˆ†æ³•)\n")
        f.write("- **å¤„ç†ç»„**: é€šè¿‡ç®¡é“è¿›è¡ŒåŸæ²¹å’Œå¤©ç„¶æ°”è´¸æ˜“çš„ç¾-åŠ ã€ç¾-å¢¨å…³ç³»\n")
        f.write("  - å›½å®¶: åŠ æ‹¿å¤§(CAN)ã€å¢¨è¥¿å“¥(MEX)\n")
        f.write("  - äº§å“: åŸæ²¹(Crude_Oil)ã€å¤©ç„¶æ°”(Natural_Gas)\n")
        f.write("  - ç‰¹å¾: é«˜æ²‰æ²¡æˆæœ¬çš„ä¸“ç”¨æ€§åŸºç¡€è®¾æ–½é”å®š\n\n")
        
        f.write("- **æ§åˆ¶ç»„**: é€šè¿‡æµ·è¿è¿›è¡Œè´¸æ˜“çš„å…³ç³»\n")
        f.write("  - å›½å®¶: æ²™ç‰¹é˜¿æ‹‰ä¼¯(SAU)ã€å¡å¡”å°”(QAT)ã€å§”å†…ç‘æ‹‰(VEN)ã€æŒªå¨(NOR)ã€è‹±å›½(GBR)ã€ä¿„ç½—æ–¯(RUS)ã€é˜¿è”é…‹(ARE)\n")
        f.write("  - ç‰¹å¾: åŸºç¡€è®¾æ–½ä¸“ç”¨æ€§è¾ƒä½ï¼Œè½¬æ¢æˆæœ¬æ›´çµæ´»\n\n")
        
        f.write(f"- **æ”¿ç­–å†²å‡»æ—¶ç‚¹**: {POLICY_SHOCK_YEAR}å¹´ï¼ˆé¡µå²©é©å‘½æ˜¾è‘—äº§å‡ºæ•ˆåº”å¹´ä»½ï¼‰\n\n")
        
        f.write("### 1.3 DIDæ¨¡å‹\n")
        f.write("```\n")
        f.write("DLI_ijt = Î± + Î²â‚Ã—Treatment_ij + Î²â‚‚Ã—Post_t + Î²â‚ƒÃ—(Treatment_ij Ã— Post_t) + Î³Ã—X_ijt + Îµ_ijt\n")
        f.write("```\n")
        f.write("å…¶ä¸­ Î²â‚ƒ ä¸ºDIDä¼°è®¡é‡ï¼Œè¡¡é‡æ”¿ç­–å¯¹å¤„ç†ç»„çš„å‡€å½±å“ã€‚\n\n")
        
        # æ•°æ®æè¿°æ€§ç»Ÿè®¡
        if did_data is not None:
            f.write("## 2. æ•°æ®æ¦‚å†µ\n\n")
            
            summary_stats = did_results.get('_summary', {})
            f.write(f"- **æ€»è§‚æµ‹æ•°**: {len(did_data):,} æ¡è®°å½•\n")
            f.write(f"- **æ—¶é—´è·¨åº¦**: {did_data['year'].min()}-{did_data['year'].max()}\n")
            f.write(f"- **è´¸æ˜“ä¼™ä¼´**: {did_data['us_partner'].nunique()} ä¸ªå›½å®¶\n")
            f.write(f"- **èƒ½æºäº§å“**: {did_data['energy_product'].nunique()} ç§\n")
            f.write(f"- **å¤„ç†ç»„è§‚æµ‹**: {did_data['treatment'].sum():,} ({did_data['treatment'].mean()*100:.1f}%)\n")
            f.write(f"- **æ”¿ç­–åè§‚æµ‹**: {did_data['post'].sum():,} ({did_data['post'].mean()*100:.1f}%)\n\n")
            
            # æŒ‰ç»„å’Œæ—¶æœŸçš„æè¿°æ€§ç»Ÿè®¡
            desc_stats = did_data.groupby(['treatment', 'period'])['dli_composite_adjusted'].agg(['count', 'mean', 'std']).round(4)
            f.write("### 2.1 æŒ‰ç»„å’Œæ—¶æœŸçš„DLIå‡å€¼\n\n")
            f.write("| ç»„åˆ« | æ—¶æœŸ | è§‚æµ‹æ•° | å‡å€¼ | æ ‡å‡†å·® |\n")
            f.write("|------|------|--------|------|--------|\n")
            for (treatment, period), row in desc_stats.iterrows():
                group_name = "å¤„ç†ç»„" if treatment == 1 else "æ§åˆ¶ç»„"
                period_name = "æ”¿ç­–å‰" if period == 'pre' else "æ”¿ç­–å"
                f.write(f"| {group_name} | {period_name} | {row['count']} | {row['mean']:.4f} | {row['std']:.4f} |\n")
            f.write("\n")
        
        # DIDåˆ†æç»“æœ
        f.write("## 3. DIDåˆ†æç»“æœ\n\n")
        
        summary = did_results.get('_summary', {})
        f.write(f"- **æˆåŠŸåˆ†æå˜é‡æ•°**: {summary.get('successful_analyses', 0)}/{summary.get('total_variables_analyzed', 0)}\n")
        f.write(f"- **5%æ°´å¹³æ˜¾è‘—å˜é‡**: {summary.get('significant_5pct', 0)} ä¸ª\n")
        f.write(f"- **10%æ°´å¹³æ˜¾è‘—å˜é‡**: {summary.get('significant_10pct', 0)} ä¸ª\n\n")
        
        # è¯¦ç»†ç»“æœè¡¨
        f.write("### 3.1 è¯¦ç»†å›å½’ç»“æœ\n\n")
        f.write("| è¢«è§£é‡Šå˜é‡ | DIDç³»æ•° | æ ‡å‡†è¯¯ | tç»Ÿè®¡é‡ | på€¼ | RÂ² | è§‚æµ‹æ•° | 5%æ˜¾è‘— |\n")
        f.write("|------------|---------|--------|---------|-----|-----|--------|--------|\n")
        
        for var, results in did_results.items():
            if var.startswith('_') or 'error' in results:
                continue
            
            coef = results.get('did_coefficient', 0)
            stderr = results.get('did_std_error', 0)
            t_stat = results.get('did_t_statistic', 0)
            p_val = results.get('did_p_value', 1)
            r_sq = results.get('r_squared', 0)
            n_obs = results.get('n_observations', 0)
            is_sig = "âœ“" if results.get('is_significant_5pct', False) else ""
            
            f.write(f"| {var} | {coef:.6f} | {stderr:.6f} | {t_stat:.4f} | {p_val:.6f} | {r_sq:.4f} | {n_obs:,} | {is_sig} |\n")
        
        f.write("\n")
        
        # å…³é”®å‘ç°
        f.write("## 4. å…³é”®å‘ç°\n\n")
        
        significant_vars_5 = summary.get('significant_variables_5pct', [])
        significant_vars_10 = summary.get('significant_variables_10pct', [])
        
        if significant_vars_5:
            f.write("### 4.1 ç»Ÿè®¡æ˜¾è‘—çš„æ”¿ç­–æ•ˆåº” (5%æ°´å¹³)\n\n")
            for var in significant_vars_5:
                if var in did_results:
                    result = did_results[var]
                    coef = result.get('did_coefficient', 0)
                    f.write(f"- **{var}**: DIDç³»æ•° = {coef:.6f}")
                    if coef > 0:
                        f.write(" (æ”¿ç­–å¢å¼ºäº†é”å®šæ•ˆåº”)\n")
                    else:
                        f.write(" (æ”¿ç­–å‡å¼±äº†é”å®šæ•ˆåº”)\n")
            f.write("\n")
        else:
            f.write("### 4.1 ç»Ÿè®¡æ˜¾è‘—æ€§\n")
            f.write("åœ¨5%æ˜¾è‘—æ€§æ°´å¹³ä¸‹ï¼Œæœªå‘ç°é¡µå²©é©å‘½å¯¹DLIæŒ‡æ ‡çš„æ˜¾è‘—å½±å“ã€‚\n\n")
        
        if significant_vars_10:
            f.write("### 4.2 è¾¹é™…æ˜¾è‘—çš„æ”¿ç­–æ•ˆåº” (10%æ°´å¹³)\n\n")
            for var in significant_vars_10:
                if var in did_results and var not in significant_vars_5:
                    result = did_results[var]
                    coef = result.get('did_coefficient', 0)
                    f.write(f"- **{var}**: DIDç³»æ•° = {coef:.6f}")
                    if coef > 0:
                        f.write(" (æ”¿ç­–å¯èƒ½å¢å¼ºäº†é”å®šæ•ˆåº”)\n")
                    else:
                        f.write(" (æ”¿ç­–å¯èƒ½å‡å¼±äº†é”å®šæ•ˆåº”)\n")
            f.write("\n")
        
        # ç»“è®º
        f.write("## 5. ç»“è®º\n\n")
        
        if significant_vars_5:
            f.write("åŸºäºåŒé‡å·®åˆ†åˆ†æï¼Œæˆ‘ä»¬å‘ç°é¡µå²©é©å‘½å¯¹ç¾å›½èƒ½æºè´¸æ˜“é”å®šæ ¼å±€äº§ç”Ÿäº†ç»Ÿè®¡æ˜¾è‘—çš„å½±å“ã€‚")
            f.write("å…·ä½“è€Œè¨€ï¼Œç®¡é“è´¸æ˜“å…³ç³»ï¼ˆç¾-åŠ ã€ç¾-å¢¨ï¼‰ç›¸è¾ƒäºæµ·è¿è´¸æ˜“å…³ç³»ï¼Œ")
            f.write("åœ¨é¡µå²©é©å‘½åè¡¨ç°å‡ºäº†ä¸åŒçš„é”å®šæ¨¡å¼å˜åŒ–ã€‚è¿™ä¸€å‘ç°æ”¯æŒäº†æˆ‘ä»¬å…³äº")
            f.write("åŸºç¡€è®¾æ–½ä¸“ç”¨æ€§åœ¨æ”¿ç­–ä¼ å¯¼ä¸­é‡è¦ä½œç”¨çš„ç†è®ºå‡è¯´ã€‚\n\n")
        else:
            f.write("åŸºäºåŒé‡å·®åˆ†åˆ†æï¼Œæˆ‘ä»¬æœªèƒ½åœ¨5%æ˜¾è‘—æ€§æ°´å¹³ä¸‹å‘ç°é¡µå²©é©å‘½å¯¹ç¾å›½èƒ½æºè´¸æ˜“é”å®šæ ¼å±€çš„ç»Ÿè®¡æ˜¾è‘—å½±å“ã€‚")
            f.write("è¿™å¯èƒ½è¡¨æ˜ï¼š(1) æ”¿ç­–æ•ˆåº”ç¡®å®ä¸å­˜åœ¨ï¼›(2) æ•ˆåº”å­˜åœ¨ä½†ç›¸å¯¹è¾ƒå°ï¼Œéœ€è¦æ›´å¤§æ ·æœ¬æ‰èƒ½æ£€æµ‹åˆ°ï¼›")
            f.write("(3) å®éªŒè®¾è®¡éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚å»ºè®®åç»­ç ”ç©¶è€ƒè™‘æ›´ç²¾ç»†çš„åˆ†ç»„ç­–ç•¥æˆ–æ›´é•¿çš„è§‚æµ‹æœŸã€‚\n\n")
        
        f.write("## 6. æ–¹æ³•è®ºæ³¨è®°\n\n")
        f.write("- **å› æœæ¨æ–­æ–¹æ³•**: åŒé‡å·®åˆ†æ³•(Difference-in-Differences)\n")
        f.write("- **æ ‡å‡†è¯¯ä¼°è®¡**: å¼‚æ–¹å·®ç¨³å¥æ ‡å‡†è¯¯\n")
        if HAS_STATSMODELS:
            f.write("- **è¯Šæ–­æ£€éªŒ**: Breusch-Paganå¼‚æ–¹å·®æ£€éªŒ, Durbin-Watsonåºåˆ—ç›¸å…³æ£€éªŒ\n")
        f.write("- **æ˜¾è‘—æ€§æ°´å¹³**: 5%å’Œ10%\n")
        f.write("- **è½¯ä»¶å·¥å…·**: Python statsmodels\n\n")
        
        f.write("---\n")
        f.write("*æœ¬æŠ¥å‘Šç”±DLIåˆ†ææ¨¡å—è‡ªåŠ¨ç”Ÿæˆ*\n")
    
    logger.info(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_report_path}")
    
    # åŒæ—¶ç”ŸæˆCSVç»“æœè¡¨
    csv_report_path = output_dir / "dli_verification_results.csv"
    
    results_for_csv = []
    for var, results in did_results.items():
        if var.startswith('_') or 'error' in results:
            continue
        
        row = {
            'variable': var,
            'did_coefficient': results.get('did_coefficient', np.nan),
            'did_std_error': results.get('did_std_error', np.nan),
            'did_t_statistic': results.get('did_t_statistic', np.nan),
            'did_p_value': results.get('did_p_value', np.nan),
            'significant_5pct': results.get('is_significant_5pct', False),
            'significant_10pct': results.get('is_significant_10pct', False),
            'r_squared': results.get('r_squared', np.nan),
            'n_observations': results.get('n_observations', 0)
        }
        
        if HAS_STATSMODELS and 'did_ci_lower' in results:
            row['ci_lower'] = results['did_ci_lower']
            row['ci_upper'] = results['did_ci_upper']
        
        results_for_csv.append(row)
    
    results_df = pd.DataFrame(results_for_csv)
    results_df.to_csv(csv_report_path, index=False)
    logger.info(f"ğŸ“Š CSVç»“æœå·²ç”Ÿæˆ: {csv_report_path}")
    
    logger.info("âœ… éªŒè¯æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
    return str(md_report_path)

def run_full_verification_analysis(dli_data_path: str = None,
                                  output_dir: str = None) -> Dict[str, str]:
    """
    æ‰§è¡Œå®Œæ•´çš„DLIç»Ÿè®¡éªŒè¯åˆ†ææµç¨‹
    
    è¿™æ˜¯ç»Ÿè®¡éªŒè¯æ¨¡å—çš„ä¸»è¦æ¥å£å‡½æ•°
    
    Args:
        dli_data_path: DLIé¢æ¿æ•°æ®æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        åŒ…å«è¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    
    logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„DLIç»Ÿè®¡éªŒè¯åˆ†æ...")
    
    try:
        # ç¬¬1æ­¥ï¼šå‡†å¤‡DIDæ•°æ®é›†
        logger.info("ğŸ“‹ ç¬¬1æ­¥ï¼šå‡†å¤‡DIDåˆ†ææ•°æ®é›†...")
        did_data = prepare_did_dataset(data_file_path=dli_data_path)
        
        # ç¬¬2æ­¥ï¼šæ‰§è¡ŒDIDåˆ†æ
        logger.info("ğŸ“Š ç¬¬2æ­¥ï¼šæ‰§è¡ŒåŒé‡å·®åˆ†åˆ†æ...")
        did_results = run_did_analysis(did_data)
        
        # ç¬¬3æ­¥ï¼šç”ŸæˆéªŒè¯æŠ¥å‘Š
        logger.info("ğŸ“ ç¬¬3æ­¥ï¼šç”Ÿæˆç»Ÿè®¡éªŒè¯æŠ¥å‘Š...")
        report_path = generate_verification_report(did_results, did_data, output_dir)
        
        # è¿”å›è¾“å‡ºæ–‡ä»¶
        output_files = {
            'verification_report_md': report_path,
            'verification_results_csv': report_path.replace('.md', '_results.csv')
        }
        
        logger.info("âœ… å®Œæ•´çš„DLIç»Ÿè®¡éªŒè¯åˆ†æå®Œæˆ!")
        logger.info(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {output_files['verification_report_md']}")
        logger.info(f"ğŸ“Š ç»“æœæ–‡ä»¶: {output_files['verification_results_csv']}")
        
        return output_files
        
    except Exception as e:
        logger.error(f"âŒ DLIç»Ÿè®¡éªŒè¯åˆ†æå¤±è´¥: {e}")
        raise

# ç®€åŒ–ç‰ˆç»Ÿè®¡å‡½æ•°ï¼ˆå½“æ²¡æœ‰statsmodelsæ—¶ä½¿ç”¨ï¼‰
if not HAS_STATSMODELS:
    from scipy import stats
    logger.warning("statsmodelsæœªå®‰è£…ï¼Œå°†ä½¿ç”¨scipyè¿›è¡ŒåŸºç¡€ç»Ÿè®¡åˆ†æ")

if __name__ == "__main__":
    # æµ‹è¯•ç»Ÿè®¡éªŒè¯åŠŸèƒ½
    try:
        output_files = run_full_verification_analysis()
        print("âœ… DLIç»Ÿè®¡éªŒè¯åˆ†ææˆåŠŸå®Œæˆ!")
        for file_type, path in output_files.items():
            print(f"ğŸ“ {file_type}: {path}")
        
    except Exception as e:
        logger.error(f"âŒ ç»Ÿè®¡éªŒè¯åˆ†æå¤±è´¥: {e}")
        raise