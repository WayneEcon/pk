#!/usr/bin/env python3
"""
DLIæŒ‡æ ‡è®¡ç®—æ¨¡å— (DLI Calculator Module)
==========================================

æœ¬æ¨¡å—å®ç°åŠ¨æ€é”å®šæŒ‡æ•°(Dynamic Locking Index)çš„æ ¸å¿ƒè®¡ç®—ç®—æ³•ã€‚
DLIé€šè¿‡å››ä¸ªç»´åº¦é‡åŒ–å›½å®¶é—´èƒ½æºè´¸æ˜“å…³ç³»çš„è·¯å¾„ä¾èµ–å’Œè½¬æ¢æˆæœ¬ï¼š

1. è´¸æ˜“æŒç»­æ€§ (Continuity): è¡¡é‡å…³ç³»çš„é•¿æœŸæ€§
2. åŸºç¡€è®¾æ–½å¼ºåº¦ (Infrastructure): è¡¡é‡ä¸“ç”¨æ€§èµ„äº§å¯¼è‡´çš„é”å®š
3. è´¸æ˜“ç¨³å®šæ€§ (Stability): è¡¡é‡å…³ç³»çš„å¯é æ€§
4. å¸‚åœºé”å®šåŠ› (Market Locking Power): è¡¡é‡å¸‚åœºç»“æ„å¯¼è‡´çš„é”å®šæ•ˆåº”

æœ€ç»ˆé€šè¿‡ä¸»æˆåˆ†åˆ†æ(PCA)ç¡®å®šæƒé‡ï¼ŒåˆæˆDLIæ€»åˆ†ã€‚

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import Dict, List, Tuple, Optional, Union
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# å¯¼å…¥æ•°æ®å‡†å¤‡æ¨¡å—çš„å…¨å±€æ•°æ®åŠ è½½åŠŸèƒ½
from data_preparation import load_global_trade_data_range
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def calculate_continuity(df: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®—è´¸æ˜“æŒç»­æ€§æŒ‡æ ‡ (Continuity)
    
    å…¬å¼: Continuity_ijt = (ä»2001å¹´åˆ°tå¹´å­˜åœ¨è´¸æ˜“çš„å¹´æ•°) / (t - 2001 + 1)
    
    è§£é‡Š: è¡¡é‡ç¾å›½ä¸ä¼™ä¼´å›½jåœ¨èƒ½æºäº§å“iä¸Šçš„è´¸æ˜“å…³ç³»æŒç»­æ€§ã€‚
    å€¼è¶Šæ¥è¿‘1ï¼Œè¡¨ç¤ºè´¸æ˜“å…³ç³»è¶Šç¨³å®šæŒç»­ï¼›å€¼è¶Šæ¥è¿‘0ï¼Œè¡¨ç¤ºè´¸æ˜“å…³ç³»è¶Šä¸ç¨³å®šã€‚
    
    Args:
        df: åŒ…å«ç¾å›½è´¸æ˜“æ•°æ®çš„DataFrame
        å¿…é¡»åŒ…å«åˆ—: year, us_partner, energy_product, trade_value_usd
        
    Returns:
        æ·»åŠ äº†continuityåˆ—çš„DataFrame
        
    ç¤ºä¾‹:
        å¦‚æœç¾å›½ä¸åŠ æ‹¿å¤§çš„åŸæ²¹è´¸æ˜“åœ¨2001-2010å¹´é—´æœ‰8å¹´å­˜åœ¨è´¸æ˜“ï¼Œ
        é‚£ä¹ˆ2010å¹´çš„æŒç»­æ€§ = 8 / (2010-2001+1) = 8/10 = 0.8
    """
    
    logger.info("ğŸ”„ å¼€å§‹è®¡ç®—è´¸æ˜“æŒç»­æ€§æŒ‡æ ‡...")
    
    df_continuity = df.copy()
    
    # æŒ‰ä¼™ä¼´å›½å’Œèƒ½æºäº§å“åˆ†ç»„è®¡ç®—æŒç»­æ€§
    continuity_results = []
    
    # è·å–æ‰€æœ‰uniqueç»„åˆï¼ˆå¿…é¡»åŒ…å«us_roleä»¥åŒºåˆ†è¿›å‡ºå£ï¼‰
    if 'us_role' in df_continuity.columns:
        groups = df_continuity.groupby(['us_partner', 'energy_product', 'us_role'])
    else:
        logger.warning("ç¼ºå°‘us_roleå­—æ®µï¼Œå°†æŒ‰å›½å®¶-äº§å“ç»„åˆè®¡ç®—æŒç»­æ€§ï¼ˆå¯èƒ½æ··åˆè¿›å‡ºå£æ•°æ®ï¼‰")
        groups = df_continuity.groupby(['us_partner', 'energy_product'])
    
    for group_key, group_data in groups:
        # è§£åŒ…ç»„åˆé”®
        if 'us_role' in df_continuity.columns:
            partner, product, us_role = group_key
        else:
            partner, product = group_key
            us_role = None
            
        # è·å–è¯¥ç»„åˆçš„æ‰€æœ‰å¹´ä»½
        trade_years = set(group_data['year'].unique())
        
        # ä¸ºæ¯ä¸€å¹´è®¡ç®—æŒç»­æ€§
        for year in trade_years:
            # è®¡ç®—ä»2001å¹´åˆ°å½“å‰å¹´ä»½åº”æœ‰çš„å¹´æ•°
            total_possible_years = year - 2001 + 1
            
            # è®¡ç®—ä»2001å¹´åˆ°å½“å‰å¹´ä»½å®é™…å­˜åœ¨è´¸æ˜“çš„å¹´æ•°
            actual_trade_years = len([y for y in trade_years if 2001 <= y <= year])
            
            # è®¡ç®—æŒç»­æ€§
            continuity = actual_trade_years / total_possible_years if total_possible_years > 0 else 0
            
            result_record = {
                'year': year,
                'us_partner': partner,
                'energy_product': product,
                'continuity': continuity,
                'actual_trade_years': actual_trade_years,
                'total_possible_years': total_possible_years
            }
            
            if us_role is not None:
                result_record['us_role'] = us_role
                
            continuity_results.append(result_record)
    
    # è½¬æ¢ä¸ºDataFrame
    continuity_df = pd.DataFrame(continuity_results)
    
    # ä¸åŸæ•°æ®åˆå¹¶ï¼ˆç¡®ä¿åŒ…å«us_roleå­—æ®µé¿å…è¿›å‡ºå£æ•°æ®æ··æ·†ï¼‰
    merge_keys = ['year', 'us_partner', 'energy_product']
    if 'us_role' in df_continuity.columns:
        merge_keys.append('us_role')
    
    df_with_continuity = pd.merge(
        df_continuity, 
        continuity_df[merge_keys + ['continuity']], 
        on=merge_keys, 
        how='left'
    )
    
    # æ•°æ®éªŒè¯
    assert df_with_continuity['continuity'].isnull().sum() == 0, "æŒç»­æ€§æŒ‡æ ‡è®¡ç®—ä¸­å­˜åœ¨ç¼ºå¤±å€¼"
    assert (df_with_continuity['continuity'] >= 0).all() and (df_with_continuity['continuity'] <= 1).all(), "æŒç»­æ€§æŒ‡æ ‡å€¼è¶…å‡º[0,1]èŒƒå›´"
    
    # ç»Ÿè®¡æ‘˜è¦
    logger.info(f"ğŸ“Š è´¸æ˜“æŒç»­æ€§ç»Ÿè®¡:")
    logger.info(f"  å¹³å‡æŒç»­æ€§: {df_with_continuity['continuity'].mean():.3f}")
    logger.info(f"  ä¸­ä½æ•°æŒç»­æ€§: {df_with_continuity['continuity'].median():.3f}")
    logger.info(f"  æœ€é«˜æŒç»­æ€§: {df_with_continuity['continuity'].max():.3f}")
    logger.info(f"  æœ€ä½æŒç»­æ€§: {df_with_continuity['continuity'].min():.3f}")
    logger.info(f"  å®Œå…¨æŒç»­å…³ç³»(=1): {(df_with_continuity['continuity'] == 1).sum()} æ¡è®°å½•")
    
    logger.info("âœ… è´¸æ˜“æŒç»­æ€§æŒ‡æ ‡è®¡ç®—å®Œæˆ!")
    return df_with_continuity

def calculate_infrastructure(df: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®—åŸºç¡€è®¾æ–½å¼ºåº¦æŒ‡æ ‡ (Infrastructure)
    
    å…¬å¼: Infrastructure_ijt = log(Trade_Volume_ijt / Distance_ij + 1)
    
    æ–¹æ³•è®ºè¯´æ˜:
    ç›®æ ‡: è¡¡é‡ç”±é«˜æ²‰æ²¡æˆæœ¬çš„ä¸“ç”¨æ€§èµ„äº§ï¼ˆå¦‚ç®¡é“ã€ä¸“ç”¨æ¸¯å£ï¼‰å¯¼è‡´çš„é”å®šã€‚
    æŒ‘æˆ˜: è¿™äº›èµ„äº§çš„ä»·å€¼æ˜¯ä¸å¯è§‚æµ‹çš„ã€‚
    è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨ä»£ç†å˜é‡ã€‚ç»æµå­¦é€»è¾‘æ˜¯ï¼šåªæœ‰åœ¨åœ°ç†è·ç¦»è¿‘ä¸”è´¸æ˜“é‡å·¨å¤§çš„æƒ…å†µä¸‹ï¼Œ
    æŠ•èµ„æ˜‚è´µçš„ã€ä¸å¯ç§»åŠ¨çš„ä¸“ç”¨æ€§åŸºç¡€è®¾æ–½æ‰å…·æœ‰ç»æµåˆç†æ€§ã€‚
    å› æ­¤ï¼Œè´¸æ˜“é¢/è·ç¦» è¿™ä¸ªæ¯”å€¼ï¼Œå¯ä»¥ä½œä¸ºè¡¡é‡è¿™ç§"åŸºç¡€è®¾æ–½ç»‘å®š"å¼ºåº¦çš„æœ‰æ•ˆä»£ç†ã€‚
    
    Args:
        df: åŒ…å«ç¾å›½è´¸æ˜“æ•°æ®çš„DataFrame
        å¿…é¡»åŒ…å«åˆ—: trade_value_usd, distance_km
        
    Returns:
        æ·»åŠ äº†infrastructureåˆ—çš„DataFrame
        
    ç¤ºä¾‹:
        ç¾å›½ä¸åŠ æ‹¿å¤§åŸæ²¹è´¸æ˜“é¢1000ä¸‡ç¾å…ƒï¼Œè·ç¦»735å…¬é‡Œ
        Infrastructure = log(10000000 / 735 + 1) = log(13606 + 1) = log(13607) â‰ˆ 9.52
    """
    
    logger.info("ğŸ—ï¸ å¼€å§‹è®¡ç®—åŸºç¡€è®¾æ–½å¼ºåº¦æŒ‡æ ‡...")
    
    df_infrastructure = df.copy()
    
    # æ•°æ®éªŒè¯
    assert 'trade_value_usd' in df_infrastructure.columns, "ç¼ºå°‘trade_value_usdåˆ—"
    assert 'distance_km' in df_infrastructure.columns, "ç¼ºå°‘distance_kmåˆ—"
    assert (df_infrastructure['trade_value_usd'] > 0).all(), "è´¸æ˜“å€¼å¿…é¡»å¤§äº0"
    assert (df_infrastructure['distance_km'] > 0).all(), "è·ç¦»å¿…é¡»å¤§äº0"
    
    # è®¡ç®—åŸºç¡€è®¾æ–½å¼ºåº¦
    # ä½¿ç”¨+1é¿å…log(0)çš„æƒ…å†µï¼Œè™½ç„¶åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸­ä¸å¤ªå¯èƒ½å‡ºç°
    df_infrastructure['trade_distance_ratio'] = df_infrastructure['trade_value_usd'] / df_infrastructure['distance_km']
    df_infrastructure['infrastructure'] = np.log(df_infrastructure['trade_distance_ratio'] + 1)
    
    # æ•°æ®éªŒè¯
    assert df_infrastructure['infrastructure'].isnull().sum() == 0, "åŸºç¡€è®¾æ–½æŒ‡æ ‡è®¡ç®—ä¸­å­˜åœ¨ç¼ºå¤±å€¼"
    assert (df_infrastructure['infrastructure'] >= 0).all(), "åŸºç¡€è®¾æ–½æŒ‡æ ‡å€¼ä¸èƒ½ä¸ºè´Ÿ"
    
    # ç»Ÿè®¡æ‘˜è¦
    logger.info(f"ğŸ“Š åŸºç¡€è®¾æ–½å¼ºåº¦ç»Ÿè®¡:")
    logger.info(f"  å¹³å‡å¼ºåº¦: {df_infrastructure['infrastructure'].mean():.3f}")
    logger.info(f"  ä¸­ä½æ•°å¼ºåº¦: {df_infrastructure['infrastructure'].median():.3f}")
    logger.info(f"  æœ€é«˜å¼ºåº¦: {df_infrastructure['infrastructure'].max():.3f}")
    logger.info(f"  æœ€ä½å¼ºåº¦: {df_infrastructure['infrastructure'].min():.3f}")
    logger.info(f"  æ ‡å‡†å·®: {df_infrastructure['infrastructure'].std():.3f}")
    
    # æŒ‰è·ç¦»åŒºé—´åˆ†æ
    if 'distance_category' in df_infrastructure.columns:
        distance_infra_stats = df_infrastructure.groupby('distance_category')['infrastructure'].agg(['mean', 'std', 'count'])
        logger.info(f"  æŒ‰è·ç¦»åŒºé—´çš„åŸºç¡€è®¾æ–½å¼ºåº¦:")
        for category, stats in distance_infra_stats.iterrows():
            logger.info(f"    {category}: å‡å€¼={stats['mean']:.3f}, æ ‡å‡†å·®={stats['std']:.3f}, è®°å½•æ•°={stats['count']}")
    
    logger.info("âœ… åŸºç¡€è®¾æ–½å¼ºåº¦æŒ‡æ ‡è®¡ç®—å®Œæˆ!")
    return df_infrastructure

def calculate_stability(df: pd.DataFrame, window_years: int = 5) -> pd.DataFrame:
    """
    è®¡ç®—è´¸æ˜“ç¨³å®šæ€§æŒ‡æ ‡ (Stability)
    
    å…¬å¼: Stability_ijt = 1 / (CV_ijt + 0.1)
    å…¶ä¸­ CV_ijt æ˜¯ä¼™ä¼´jåœ¨äº§å“iä¸Šè¿‡å»window_yearså¹´è´¸æ˜“é¢çš„å˜å¼‚ç³»æ•° (æ ‡å‡†å·®/å¹³å‡å€¼)
    
    è§£é‡Š: è¡¡é‡å…³ç³»çš„å¯é æ€§ï¼Œæ³¢åŠ¨è¶Šå°ï¼Œé”å®šè¶Šå¼ºã€‚
    å˜å¼‚ç³»æ•°è¶Šå°ï¼Œç¨³å®šæ€§æŒ‡æ ‡è¶Šå¤§ï¼Œè¡¨ç¤ºè´¸æ˜“å…³ç³»è¶Šç¨³å®šã€‚
    
    Args:
        df: åŒ…å«ç¾å›½è´¸æ˜“æ•°æ®çš„DataFrame
        window_years: è®¡ç®—ç¨³å®šæ€§çš„æ»‘åŠ¨çª—å£å¹´æ•°ï¼Œé»˜è®¤ä¸º5å¹´
        
    Returns:
        æ·»åŠ äº†stabilityåˆ—çš„DataFrame
        
    ç¤ºä¾‹:
        å¦‚æœæŸå›½æŸäº§å“è¿‡å»5å¹´è´¸æ˜“é¢æ ‡å‡†å·®ä¸º1000ä¸‡ï¼Œå¹³å‡å€¼ä¸º5000ä¸‡
        CV = 1000/5000 = 0.2
        Stability = 1 / (0.2 + 0.1) = 1 / 0.3 = 3.33
    """
    
    logger.info(f"ğŸ“ˆ å¼€å§‹è®¡ç®—è´¸æ˜“ç¨³å®šæ€§æŒ‡æ ‡ (çª—å£æœŸ={window_years}å¹´)...")
    
    df_stability = df.copy()
    
    # æŒ‰ä¼™ä¼´å›½ã€èƒ½æºäº§å“å’Œè´¸æ˜“è§’è‰²åˆ†ç»„ï¼Œè®¡ç®—æ¯å¹´çš„ç¨³å®šæ€§
    stability_results = []
    
    if 'us_role' in df_stability.columns:
        groups = df_stability.groupby(['us_partner', 'energy_product', 'us_role'])
    else:
        logger.warning("ç¼ºå°‘us_roleå­—æ®µï¼Œå°†æŒ‰å›½å®¶-äº§å“ç»„åˆè®¡ç®—ç¨³å®šæ€§ï¼ˆå¯èƒ½æ··åˆè¿›å‡ºå£æ•°æ®ï¼‰")
        groups = df_stability.groupby(['us_partner', 'energy_product'])
    
    for group_key, group_data in groups:
        # è§£åŒ…ç»„åˆé”®
        if 'us_role' in df_stability.columns:
            partner, product, us_role = group_key
        else:
            partner, product = group_key
            us_role = None
        # æŒ‰å¹´ä»½æ’åº
        group_data = group_data.sort_values('year')
        
        # èšåˆæ¯å¹´çš„è´¸æ˜“é¢ï¼ˆå› ä¸ºå¯èƒ½æœ‰è¿›å£å’Œå‡ºå£ï¼‰
        yearly_trade = group_data.groupby('year')['trade_value_usd'].sum().reset_index()
        
        # ä¸ºæ¯ä¸€å¹´è®¡ç®—ç¨³å®šæ€§
        for i, row in yearly_trade.iterrows():
            current_year = row['year']
            
            # è·å–è¿‡å»window_yearså¹´çš„å†å²æ•°æ®ï¼ˆä¸åŒ…æ‹¬å½“å‰å¹´ï¼Œé¿å…å‰è§†åè¯¯ï¼‰
            start_year = current_year - window_years
            window_data = yearly_trade[
                (yearly_trade['year'] >= start_year) & 
                (yearly_trade['year'] < current_year)  # ä¸¥æ ¼å°äºå½“å‰å¹´
            ]
            
            if len(window_data) >= 3:  # è‡³å°‘éœ€è¦3ä¸ªå†å²è§‚æµ‹å€¼æ‰å¯é 
                trade_values = window_data['trade_value_usd'].values
                
                # è®¡ç®—å˜å¼‚ç³»æ•° (CV = std / mean)
                mean_trade = np.mean(trade_values)
                std_trade = np.std(trade_values)
                
                if mean_trade > 0:
                    cv = std_trade / mean_trade
                    stability = 1 / (cv + 0.1)  # åŠ 0.1é¿å…åˆ†æ¯ä¸º0
                else:
                    # å‡å€¼ä¸º0è¡¨ç¤ºæ•°æ®è´¨é‡é—®é¢˜ï¼Œæ ‡è®°ä¸ºç¼ºå¤±
                    stability = np.nan
                    cv = np.nan
                
                result_record = {
                    'year': current_year,
                    'us_partner': partner,
                    'energy_product': product,
                    'stability': stability,
                    'cv': cv,
                    'window_years_used': len(window_data),
                    'mean_trade_value': mean_trade,
                    'std_trade_value': std_trade
                }
                
                if us_role is not None:
                    result_record['us_role'] = us_role
                    
                stability_results.append(result_record)
            elif len(window_data) == 2:
                # å†å²æ•°æ®ä¸è¶³ä½†æœ‰ä¸€äº›ä¿¡æ¯ï¼Œç»™äºˆä¸­ç­‰ç¨³å®šæ€§è¯„åˆ†
                result_record = {
                    'year': current_year,
                    'us_partner': partner,
                    'energy_product': product,
                    'stability': 5.0,  # ä¸­ç­‰æ°´å¹³ç¨³å®šæ€§
                    'cv': np.nan,
                    'window_years_used': len(window_data),
                    'mean_trade_value': np.mean(window_data['trade_value_usd'].values),
                    'std_trade_value': np.std(window_data['trade_value_usd'].values)
                }
                
                if us_role is not None:
                    result_record['us_role'] = us_role
                    
                stability_results.append(result_record)
            else:
                # å†å²æ•°æ®ä¸¥é‡ä¸è¶³ï¼Œæ ‡è®°ä¸ºç¼ºå¤±å€¼ä½†è®°å½•å½“å‰å¹´ä¿¡æ¯
                result_record = {
                    'year': current_year,
                    'us_partner': partner,
                    'energy_product': product,
                    'stability': np.nan,  # æ•°æ®ä¸è¶³ï¼Œæ ‡è®°ä¸ºç¼ºå¤±
                    'cv': np.nan,
                    'window_years_used': len(window_data),
                    'mean_trade_value': row['trade_value_usd'],
                    'std_trade_value': 0
                }
                
                if us_role is not None:
                    result_record['us_role'] = us_role
                    
                stability_results.append(result_record)
    
    # è½¬æ¢ä¸ºDataFrame
    stability_df = pd.DataFrame(stability_results)
    
    # ä¸åŸæ•°æ®åˆå¹¶ï¼ˆç¡®ä¿åŒ…å«us_roleå­—æ®µé¿å…è¿›å‡ºå£æ•°æ®æ··æ·†ï¼‰
    merge_keys = ['year', 'us_partner', 'energy_product']
    if 'us_role' in df_stability.columns:
        merge_keys.append('us_role')
        
    df_with_stability = pd.merge(
        df_stability, 
        stability_df[merge_keys + ['stability']], 
        on=merge_keys, 
        how='left'
    )
    
    # å¤„ç†ç¼ºå¤±å€¼ï¼šå¯¹äºå†å²æ•°æ®ä¸è¶³çš„æƒ…å†µï¼Œä½¿ç”¨å…¨å±€å¹³å‡ç¨³å®šæ€§
    missing_stability_count = df_with_stability['stability'].isnull().sum()
    if missing_stability_count > 0:
        logger.warning(f"å‘ç° {missing_stability_count} æ¡ç¨³å®šæ€§æŒ‡æ ‡ç¼ºå¤±å€¼ï¼Œå°†ä½¿ç”¨å…¨å±€å‡å€¼å¡«å……")
        global_mean_stability = df_with_stability['stability'].mean()
        df_with_stability['stability'] = df_with_stability['stability'].fillna(global_mean_stability)
    
    # æ•°æ®éªŒè¯ï¼ˆå…è®¸ä¸€å®šæ¯”ä¾‹çš„ç¼ºå¤±å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æµ‹è¯•å°æ‰¹é‡æ•°æ®æ—¶ï¼‰
    missing_count = df_with_stability['stability'].isnull().sum()
    if missing_count > 0:
        missing_pct = missing_count / len(df_with_stability) * 100
        if missing_pct > 80:  # å¦‚æœè¶…è¿‡80%ç¼ºå¤±ï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError(f"ç¨³å®šæ€§æŒ‡æ ‡ç¼ºå¤±æ¯”ä¾‹è¿‡é«˜: {missing_pct:.1f}% ({missing_count}/{len(df_with_stability)})")
        elif missing_pct > 50:  # å¦‚æœè¶…è¿‡50%ç¼ºå¤±ï¼Œç»™å‡ºè­¦å‘Š
            logger.warning(f"ç¨³å®šæ€§æŒ‡æ ‡ç¼ºå¤±æ¯”ä¾‹è¾ƒé«˜: {missing_pct:.1f}% ({missing_count}/{len(df_with_stability)})")
        else:
            logger.info(f"ç¨³å®šæ€§æŒ‡æ ‡å°‘é‡ç¼ºå¤±: {missing_pct:.1f}% ({missing_count}/{len(df_with_stability)})")
    else:
        logger.info("âœ… ç¨³å®šæ€§æŒ‡æ ‡æ— ç¼ºå¤±å€¼")
    valid_stability = df_with_stability['stability'][df_with_stability['stability'].notna()]
    if len(valid_stability) > 0:
        assert (valid_stability > 0).all(), "ç¨³å®šæ€§æŒ‡æ ‡å€¼å¿…é¡»å¤§äº0"
    
    # ç»Ÿè®¡æ‘˜è¦
    logger.info(f"ğŸ“Š è´¸æ˜“ç¨³å®šæ€§ç»Ÿè®¡:")
    logger.info(f"  å¹³å‡ç¨³å®šæ€§: {df_with_stability['stability'].mean():.3f}")
    logger.info(f"  ä¸­ä½æ•°ç¨³å®šæ€§: {df_with_stability['stability'].median():.3f}")
    logger.info(f"  æœ€é«˜ç¨³å®šæ€§: {df_with_stability['stability'].max():.3f}")
    logger.info(f"  æœ€ä½ç¨³å®šæ€§: {df_with_stability['stability'].min():.3f}")
    logger.info(f"  æ ‡å‡†å·®: {df_with_stability['stability'].std():.3f}")
    
    # æŒ‰äº§å“åˆ†æç¨³å®šæ€§
    product_stability = df_with_stability.groupby('energy_product')['stability'].agg(['mean', 'std', 'count'])
    logger.info(f"  æŒ‰èƒ½æºäº§å“çš„ç¨³å®šæ€§:")
    for product, stats in product_stability.iterrows():
        logger.info(f"    {product}: å‡å€¼={stats['mean']:.3f}, æ ‡å‡†å·®={stats['std']:.3f}, è®°å½•æ•°={stats['count']}")
    
    logger.info("âœ… è´¸æ˜“ç¨³å®šæ€§æŒ‡æ ‡è®¡ç®—å®Œæˆ!")
    return df_with_stability

def calculate_import_locking_power(df: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®—è¿›å£é”å®šåŠ›æŒ‡æ ‡ (Import Locking Power)
    
    å…¬å¼: Import_Locking_Power_ijt = HHI_it * share_ijt
    
    è§£é‡Š: æ­¤æŒ‡æ ‡è¡¡é‡ç¾å›½åœ¨è¿›å£æ—¶é¢ä¸´çš„å¸‚åœºç»“æ„é”å®šæ•ˆåº”ã€‚
    ç¾å›½åœ¨æŸäº§å“iä¸Šçš„ä¾›åº”å•†å¸‚åœºè¶Šé›†ä¸­ï¼ˆèµ«èŠ¬è¾¾å°”-èµ«å¸Œæ›¼æŒ‡æ•°HHI_itè¶Šé«˜ï¼‰ï¼Œ
    ä¸”å½“å‰ä¼™ä¼´jçš„è´¸æ˜“ä»½é¢(share_ijt)è¶Šé«˜ï¼Œæ„å‘³ç€æ›¿æ¢è¯¥ä¾›åº”å•†çš„éš¾åº¦è¶Šå¤§ï¼Œ
    å› æ­¤å…¶è¿›å£é”å®šåŠ›è¶Šå¼ºã€‚
    
    Args:
        df: åŒ…å«ç¾å›½è´¸æ˜“æ•°æ®çš„DataFrame
        å¿…é¡»åŒ…å«åˆ—: year, us_partner, energy_product, trade_value_usd, us_role
        
    Returns:
        æ·»åŠ äº†market_locking_poweråˆ—çš„DataFrameï¼ˆåªè®¡ç®—è¿›å£éƒ¨åˆ†ï¼‰
        
    è®¡ç®—æ­¥éª¤:
        1. ç­›é€‰ç¾å›½è¿›å£æ•°æ®
        2. æŒ‰å¹´ä»½å’Œäº§å“åˆ†ç»„è®¡ç®—ä¾›åº”å•†HHI
        3. è®¡ç®—æ¯ä¸ªä¾›åº”å•†åœ¨æ¯ç§äº§å“ä¸Šçš„å¸‚åœºä»½é¢
        4. è¿›å£é”å®šåŠ› = ä¾›åº”å•†HHI Ã— ä¾›åº”å•†ä»½é¢
    """
    
    logger.info("ğŸ“¥ å¼€å§‹è®¡ç®—è¿›å£é”å®šåŠ›æŒ‡æ ‡...")
    
    df_locking = df.copy()
    
    # åªå¤„ç†ç¾å›½ä½œä¸ºè¿›å£æ–¹çš„æ•°æ®
    import_data = df_locking[df_locking['us_role'] == 'importer'].copy()
    
    if len(import_data) == 0:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°ç¾å›½è¿›å£æ•°æ®ï¼Œè¿”å›åŸæ•°æ®")
        return df_locking
    
    locking_results = []
    
    # æŒ‰å¹´ä»½å’Œäº§å“è®¡ç®—HHIå’Œå¸‚åœºä»½é¢
    for year in import_data['year'].unique():
        year_data = import_data[import_data['year'] == year]
        
        for product in year_data['energy_product'].unique():
            product_data = year_data[year_data['energy_product'] == product]
            
            # è®¡ç®—æ€»è¿›å£é¢
            total_import = product_data['trade_value_usd'].sum()
            
            if total_import > 0:
                # è®¡ç®—æ¯ä¸ªä¾›åº”å•†çš„å¸‚åœºä»½é¢
                supplier_shares = product_data.groupby('us_partner')['trade_value_usd'].sum() / total_import
                
                # è®¡ç®—ä¾›åº”å•†HHI (Herfindahl-Hirschman Index)
                hhi = (supplier_shares ** 2).sum()
                
                # ä¸ºæ¯ä¸ªä¾›åº”å•†è®¡ç®—è¿›å£é”å®šåŠ›
                for partner, share in supplier_shares.items():
                    import_locking_power = hhi * share
                    
                    locking_results.append({
                        'year': year,
                        'us_partner': partner,
                        'energy_product': product,
                        'us_role': 'importer',
                        'market_locking_power': import_locking_power,
                        'supplier_share': share,
                        'supplier_hhi': hhi,
                        'total_suppliers': len(supplier_shares)
                    })
    
    # è½¬æ¢ä¸ºDataFrame
    locking_df = pd.DataFrame(locking_results)
    
    # ä¸åŸæ•°æ®åˆå¹¶ï¼ˆä¿æŒè¿›å£æ•°æ®ï¼Œå‡ºå£æ•°æ®ç¨åå•ç‹¬è®¡ç®—ï¼‰
    df_with_locking = pd.merge(
        df_locking, 
        locking_df[['year', 'us_partner', 'energy_product', 'us_role', 'market_locking_power']], 
        on=['year', 'us_partner', 'energy_product', 'us_role'], 
        how='left'
    )
    
    # å¡«å……ç¼ºå¤±å€¼ä¸º0
    df_with_locking['market_locking_power'] = df_with_locking['market_locking_power'].fillna(0)
    
    # æ•°æ®éªŒè¯
    assert df_with_locking['market_locking_power'].isnull().sum() == 0, "å¸‚åœºé”å®šåŠ›æŒ‡æ ‡è®¡ç®—ä¸­å­˜åœ¨ç¼ºå¤±å€¼"
    assert (df_with_locking['market_locking_power'] >= 0).all(), "å¸‚åœºé”å®šåŠ›æŒ‡æ ‡å€¼ä¸èƒ½ä¸ºè´Ÿ"
    
    # ç»Ÿè®¡æ‘˜è¦
    logger.info(f"ğŸ“Š å¸‚åœºé”å®šåŠ›ç»Ÿè®¡:")
    logger.info(f"  å¹³å‡é”å®šåŠ›: {df_with_locking['market_locking_power'].mean():.4f}")
    logger.info(f"  ä¸­ä½æ•°é”å®šåŠ›: {df_with_locking['market_locking_power'].median():.4f}")
    logger.info(f"  æœ€é«˜é”å®šåŠ›: {df_with_locking['market_locking_power'].max():.4f}")
    logger.info(f"  æœ€ä½é”å®šåŠ›: {df_with_locking['market_locking_power'].min():.4f}")
    logger.info(f"  éé›¶é”å®šåŠ›è®°å½•: {(df_with_locking['market_locking_power'] > 0).sum()} æ¡")
    
    # æŒ‰äº§å“åˆ†æé”å®šåŠ›ï¼ˆä»…è¿›å£æ•°æ®ï¼‰
    if len(locking_df) > 0:
        product_locking = locking_df.groupby('energy_product').agg({
            'market_locking_power': ['mean', 'max'],
            'supplier_hhi': 'mean',
            'total_suppliers': 'mean'
        }).round(4)
        
        logger.info(f"  æŒ‰èƒ½æºäº§å“çš„å¸‚åœºé›†ä¸­åº¦:")
        for product in product_locking.index:
            stats = product_locking.loc[product]
            logger.info(f"    {product}: å¹³å‡HHI={stats[('supplier_hhi', 'mean')]:.4f}, " +
                       f"å¹³å‡é”å®šåŠ›={stats[('market_locking_power', 'mean')]:.4f}, " +
                       f"å¹³å‡ä¾›åº”å•†æ•°={stats[('total_suppliers', 'mean')]:.1f}")
    
    logger.info("âœ… è¿›å£é”å®šåŠ›æŒ‡æ ‡è®¡ç®—å®Œæˆ!")
    return df_with_locking


def calculate_export_locking_power(df: pd.DataFrame, global_trade_data: Dict[int, pd.DataFrame]) -> pd.DataFrame:
    """
    è®¡ç®—å‡ºå£é”å®šåŠ›æŒ‡æ ‡ (Export Locking Power) - é•œåƒè®¡ç®—é€»è¾‘
    
    ç†è®ºæ¡†æ¶ï¼šå½“ç¾å›½å‘æŸå›½å‡ºå£èƒ½æºæ—¶ï¼Œè¯„ä¼°è¯¥å›½å¯¹ç¾å›½çš„"è¢«é”å®š"ç¨‹åº¦
    
    è®¡ç®—é€»è¾‘ï¼š
    1. å¯¹äºç¾å›½å‘å›½å®¶Xå‡ºå£äº§å“Pçš„æ¯ä¸€æ¡è®°å½•
    2. æŸ¥è¯¢å…¨çƒæ•°æ®ï¼Œæ‰¾åˆ°å›½å®¶Xåœ¨è¯¥å¹´ä»½è¿›å£äº§å“Pçš„æ‰€æœ‰ä¾›åº”å•†
    3. è®¡ç®—å›½å®¶Xåœ¨äº§å“Pä¸Šçš„è¿›å£é›†ä¸­åº¦ï¼ˆä¾›åº”å•†HHIï¼‰
    4. è®¡ç®—ç¾å›½åœ¨å›½å®¶Xçš„äº§å“Pè¿›å£ä¸­çš„ä»½é¢
    5. å‡ºå£é”å®šåŠ› = å›½å®¶Xçš„è¿›å£HHI Ã— ç¾å›½åœ¨Xå›½å¸‚åœºçš„ä»½é¢
    
    Args:
        df: åŒ…å«ç¾å›½è´¸æ˜“æ•°æ®çš„DataFrame
        global_trade_data: å…¨çƒè´¸æ˜“æ•°æ®å­—å…¸ï¼Œæ ¼å¼{year: DataFrame}
        
    Returns:
        æ·»åŠ äº†market_locking_poweråˆ—çš„DataFrameï¼ˆåªè®¡ç®—å‡ºå£éƒ¨åˆ†ï¼‰
    """
    
    logger.info("ğŸ“¤ å¼€å§‹è®¡ç®—å‡ºå£é”å®šåŠ›æŒ‡æ ‡ï¼ˆé•œåƒé€»è¾‘ï¼‰...")
    
    df_locking = df.copy()
    
    # åªå¤„ç†ç¾å›½ä½œä¸ºå‡ºå£æ–¹çš„æ•°æ®
    export_data = df_locking[df_locking['us_role'] == 'exporter'].copy()
    
    if len(export_data) == 0:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°ç¾å›½å‡ºå£æ•°æ®ï¼Œè¿”å›åŸæ•°æ®")
        return df_locking
    
    if not global_trade_data:
        logger.warning("æœªæä¾›å…¨çƒè´¸æ˜“æ•°æ®ï¼Œå‡ºå£é”å®šåŠ›å°†è®¾ä¸º0")
        df_locking.loc[df_locking['us_role'] == 'exporter', 'market_locking_power'] = 0
        return df_locking
    
    locking_results = []
    
    # ä¸ºæ¯ä¸ªç¾å›½å‡ºå£è®°å½•è®¡ç®—å¯¹åº”çš„å‡ºå£é”å®šåŠ›
    for idx, row in export_data.iterrows():
        year = row['year']
        partner_country = row['us_partner']  # ç¾å›½çš„å‡ºå£ç›®æ ‡å›½
        product = row['energy_product']
        us_export_value = row['trade_value_usd']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥å¹´ä»½çš„å…¨çƒæ•°æ®
        if year not in global_trade_data:
            logger.debug(f"ç¼ºå°‘{year}å¹´å…¨çƒæ•°æ®ï¼Œè·³è¿‡")
            continue
        
        global_year_data = global_trade_data[year]
        
        # æŸ¥æ‰¾ç›®æ ‡å›½åœ¨è¯¥å¹´ä»½ã€è¯¥äº§å“ä¸Šçš„æ‰€æœ‰è¿›å£è®°å½•
        # æ³¨æ„ï¼šåœ¨å…¨çƒæ•°æ®ä¸­ï¼Œç›®æ ‡å›½ä½œä¸ºreporterï¼Œæµå‘ä¸ºM(Import)
        partner_imports = global_year_data[
            (global_year_data['reporter'] == partner_country) & 
            (global_year_data['flow'] == 'M') & 
            (global_year_data['energy_product'] == product)
        ].copy()
        
        if len(partner_imports) == 0:
            # ç›®æ ‡å›½åœ¨è¯¥äº§å“ä¸Šæ²¡æœ‰è¿›å£è®°å½•ï¼Œé”å®šåŠ›ä¸º0
            locking_results.append({
                'year': year,
                'us_partner': partner_country,
                'energy_product': product,
                'us_role': 'exporter',
                'market_locking_power': 0,
                'target_import_hhi': 0,
                'us_share_in_target': 0,
                'target_total_suppliers': 0,
                'target_total_imports': 0
            })
            continue
        
        # è®¡ç®—ç›®æ ‡å›½çš„æ€»è¿›å£é¢
        total_imports = partner_imports['trade_value_usd'].sum()
        
        if total_imports <= 0:
            locking_results.append({
                'year': year,
                'us_partner': partner_country,
                'energy_product': product,
                'us_role': 'exporter',
                'market_locking_power': 0,
                'target_import_hhi': 0,
                'us_share_in_target': 0,
                'target_total_suppliers': 0,
                'target_total_imports': 0
            })
            continue
        
        # è®¡ç®—ç›®æ ‡å›½å„ä¾›åº”å•†çš„å¸‚åœºä»½é¢
        supplier_shares = partner_imports.groupby('partner')['trade_value_usd'].sum() / total_imports
        
        # è®¡ç®—ç›®æ ‡å›½çš„è¿›å£é›†ä¸­åº¦ï¼ˆä¾›åº”å•†HHIï¼‰
        import_hhi = (supplier_shares ** 2).sum()
        
        # è®¡ç®—ç¾å›½åœ¨ç›®æ ‡å›½å¸‚åœºä¸­çš„ä»½é¢
        us_share = supplier_shares.get('USA', 0)  # å¦‚æœç¾å›½ä¸åœ¨ä¾›åº”å•†åˆ—è¡¨ä¸­ï¼Œä»½é¢ä¸º0
        
        # è®¡ç®—å‡ºå£é”å®šåŠ›ï¼šç›®æ ‡å›½è¿›å£HHI Ã— ç¾å›½åœ¨ç›®æ ‡å›½å¸‚åœºçš„ä»½é¢
        export_locking_power = import_hhi * us_share
        
        locking_results.append({
            'year': year,
            'us_partner': partner_country,
            'energy_product': product,
            'us_role': 'exporter',
            'market_locking_power': export_locking_power,
            'target_import_hhi': import_hhi,
            'us_share_in_target': us_share,
            'target_total_suppliers': len(supplier_shares),
            'target_total_imports': total_imports
        })
    
    # è½¬æ¢ä¸ºDataFrame
    locking_df = pd.DataFrame(locking_results)
    
    # ä¸åŸæ•°æ®åˆå¹¶
    df_with_locking = pd.merge(
        df_locking, 
        locking_df[['year', 'us_partner', 'energy_product', 'us_role', 'market_locking_power']], 
        on=['year', 'us_partner', 'energy_product', 'us_role'], 
        how='left'
    )
    
    # å¡«å……ç¼ºå¤±å€¼ä¸º0
    df_with_locking['market_locking_power'] = df_with_locking['market_locking_power'].fillna(0)
    
    # ç»Ÿè®¡æ‘˜è¦
    if len(locking_df) > 0:
        logger.info(f"ğŸ“Š å‡ºå£é”å®šåŠ›ç»Ÿè®¡:")
        logger.info(f"  å¹³å‡é”å®šåŠ›: {locking_df['market_locking_power'].mean():.4f}")
        logger.info(f"  æœ€é«˜é”å®šåŠ›: {locking_df['market_locking_power'].max():.4f}")
        logger.info(f"  éé›¶é”å®šåŠ›è®°å½•: {(locking_df['market_locking_power'] > 0).sum()} æ¡")
        logger.info(f"  ç¾å›½åœ¨ç›®æ ‡å¸‚åœºå¹³å‡ä»½é¢: {locking_df['us_share_in_target'].mean():.4f}")
        logger.info(f"  ç›®æ ‡å›½å¹³å‡ä¾›åº”å•†æ•°: {locking_df['target_total_suppliers'].mean():.1f}")
        
        # æŒ‰äº§å“åˆ†æ
        product_stats = locking_df.groupby('energy_product').agg({
            'market_locking_power': ['mean', 'max'],
            'target_import_hhi': 'mean',
            'us_share_in_target': 'mean'
        }).round(4)
        
        logger.info(f"  æŒ‰èƒ½æºäº§å“çš„å‡ºå£é”å®šåŠ›:")
        for product in product_stats.index:
            stats = product_stats.loc[product]
            logger.info(f"    {product}: å¹³å‡é”å®šåŠ›={stats[('market_locking_power', 'mean')]:.4f}, " +
                       f"ç›®æ ‡å›½å¹³å‡HHI={stats[('target_import_hhi', 'mean')]:.4f}")
    
    logger.info("âœ… å‡ºå£é”å®šåŠ›æŒ‡æ ‡è®¡ç®—å®Œæˆ!")
    return df_with_locking


def calculate_dli_composite(df: pd.DataFrame, 
                           use_pca: bool = True, 
                           custom_weights: Dict[str, float] = None) -> pd.DataFrame:
    """
    è®¡ç®—DLIç»¼åˆæŒ‡æ ‡
    
    æ­¥éª¤:
    1. æ ‡å‡†åŒ–å››ä¸ªç»´åº¦çš„åˆ†å€¼ï¼ˆå‡å€¼=0ï¼Œæ ‡å‡†å·®=1ï¼‰
    2. ä½¿ç”¨PCAç¡®å®šæƒé‡ï¼ˆç¬¬ä¸€ä¸»æˆåˆ†çš„è½½è·ä½œä¸ºæƒé‡ï¼‰
    3. è®¡ç®—åŠ æƒç»¼åˆå¾—åˆ†
    
    Args:
        df: åŒ…å«å››ä¸ªDLIç»´åº¦çš„DataFrame
        use_pca: æ˜¯å¦ä½¿ç”¨PCAç¡®å®šæƒé‡ï¼ŒFalseåˆ™ä½¿ç”¨ç­‰æƒé‡æˆ–è‡ªå®šä¹‰æƒé‡
        custom_weights: è‡ªå®šä¹‰æƒé‡å­—å…¸ï¼Œå¦‚ {'continuity': 0.3, 'infrastructure': 0.3, ...}
        
    Returns:
        æ·»åŠ äº†dli_compositeåˆ—çš„DataFrame
        
    DLIå…¬å¼:
        DLI_ijt = w1 * Continuity_ijt + w2 * Infrastructure_ijt + w3 * Stability_ijt + w4 * Market_Locking_Power_ijt
        æƒé‡ç”±PCAç¬¬ä¸€ä¸»æˆåˆ†ç¡®å®š
    """
    
    logger.info("ğŸ¯ å¼€å§‹è®¡ç®—DLIç»¼åˆæŒ‡æ ‡...")
    
    df_composite = df.copy()
    
    # æ£€æŸ¥å¿…éœ€çš„åˆ—
    required_columns = ['continuity', 'infrastructure', 'stability', 'market_locking_power']
    missing_columns = [col for col in required_columns if col not in df_composite.columns]
    if missing_columns:
        raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„DLIç»´åº¦åˆ—: {missing_columns}")
    
    # æå–å››ä¸ªç»´åº¦çš„æ•°æ®
    dli_dimensions = df_composite[required_columns].copy()
    
    # æ•°æ®éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼æˆ–å¼‚å¸¸å€¼
    logger.info("ğŸ” DLIç»´åº¦æ•°æ®è´¨é‡æ£€æŸ¥:")
    for col in required_columns:
        missing_count = dli_dimensions[col].isnull().sum()
        if missing_count > 0:
            logger.warning(f"  {col}: {missing_count} ä¸ªç¼ºå¤±å€¼")
        
        # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡
        stats = dli_dimensions[col].describe()
        logger.info(f"  {col}: å‡å€¼={stats['mean']:.4f}, æ ‡å‡†å·®={stats['std']:.4f}, " + 
                   f"èŒƒå›´=[{stats['min']:.4f}, {stats['max']:.4f}]")
    
    # å¤„ç†ç¼ºå¤±å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
    if dli_dimensions.isnull().any().any():
        logger.warning("å‘ç°ç¼ºå¤±å€¼ï¼Œå°†ä½¿ç”¨å„åˆ—å‡å€¼å¡«å……")
        dli_dimensions = dli_dimensions.fillna(dli_dimensions.mean())
    
    # ç¬¬1æ­¥ï¼šæ ‡å‡†åŒ–å¤„ç†
    logger.info("ğŸ“Š æ‰§è¡Œæ ‡å‡†åŒ–å¤„ç†...")
    scaler = StandardScaler()
    dli_standardized = scaler.fit_transform(dli_dimensions)
    dli_std_df = pd.DataFrame(dli_standardized, columns=required_columns, index=dli_dimensions.index)
    
    # æ˜¾ç¤ºæ ‡å‡†åŒ–åçš„ç»Ÿè®¡
    logger.info("æ ‡å‡†åŒ–åçš„æ•°æ®ç»Ÿè®¡:")
    for col in required_columns:
        mean_val = dli_std_df[col].mean()
        std_val = dli_std_df[col].std()
        logger.info(f"  {col}: å‡å€¼={mean_val:.6f}, æ ‡å‡†å·®={std_val:.6f}")
    
    # ç¬¬2æ­¥ï¼šç¡®å®šæƒé‡
    if use_pca and custom_weights is None:
        logger.info("ğŸ”¬ ä½¿ç”¨PCAç¡®å®šæƒé‡...")
        
        # æ‰§è¡Œä¸»æˆåˆ†åˆ†æ
        pca = PCA(n_components=4)
        pca_result = pca.fit_transform(dli_std_df)
        
        # è·å–ç¬¬ä¸€ä¸»æˆåˆ†çš„è½½è·ä½œä¸ºæƒé‡
        pc1_loadings = pca.components_[0]
        
        # ç¡®ä¿æƒé‡ä¸ºæ­£å€¼ï¼ˆå–ç»å¯¹å€¼ï¼‰å¹¶å½’ä¸€åŒ–
        weights = np.abs(pc1_loadings)
        weights = weights / weights.sum()
        
        weight_dict = dict(zip(required_columns, weights))
        
        # æ˜¾ç¤ºPCAç»“æœ
        logger.info(f"ğŸ“ˆ PCAåˆ†æç»“æœ:")
        logger.info(f"  ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_[0]:.3f}")
        logger.info(f"  ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_[:2].sum():.3f} (å‰ä¸¤ä¸ªä¸»æˆåˆ†)")
        logger.info(f"  PCAç¡®å®šçš„æƒé‡:")
        for dim, weight in weight_dict.items():
            logger.info(f"    {dim}: {weight:.4f}")
            
    elif custom_weights is not None:
        logger.info("âš™ï¸ ä½¿ç”¨è‡ªå®šä¹‰æƒé‡...")
        
        # éªŒè¯è‡ªå®šä¹‰æƒé‡
        if set(custom_weights.keys()) != set(required_columns):
            raise ValueError(f"è‡ªå®šä¹‰æƒé‡å¿…é¡»åŒ…å«æ‰€æœ‰ç»´åº¦: {required_columns}")
        
        if abs(sum(custom_weights.values()) - 1.0) > 1e-6:
            logger.warning("è‡ªå®šä¹‰æƒé‡ä¹‹å’Œä¸ä¸º1ï¼Œå°†è¿›è¡Œå½’ä¸€åŒ–")
            total_weight = sum(custom_weights.values())
            custom_weights = {k: v/total_weight for k, v in custom_weights.items()}
        
        weight_dict = custom_weights
        logger.info(f"  è‡ªå®šä¹‰æƒé‡:")
        for dim, weight in weight_dict.items():
            logger.info(f"    {dim}: {weight:.4f}")
    
    else:
        logger.info("âš–ï¸ ä½¿ç”¨ç­‰æƒé‡...")
        weight_dict = {dim: 0.25 for dim in required_columns}
        logger.info(f"  ç­‰æƒé‡: æ¯ä¸ªç»´åº¦æƒé‡ = 0.25")
    
    # ç¬¬3æ­¥ï¼šè®¡ç®—åŠ æƒç»¼åˆå¾—åˆ†
    logger.info("ğŸ§® è®¡ç®—DLIç»¼åˆå¾—åˆ†...")
    
    dli_composite_score = np.zeros(len(dli_std_df))
    
    for dim, weight in weight_dict.items():
        dli_composite_score += weight * dli_std_df[dim].values
    
    # æ·»åŠ åˆ°åŸDataFrame
    df_composite['dli_composite'] = dli_composite_score
    
    # ä¸ºäº†ä¾¿äºè§£é‡Šï¼Œå°†DLIç»¼åˆå¾—åˆ†è½¬æ¢ä¸ºæ­£å€¼ï¼ˆæœ€å°å€¼æ˜ å°„ä¸º0ï¼‰
    min_dli = df_composite['dli_composite'].min()
    if min_dli < 0:
        df_composite['dli_composite_adjusted'] = df_composite['dli_composite'] - min_dli
        logger.info(f"å°†DLIç»¼åˆå¾—åˆ†è°ƒæ•´ä¸ºéè´Ÿå€¼ (æœ€å°å€¼è°ƒæ•´: {min_dli:.4f})")
    else:
        df_composite['dli_composite_adjusted'] = df_composite['dli_composite']
    
    # åŒæ—¶ä¿å­˜æ ‡å‡†åŒ–åçš„å„ç»´åº¦åˆ†å€¼å’Œæƒé‡ä¿¡æ¯
    for dim in required_columns:
        df_composite[f'{dim}_standardized'] = dli_std_df[dim]
    
    # å°†æƒé‡ä¿¡æ¯ä¿å­˜ä¸ºå±æ€§ï¼ˆç”¨äºåç»­åˆ†æï¼‰
    df_composite.attrs = {
        'dli_weights': weight_dict,
        'pca_explained_variance': pca.explained_variance_ratio_[0] if use_pca else None,
        'standardization_params': {
            'mean': scaler.mean_.tolist(),
            'scale': scaler.scale_.tolist()
        }
    }
    
    # ç»Ÿè®¡æ‘˜è¦
    logger.info(f"ğŸ“Š DLIç»¼åˆæŒ‡æ ‡ç»Ÿè®¡:")
    logger.info(f"  åŸå§‹DLIå¾—åˆ†:")
    logger.info(f"    å‡å€¼: {df_composite['dli_composite'].mean():.4f}")
    logger.info(f"    æ ‡å‡†å·®: {df_composite['dli_composite'].std():.4f}")
    logger.info(f"    èŒƒå›´: [{df_composite['dli_composite'].min():.4f}, {df_composite['dli_composite'].max():.4f}]")
    
    if 'dli_composite_adjusted' in df_composite.columns:
        logger.info(f"  è°ƒæ•´åDLIå¾—åˆ†:")
        logger.info(f"    å‡å€¼: {df_composite['dli_composite_adjusted'].mean():.4f}")
        logger.info(f"    æ ‡å‡†å·®: {df_composite['dli_composite_adjusted'].std():.4f}")
        logger.info(f"    èŒƒå›´: [{df_composite['dli_composite_adjusted'].min():.4f}, {df_composite['dli_composite_adjusted'].max():.4f}]")
    
    # æ˜¾ç¤ºå„ç»´åº¦ä¸ç»¼åˆå¾—åˆ†çš„ç›¸å…³æ€§
    logger.info(f"  å„ç»´åº¦ä¸DLIç»¼åˆå¾—åˆ†çš„ç›¸å…³æ€§:")
    for dim in required_columns:
        corr = df_composite[dim].corr(df_composite['dli_composite'])
        logger.info(f"    {dim}: {corr:.4f}")
    
    logger.info("âœ… DLIç»¼åˆæŒ‡æ ‡è®¡ç®—å®Œæˆ!")
    return df_composite

def calculate_dli_composite_unified(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä½¿ç”¨ç»Ÿä¸€æƒé‡è®¡ç®—åŒå‘DLIç»¼åˆæŒ‡æ ‡
    
    å…³é”®ç‰¹ç‚¹ï¼š
    1. ä½¿ç”¨åŒ…å«è¿›å£å’Œå‡ºå£çš„å®Œæ•´æ•°æ®é›†æ¥è¿è¡ŒPCA
    2. ç¡®ä¿æ‰€æœ‰dli_scoreéƒ½åœ¨åŒä¸€æ ‡å°ºä¸‹å¯æ¯”
    3. æƒé‡ç»Ÿä¸€æ€§åŸåˆ™çš„å…·ä½“å®ç°
    
    Args:
        df: åŒ…å«å››ä¸ªDLIç»´åº¦çš„å®Œæ•´DataFrameï¼ˆè¿›å£+å‡ºå£ï¼‰
        
    Returns:
        æ·»åŠ äº†ç»Ÿä¸€æ ‡å°ºdli_scoreåˆ—çš„DataFrame
    """
    
    logger.info("ğŸ¯ è®¡ç®—ç»Ÿä¸€æ ‡å°ºDLIç»¼åˆæŒ‡æ ‡...")
    
    df_unified = df.copy()
    
    # æ£€æŸ¥å¿…éœ€çš„å››ä¸ªç»´åº¦
    required_dimensions = ['continuity', 'infrastructure', 'stability', 'market_locking_power']
    missing_dimensions = [dim for dim in required_dimensions if dim not in df_unified.columns]
    if missing_dimensions:
        raise ValueError(f"ç¼ºå°‘DLIç»´åº¦: {missing_dimensions}")
    
    logger.info("ğŸ” åŒå‘DLIç»´åº¦æ•°æ®è´¨é‡æ£€æŸ¥:")
    for dim in required_dimensions:
        logger.info(f"  {dim}: å‡å€¼={df_unified[dim].mean():.4f}, æ ‡å‡†å·®={df_unified[dim].std():.4f}, " + 
                   f"èŒƒå›´=[{df_unified[dim].min():.4f}, {df_unified[dim].max():.4f}]")
    
    # æ ‡å‡†åŒ–å¤„ç†ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
    logger.info("ğŸ“Š å¯¹å®Œæ•´åŒå‘æ•°æ®é›†æ‰§è¡Œæ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    standardized_dimensions = scaler.fit_transform(df_unified[required_dimensions])
    
    standardized_df = pd.DataFrame(standardized_dimensions, columns=required_dimensions, index=df_unified.index)
    
    # éªŒè¯æ ‡å‡†åŒ–æ•ˆæœ
    logger.info("æ ‡å‡†åŒ–åçš„æ•°æ®ç»Ÿè®¡:")
    for dim in required_dimensions:
        logger.info(f"  {dim}: å‡å€¼={standardized_df[dim].mean():.6f}, æ ‡å‡†å·®={standardized_df[dim].std():.6f}")
    
    # ä½¿ç”¨PCAç¡®å®šç»Ÿä¸€æƒé‡
    logger.info("ğŸ”¬ ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿è¡ŒPCAç¡®å®šç»Ÿä¸€æƒé‡...")
    pca = PCA(n_components=4)
    pca_result = pca.fit_transform(standardized_dimensions)
    
    # è·å–ç¬¬ä¸€ä¸»æˆåˆ†çš„è½½è·ä½œä¸ºæƒé‡
    first_component = pca.components_[0]
    weights_raw = np.abs(first_component)  # å–ç»å¯¹å€¼
    weights_normalized = weights_raw / weights_raw.sum()  # å½’ä¸€åŒ–
    
    # åˆ›å»ºæƒé‡å­—å…¸
    weights_dict = dict(zip(required_dimensions, weights_normalized))
    
    logger.info("ğŸ“ˆ ç»Ÿä¸€PCAæƒé‡ç»“æœ:")
    logger.info(f"  ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_[0]:.3f}")
    logger.info(f"  ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_[:2].sum():.3f} (å‰ä¸¤ä¸ªä¸»æˆåˆ†)")
    logger.info("  ç»Ÿä¸€æƒé‡åˆ†é…:")
    for dim, weight in weights_dict.items():
        logger.info(f"    {dim}: {weight:.4f}")
    
    # è®¡ç®—ç»Ÿä¸€æ ‡å°ºä¸‹çš„DLIç»¼åˆå¾—åˆ†
    logger.info("ğŸ§® è®¡ç®—ç»Ÿä¸€æ ‡å°ºDLIç»¼åˆå¾—åˆ†...")
    dli_scores = []
    for idx in df_unified.index:
        score = sum(standardized_df.loc[idx, dim] * weights_dict[dim] for dim in required_dimensions)
        dli_scores.append(score)
    
    df_unified['dli_score'] = dli_scores
    
    # è°ƒæ•´ä¸ºéè´Ÿå€¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
    min_score = df_unified['dli_score'].min()
    if min_score < 0:
        df_unified['dli_score_adjusted'] = df_unified['dli_score'] - min_score
        logger.info(f"å°†DLIå¾—åˆ†è°ƒæ•´ä¸ºéè´Ÿå€¼ (æœ€å°å€¼è°ƒæ•´: {min_score:.4f})")
    else:
        df_unified['dli_score_adjusted'] = df_unified['dli_score']
    
    # ç»Ÿè®¡æ‘˜è¦
    logger.info(f"ğŸ“Š ç»Ÿä¸€DLIç»¼åˆæŒ‡æ ‡ç»Ÿè®¡:")
    logger.info(f"  åŸå§‹DLIå¾—åˆ†:")
    logger.info(f"    å‡å€¼: {df_unified['dli_score'].mean():.4f}")
    logger.info(f"    æ ‡å‡†å·®: {df_unified['dli_score'].std():.4f}")
    logger.info(f"    èŒƒå›´: [{df_unified['dli_score'].min():.4f}, {df_unified['dli_score'].max():.4f}]")
    
    if 'dli_score_adjusted' in df_unified.columns:
        logger.info(f"  è°ƒæ•´åDLIå¾—åˆ†:")
        logger.info(f"    å‡å€¼: {df_unified['dli_score_adjusted'].mean():.4f}")
        logger.info(f"    æ ‡å‡†å·®: {df_unified['dli_score_adjusted'].std():.4f}")
        logger.info(f"    èŒƒå›´: [{df_unified['dli_score_adjusted'].min():.4f}, {df_unified['dli_score_adjusted'].max():.4f}]")
    
    # æŒ‰é”å®šç±»å‹åˆ†æ
    if 'locking_dimension_type' in df_unified.columns:
        type_stats = df_unified.groupby('locking_dimension_type')['dli_score'].agg(['count', 'mean', 'std']).round(4)
        logger.info("  æŒ‰é”å®šç±»å‹ç»Ÿè®¡:")
        for locking_type, stats in type_stats.iterrows():
            logger.info(f"    {locking_type}: {stats['count']} æ¡è®°å½•, å‡å€¼={stats['mean']:.4f}, æ ‡å‡†å·®={stats['std']:.4f}")
    
    # ä¿å­˜æƒé‡ä¿¡æ¯ç”¨äºåç»­åˆ†æ
    df_unified._pca_weights = weights_dict
    df_unified._pca_explained_variance = pca.explained_variance_ratio_[0]
    
    logger.info("âœ… ç»Ÿä¸€DLIç»¼åˆæŒ‡æ ‡è®¡ç®—å®Œæˆ!")
    return df_unified


def generate_dli_panel_data_v2(trade_data: pd.DataFrame = None, 
                              data_file_path: str = None,
                              output_path: str = None,
                              enable_global_data: bool = True) -> pd.DataFrame:
    """
    ç”ŸæˆåŒå‘DLIé¢æ¿æ•°æ®é›† (Version 2.0)
    
    è¿™æ˜¯å‡çº§ç‰ˆçš„DLIè®¡ç®—æ¨¡å—ä¸»è¦æ¥å£ï¼Œæ”¯æŒåŒå‘é”å®šåˆ†æï¼š
    - è¿›å£é”å®š (Import Locking): ç¾å›½è¢«ä¾›åº”å•†é”å®šçš„ç¨‹åº¦
    - å‡ºå£é”å®š (Export Locking): ç¾å›½é”å®šå…¶ä»–å›½å®¶çš„ç¨‹åº¦
    
    Args:
        trade_data: é¢„å¤„ç†çš„ç¾å›½è´¸æ˜“æ•°æ®DataFrameï¼Œå¦‚æœä¸ºNoneåˆ™ä»æ–‡ä»¶åŠ è½½
        data_file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¿å­˜åˆ°outputsç›®å½•
        enable_global_data: æ˜¯å¦åŠ è½½å…¨å±€æ•°æ®ä»¥è®¡ç®—å‡ºå£é”å®šåŠ›ï¼Œé»˜è®¤True
        
    Returns:
        åŒ…å«åŒå‘DLIæŒ‡æ ‡çš„é¢æ¿æ•°æ®DataFrameï¼Œå¢åŠ locking_dimension_typeåˆ—
        
    è¾“å‡ºåˆ—åŒ…æ‹¬ï¼š
        - åŸºç¡€æ•°æ®ï¼šyear, us_partner, energy_product, trade_value_usd, distance_kmç­‰
        - é”å®šç»´åº¦ç±»å‹ï¼šlocking_dimension_type ('import_locking' æˆ– 'export_locking')
        - DLIå››ç»´åº¦ï¼šcontinuity, infrastructure, stability, market_locking_power  
        - ç»Ÿä¸€æ ‡å°ºç»¼åˆæŒ‡æ ‡ï¼šdli_score (ä½¿ç”¨ç»Ÿä¸€PCAæƒé‡)
    """
    
    logger.info("ğŸš€ å¼€å§‹ç”ŸæˆåŒå‘DLIé¢æ¿æ•°æ® (v2.0)...")
    
    # ç¬¬1æ­¥ï¼šåŠ è½½ç¾å›½è´¸æ˜“æ•°æ®
    if trade_data is not None:
        df = trade_data.copy()
        logger.info(f"ä½¿ç”¨æä¾›çš„è´¸æ˜“æ•°æ®: {len(df)} æ¡è®°å½•")
    else:
        if data_file_path is None:
            base_dir = Path(__file__).parent.parent.parent
            data_file_path = base_dir / "outputs" / "tables" / "us_trade_prepared_for_dli.csv"
        
        if not Path(data_file_path).exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file_path}")
        
        df = pd.read_csv(data_file_path)
        logger.info(f"ä»æ–‡ä»¶åŠ è½½è´¸æ˜“æ•°æ®: {data_file_path}, {len(df)} æ¡è®°å½•")
    
    # æ•°æ®éªŒè¯
    required_base_columns = ['year', 'us_partner', 'energy_product', 'trade_value_usd', 'distance_km', 'us_role']
    missing_columns = [col for col in required_base_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
    
    # ç¬¬2æ­¥ï¼šåŠ è½½å…¨å±€è´¸æ˜“æ•°æ®ï¼ˆç”¨äºè®¡ç®—å‡ºå£é”å®šåŠ›ï¼‰
    global_trade_data = {}
    if enable_global_data:
        try:
            logger.info("ğŸŒ åŠ è½½å…¨çƒè´¸æ˜“æ•°æ®ä»¥æ”¯æŒå‡ºå£é”å®šåŠ›è®¡ç®—...")
            years_needed = sorted(df['year'].unique())
            global_trade_data = load_global_trade_data_range(
                start_year=min(years_needed), 
                end_year=max(years_needed)
            )
            logger.info(f"âœ… æˆåŠŸåŠ è½½{len(global_trade_data)}å¹´å…¨çƒæ•°æ®")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å…¨å±€æ•°æ®å¤±è´¥: {e}")
            logger.warning("å°†åªè®¡ç®—è¿›å£é”å®šåŠ›ï¼Œå‡ºå£é”å®šåŠ›è®¾ä¸º0")
            global_trade_data = {}
    else:
        logger.info("âš ï¸ å…¨å±€æ•°æ®åŠ è½½å·²ç¦ç”¨ï¼Œå‡ºå£é”å®šåŠ›å°†è®¾ä¸º0")
    
    # ç¬¬3æ­¥ï¼šåˆ†åˆ«è®¡ç®—è¿›å£å’Œå‡ºå£æ•°æ®çš„å‰ä¸‰ä¸ªç»´åº¦
    logger.info("ğŸ”„ è®¡ç®—åŸºç¡€DLIç»´åº¦ï¼ˆæŒç»­æ€§ã€åŸºç¡€è®¾æ–½ã€ç¨³å®šæ€§ï¼‰...")
    
    # åŸºç¡€ä¸‰ç»´åº¦è®¡ç®—ï¼ˆè¿›å£å’Œå‡ºå£å…±äº«ï¼‰
    df = calculate_continuity(df)
    df = calculate_infrastructure(df)  
    df = calculate_stability(df)
    
    # ç¬¬4æ­¥ï¼šåˆ†åˆ«è®¡ç®—è¿›å£å’Œå‡ºå£çš„å¸‚åœºé”å®šåŠ›
    logger.info("ğŸ”’ è®¡ç®—åŒå‘å¸‚åœºé”å®šåŠ›...")
    
    # åˆ†ç¦»è¿›å£å’Œå‡ºå£æ•°æ®
    import_data = df[df['us_role'] == 'importer'].copy()
    export_data = df[df['us_role'] == 'exporter'].copy()
    
    # è®¡ç®—è¿›å£é”å®šåŠ›
    if len(import_data) > 0:
        import_data = calculate_import_locking_power(import_data)
        import_data['locking_dimension_type'] = 'import_locking'
    
    # è®¡ç®—å‡ºå£é”å®šåŠ›
    if len(export_data) > 0:
        export_data = calculate_export_locking_power(export_data, global_trade_data)
        export_data['locking_dimension_type'] = 'export_locking'
    
    # åˆå¹¶è¿›å£å’Œå‡ºå£æ•°æ®
    if len(import_data) > 0 and len(export_data) > 0:
        df_combined = pd.concat([import_data, export_data], ignore_index=True)
    elif len(import_data) > 0:
        df_combined = import_data
    elif len(export_data) > 0:
        df_combined = export_data
    else:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¿›å£æˆ–å‡ºå£æ•°æ®")
    
    logger.info(f"âœ… åŒå‘æ•°æ®åˆå¹¶å®Œæˆ: {len(df_combined)} æ¡è®°å½•")
    logger.info(f"  è¿›å£é”å®šè®°å½•: {(df_combined['locking_dimension_type'] == 'import_locking').sum()}")
    logger.info(f"  å‡ºå£é”å®šè®°å½•: {(df_combined['locking_dimension_type'] == 'export_locking').sum()}")
    
    # ç¬¬5æ­¥ï¼šä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è¿è¡ŒPCAè·å¾—ç»Ÿä¸€æƒé‡
    logger.info("ğŸ§® ä½¿ç”¨å®Œæ•´åŒå‘æ•°æ®é‡æ–°è®¡ç®—ç»Ÿä¸€PCAæƒé‡...")
    df_final = calculate_dli_composite_unified(df_combined)
    
    # ç¬¬6æ­¥ï¼šæ•°æ®æ•´ç†å’ŒéªŒè¯
    logger.info("ğŸ”§ æœ€ç»ˆæ•°æ®æ•´ç†...")
    
    # é€‰æ‹©éœ€è¦ä¿å­˜çš„åˆ—ï¼ˆé€‚ç”¨äºåŒå‘DLIåˆ†æï¼‰
    output_columns = [
        # åŸºç¡€æ ‡è¯†åˆ—
        'year', 'us_partner', 'energy_product', 'us_role', 'locking_dimension_type',
        # åŸºç¡€æ•°æ®åˆ—  
        'trade_value_usd', 'distance_km', 'distance_category',
        # DLIå››ä¸ªç»´åº¦
        'continuity', 'infrastructure', 'stability', 'market_locking_power',
        # ç»Ÿä¸€æ ‡å°ºç»¼åˆæŒ‡æ ‡
        'dli_score', 'dli_score_adjusted'
    ]
    
    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
    available_columns = [col for col in output_columns if col in df_final.columns]
    df_output = df_final[available_columns].copy()
    
    # æŒ‰å…³é”®å­—æ®µæ’åºï¼ˆåŒå‘DLIæ’åºï¼‰
    df_output = df_output.sort_values(['year', 'us_partner', 'energy_product', 'us_role', 'locking_dimension_type'])
    df_output = df_output.reset_index(drop=True)
    
    # æœ€ç»ˆæ•°æ®éªŒè¯
    logger.info("ğŸ” åŒå‘DLIæ•°æ®é›†æœ€ç»ˆéªŒè¯:")
    logger.info(f"  æ€»è®°å½•æ•°: {len(df_output):,}")
    logger.info(f"  å¹´ä»½èŒƒå›´: {df_output['year'].min()}-{df_output['year'].max()}")
    logger.info(f"  è´¸æ˜“ä¼™ä¼´æ•°: {df_output['us_partner'].nunique()}")
    logger.info(f"  èƒ½æºäº§å“æ•°: {df_output['energy_product'].nunique()}")
    
    # æŒ‰é”å®šç±»å‹ç»Ÿè®¡
    if 'locking_dimension_type' in df_output.columns:
        type_counts = df_output['locking_dimension_type'].value_counts()
        logger.info(f"  é”å®šç»´åº¦ç±»å‹åˆ†å¸ƒ:")
        for ltype, count in type_counts.items():
            logger.info(f"    {ltype}: {count:,} æ¡è®°å½• ({count/len(df_output)*100:.1f}%)")
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_summary = df_output.isnull().sum()
    if missing_summary.any():
        logger.warning("å‘ç°ç¼ºå¤±å€¼:")
        for col, count in missing_summary[missing_summary > 0].items():
            logger.warning(f"  {col}: {count} ä¸ªç¼ºå¤±å€¼")
    else:
        logger.info("âœ… æ— ç¼ºå¤±å€¼")
    
    # åŒå‘DLIæŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦
    dli_columns = ['continuity', 'infrastructure', 'stability', 'market_locking_power', 'dli_score_adjusted']
    logger.info(f"ğŸ“Š åŒå‘DLIæŒ‡æ ‡æœ€ç»ˆç»Ÿè®¡æ‘˜è¦:")
    for col in dli_columns:
        if col in df_output.columns:
            stats = df_output[col].describe()
            logger.info(f"  {col}:")
            logger.info(f"    å‡å€¼Â±æ ‡å‡†å·®: {stats['mean']:.4f}Â±{stats['std']:.4f}")
            logger.info(f"    èŒƒå›´: [{stats['min']:.4f}, {stats['max']:.4f}]")
            logger.info(f"    åˆ†ä½æ•°(25%,50%,75%): {stats['25%']:.4f}, {stats['50%']:.4f}, {stats['75%']:.4f}")
    
    # ç¬¬7æ­¥ï¼šå¯¼å‡ºæ•°æ®ï¼ˆåŒå‘DLIç‰ˆæœ¬ï¼‰
    if output_path is None:
        base_dir = Path(__file__).parent.parent.parent
        output_dir = Path(__file__).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / "dli_panel_data_v2.csv"
    
    df_output.to_csv(output_path, index=False)
    logger.info(f"ğŸ’¾ åŒå‘DLIé¢æ¿æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
    
    # ä¿å­˜ç»Ÿä¸€æƒé‡ä¿¡æ¯åˆ°jsonæ–‡ä»¶
    if hasattr(df_final, '_pca_weights'):
        import json
        weights_path = Path(output_path).parent / "dli_weights_and_params_v2.json"
        
        weights_info = {
            'version': '2.0',
            'description': 'åŒå‘DLIåˆ†æç»Ÿä¸€æƒé‡ç³»ç»Ÿ',
            'unified_pca_weights': df_final._pca_weights,
            'pca_explained_variance': df_final._pca_explained_variance,
            'generation_timestamp': pd.Timestamp.now().isoformat(),
            'data_summary': {
                'total_records': len(df_output),
                'year_range': [int(df_output['year'].min()), int(df_output['year'].max())],
                'num_partners': int(df_output['us_partner'].nunique()),
                'num_products': int(df_output['energy_product'].nunique()),
                'import_locking_records': int((df_output['locking_dimension_type'] == 'import_locking').sum()),
                'export_locking_records': int((df_output['locking_dimension_type'] == 'export_locking').sum())
            },
            'methodology_notes': {
                'pca_basis': 'ä½¿ç”¨åŒ…å«è¿›å£å’Œå‡ºå£çš„å®Œæ•´æ•°æ®é›†è¿è¡ŒPCA',
                'weight_calculation': 'ç¬¬ä¸€ä¸»æˆåˆ†è½½è·çš„ç»å¯¹å€¼å½’ä¸€åŒ–',
                'score_comparability': 'æ‰€æœ‰dli_scoreéƒ½åœ¨ç»Ÿä¸€æ ‡å°ºä¸‹å¯æ¯”',
                'locking_types': {
                    'import_locking': 'ç¾å›½è¢«ä¾›åº”å•†é”å®šçš„ç¨‹åº¦',
                    'export_locking': 'ç¾å›½é”å®šå…¶ä»–å›½å®¶çš„ç¨‹åº¦ï¼ˆé•œåƒè®¡ç®—ï¼‰'
                }
            }
        }
        
        with open(weights_path, 'w', encoding='utf-8') as f:
            json.dump(weights_info, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“„ åŒå‘DLIæƒé‡ä¿¡æ¯å·²ä¿å­˜è‡³: {weights_path}")
    
    logger.info("ğŸ‰ åŒå‘DLIé¢æ¿æ•°æ®ç”Ÿæˆå®Œæˆ!")
    return df_output

if __name__ == "__main__":
    # æµ‹è¯•åŒå‘DLIè®¡ç®—åŠŸèƒ½
    try:
        dli_panel = generate_dli_panel_data_v2()
        print(f"âœ… åŒå‘DLIé¢æ¿æ•°æ®ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“Š æ•°æ®ç»´åº¦: {dli_panel.shape}")
        print(f"ğŸ”— DLIç»¼åˆæŒ‡æ ‡èŒƒå›´: [{dli_panel['dli_score'].min():.4f}, {dli_panel['dli_score'].max():.4f}]")
        
        # æ˜¾ç¤ºåŒå‘æ•°æ®ç»Ÿè®¡
        locking_stats = dli_panel.groupby('locking_dimension_type').agg({
            'dli_score': ['count', 'mean', 'std']
        }).round(4)
        print(f"ğŸ“ˆ åŒå‘é”å®šç»Ÿè®¡:")
        print(locking_stats)
        
    except Exception as e:
        logger.error(f"âŒ åŒå‘DLIè®¡ç®—å¤±è´¥: {e}")
        raise