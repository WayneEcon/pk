#!/usr/bin/env python3
"""
æ•°æ®åŠ è½½æ¨¡å— (Data Loader Module)
============================================

09_econometric_analysis æ¨¡å—çš„æ•°æ®åŠ è½½ç»„ä»¶

ä½œè€…ï¼šEnergy Network Analysis Team
ç‰ˆæœ¬ï¼šv1.1 - å…¼å®¹ ovi_gas å¹¶å¢å¼ºè¯Šæ–­
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import Optional, Dict, List

logger = logging.getLogger(__name__)

class DataLoader:
    """
    æ•°æ®åŠ è½½å™¨ - ä¸“é—¨å¤„ç†ç©ºæ•°æ®å’Œç¼ºå¤±æ–‡ä»¶çš„å¥å£®åŠ è½½é€»è¾‘
    """
    
    def __init__(self, project_root: Optional[Path] = None):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ¨æ–­
        """
        if project_root is None:
            # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿èƒ½æ‰¾åˆ°æ­£ç¡®çš„é¡¹ç›®æ ¹ç›®å½•
            self.project_root = Path("/Users/ywz/Desktop/pku/ç¾å›½èƒ½æºç‹¬ç«‹/project/energy_network")
        else:
            self.project_root = Path(project_root)
        
        # æ›´æ–°è·¯å¾„é…ç½®ï¼šanalytical_panelä»08æ¨¡å—outputsç›®å½•åŠ è½½
        self.analytical_panel_path = self.project_root / "src" / "08_variable_construction" / "outputs" / "analytical_panel.csv"
        # ä»·æ ¼æ•°é‡å˜é‡æ–‡ä»¶è·¯å¾„
        self.price_quantity_path = self.project_root / "src" / "08_variable_construction" / "outputs" / "price_quantity_variables.csv"
        # DLIå’ŒVULå˜é‡æ–‡ä»¶è·¯å¾„
        self.node_dli_path = self.project_root / "src" / "08_variable_construction" / "08data" / "node_dli_us.csv"
        self.vul_us_path = self.project_root / "src" / "08_variable_construction" / "08data" / "vul_us.csv"
        
        logger.info(f"æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        logger.info(f"åˆ†æé¢æ¿è·¯å¾„: {self.analytical_panel_path}")
        logger.info(f"ä»·æ ¼æ•°é‡å˜é‡è·¯å¾„: {self.price_quantity_path}")
        
        # å®šä¹‰é¢„æœŸçš„åˆ—åï¼ˆåŸºäº08æ¨¡å—çš„è¾“å‡ºè§„èŒƒï¼‰
        self.expected_columns = self._get_expected_columns()
    
    def _get_expected_columns(self) -> List[str]:
        """
        å®šä¹‰é¢„æœŸçš„æ•°æ®åˆ—å
        
        Returns:
            é¢„æœŸçš„åˆ—ååˆ—è¡¨
        """
        return [
            # åŸºç¡€æ ‡è¯†å˜é‡
            'year', 'country', 'country_name',
            
            # å®è§‚ç»æµæ§åˆ¶å˜é‡
            'gdp_current_usd', 'population_total', 'trade_openness_gdp_pct',
            'log_gdp', 'log_population',
            
            # æ ¸å¿ƒç ”ç©¶å˜é‡ï¼ˆæ¥è‡ª08æ¨¡å—ï¼‰
            'node_dli_us',         # Node-DLI_US: ç¾å›½é”šå®šåŠ¨æ€é”å®šæŒ‡æ•°
            'vul_us',              # Vul_US: ç¾å›½é”šå®šè„†å¼±æ€§æŒ‡æ•°
            'ovi_gas',             # å·²ä» 'ovi' æ›´æ–°
            'us_prod_shock',       # US_ProdShock: ç¾å›½äº§é‡å†²å‡»
            
            # ç½‘ç»œæ‹“æ‰‘æŒ‡æ ‡ï¼ˆæ¥è‡ª03æ¨¡å—ï¼‰
            'betweenness_centrality', 'eigenvector_centrality',
            'in_degree', 'out_degree', 'total_degree',
            
            # è¾…åŠ©å˜é‡
            'us_production_oil', 'us_production_gas'
        ]
    
    def _fix_ovi_gas_data_integration(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ™ºèƒ½ä¿®å¤ï¼šä½¿ç”¨ç»Ÿä¸€æ ‡å‡†åŒ–ç³»ç»Ÿæ•´åˆ08æ¨¡å—åŸå§‹gas_oviæ•°æ®
        è§£å†³å›½å®¶ç¼–ç ä¸ä¸€è‡´å¯¼è‡´çš„æ•°æ®ä¸¢å¤±é—®é¢˜
        """
        logger.info("ğŸ”§ æ™ºèƒ½ä¿®å¤ï¼šæ•´åˆä¸¢å¤±çš„ovi_gasæ•°æ®...")
        
        try:
            # å¯¼å…¥ç»Ÿä¸€æ ‡å‡†åŒ–ç³»ç»Ÿ
            from country_standardizer import CountryStandardizer
            standardizer = CountryStandardizer()
            
            # å°è¯•åŠ è½½åŸå§‹gas_oviæ•°æ®
            gas_ovi_path = self.project_root / "src" / "08_variable_construction" / "08data" / "gas_ovi.csv"
            
            if not gas_ovi_path.exists():
                logger.warning(f"âš ï¸ åŸå§‹gas_oviæ–‡ä»¶ä¸å­˜åœ¨: {gas_ovi_path}")
                return df
            
            df_gas_ovi = pd.read_csv(gas_ovi_path)
            logger.info(f"ğŸ“Š åŠ è½½åŸå§‹gas_oviæ•°æ®: {df_gas_ovi.shape}")
            
            # ä½¿ç”¨ç»Ÿä¸€æ ‡å‡†åŒ–ç³»ç»Ÿå¤„ç†å›½å®¶ç¼–ç 
            df_gas_ovi_standardized = standardizer.standardize_dataframe(
                df_gas_ovi, 
                country_column='country', 
                new_column_name='country_standardized'
            )
            
            # ä¿ç•™æ ‡å‡†åŒ–æˆåŠŸçš„æ•°æ®
            df_gas_ovi_clean = df_gas_ovi_standardized.dropna(subset=['country_standardized'])
            
            # å‡†å¤‡åˆå¹¶æ•°æ®
            ovi_data = df_gas_ovi_clean[['country_standardized', 'year', 'ovi_gas']].rename(
                columns={'country_standardized': 'country'}
            )
            
            # åˆå¹¶åˆ°ä¸»æ•°æ®æ¡†
            df_before = df.copy()
            
            # å…ˆç§»é™¤åŸæœ‰çš„ovi_gasåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'ovi_gas' in df.columns:
                df = df.drop(columns=['ovi_gas'])
            
            # å·¦è¿æ¥åˆå¹¶ovi_gasæ•°æ®
            df_merged = df.merge(ovi_data, on=['country', 'year'], how='left')
            
            # ç»Ÿè®¡ä¿®å¤æ•ˆæœ
            original_ovi_count = df_before['ovi_gas'].notna().sum() if 'ovi_gas' in df_before.columns else 0
            new_ovi_count = df_merged['ovi_gas'].notna().sum()
            countries_with_ovi = df_merged[df_merged['ovi_gas'].notna()]['country'].nunique()
            
            logger.info(f"âœ… ovi_gasæ•°æ®æ™ºèƒ½ä¿®å¤å®Œæˆ:")
            logger.info(f"   â€¢ ä¿®å¤å‰æœ‰æ•ˆè§‚æµ‹: {original_ovi_count}")
            logger.info(f"   â€¢ ä¿®å¤åæœ‰æ•ˆè§‚æµ‹: {new_ovi_count}")
            logger.info(f"   â€¢ æ–°å¢æœ‰æ•ˆè§‚æµ‹: {new_ovi_count - original_ovi_count}")
            logger.info(f"   â€¢ æœ‰æ•°æ®çš„å›½å®¶: {countries_with_ovi}")
            
            return df_merged
            
        except Exception as e:
            logger.error(f"âŒ ovi_gasæ•°æ®æ™ºèƒ½ä¿®å¤å¤±è´¥: {str(e)}")
            return df

    def _load_and_merge_price_quantity_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åŠ è½½å¹¶åˆå¹¶ä»·æ ¼æ•°é‡å˜é‡æ•°æ®
        
        Args:
            df: ä¸»è¦åˆ†æé¢æ¿æ•°æ®
            
        Returns:
            åˆå¹¶äº†P_itå’Œg_itå˜é‡çš„æ•°æ®æ¡†
        """
        logger.info("ğŸ”— å¼€å§‹åŠ è½½å¹¶åˆå¹¶ä»·æ ¼æ•°é‡å˜é‡...")
        
        try:
            if not self.price_quantity_path.exists():
                logger.warning(f"âš ï¸ ä»·æ ¼æ•°é‡å˜é‡æ–‡ä»¶ä¸å­˜åœ¨: {self.price_quantity_path}")
                return df
            
            # åŠ è½½ä»·æ ¼æ•°é‡å˜é‡æ•°æ®
            df_pq = pd.read_csv(self.price_quantity_path)
            logger.info(f"ğŸ“Š ä»·æ ¼æ•°é‡å˜é‡æ•°æ®: {df_pq.shape[0]} è¡Œ Ã— {df_pq.shape[1]} åˆ—")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_cols = ['country', 'year', 'P_it', 'g_it']
            missing_cols = [col for col in required_cols if col not in df_pq.columns]
            if missing_cols:
                logger.warning(f"âš ï¸ ä»·æ ¼æ•°é‡æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                return df
            
            # å·¦è¿æ¥åˆå¹¶æ•°æ®ï¼ˆä¿ç•™ä¸»é¢æ¿çš„æ‰€æœ‰è®°å½•ï¼‰
            df_before = df.copy()
            df_merged = df.merge(df_pq[required_cols], on=['country', 'year'], how='left')
            
            # ç»Ÿè®¡åˆå¹¶æ•ˆæœ
            pit_count = df_merged['P_it'].notna().sum()
            git_count = df_merged['g_it'].notna().sum()
            
            logger.info(f"âœ… ä»·æ ¼æ•°é‡å˜é‡åˆå¹¶å®Œæˆ:")
            logger.info(f"   â€¢ P_itæœ‰æ•ˆè§‚æµ‹: {pit_count}")
            logger.info(f"   â€¢ g_itæœ‰æ•ˆè§‚æµ‹: {git_count}")
            logger.info(f"   â€¢ è¦†ç›–å›½å®¶: {df_merged[df_merged['P_it'].notna()]['country'].nunique()} ä¸ª")
            
            return df_merged
            
        except Exception as e:
            logger.error(f"âŒ ä»·æ ¼æ•°é‡å˜é‡åˆå¹¶å¤±è´¥: {str(e)}")
            return df

    def _load_and_merge_dli_vul_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åŠ è½½å¹¶åˆå¹¶node_dli_uså’Œvul_uså˜é‡æ•°æ®
        
        Args:
            df: ä¸»è¦åˆ†æé¢æ¿æ•°æ®
            
        Returns:
            åˆå¹¶äº†DLIå’ŒVULå˜é‡çš„æ•°æ®æ¡†
        """
        logger.info("ğŸ”— å¼€å§‹åŠ è½½å¹¶åˆå¹¶DLIå’ŒVULå˜é‡...")
        
        try:
            # åŠ è½½node_dli_usæ•°æ®
            if self.node_dli_path.exists():
                df_dli = pd.read_csv(self.node_dli_path)
                logger.info(f"ğŸ“Š Node-DLIæ•°æ®: {df_dli.shape[0]} è¡Œ")
                
                # åˆå¹¶DLIæ•°æ®
                df_before = df.copy()
                df = df.merge(df_dli[['country', 'year', 'node_dli_us']], on=['country', 'year'], how='left')
                dli_count = df['node_dli_us'].notna().sum()
                logger.info(f"   âœ… node_dli_usåˆå¹¶å®Œæˆ: {dli_count}æ¡æœ‰æ•ˆè§‚æµ‹")
            else:
                logger.warning(f"âš ï¸ Node-DLIæ–‡ä»¶ä¸å­˜åœ¨: {self.node_dli_path}")
            
            # åŠ è½½vul_usæ•°æ®
            if self.vul_us_path.exists():
                df_vul = pd.read_csv(self.vul_us_path)
                logger.info(f"ğŸ“Š VUL-USæ•°æ®: {df_vul.shape[0]} è¡Œ")
                
                # åˆå¹¶VULæ•°æ®
                df = df.merge(df_vul[['country', 'year', 'vul_us']], on=['country', 'year'], how='left')
                vul_count = df['vul_us'].notna().sum()
                logger.info(f"   âœ… vul_usåˆå¹¶å®Œæˆ: {vul_count}æ¡æœ‰æ•ˆè§‚æµ‹")
            else:
                logger.warning(f"âš ï¸ VUL-USæ–‡ä»¶ä¸å­˜åœ¨: {self.vul_us_path}")
                
            logger.info("âœ… DLIå’ŒVULå˜é‡åˆå¹¶å®Œæˆ")
            return df
            
        except Exception as e:
            logger.error(f"âŒ DLIå’ŒVULå˜é‡åˆå¹¶å¤±è´¥: {str(e)}")
            return df

    def load_analytical_panel(self) -> pd.DataFrame:
        """åŠ è½½åˆ†æé¢æ¿æ•°æ® - å®ç°ä¸¥æ ¼çš„å¹³è¡¡é¢æ¿æ•°æ®æ¸…æ´—é€»è¾‘"""
        logger.info("ğŸ” å¼€å§‹åŠ è½½åˆ†æé¢æ¿æ•°æ®...")
        
        if not self.analytical_panel_path.exists():
            logger.error(f"âŒ åˆ†æé¢æ¿æ–‡ä»¶ä¸å­˜åœ¨: {self.analytical_panel_path}")
            logger.error("è¿™æ˜¯è‡´å‘½é”™è¯¯ï¼å¿…é¡»ä½¿ç”¨08æ¨¡å—è¾“å‡ºçš„æ­£ç¡®æ•°æ®æ–‡ä»¶ã€‚")
            return self._create_empty_dataframe()
        
        try:
            # æ­¥éª¤1ï¼šåŠ è½½åŸå§‹æ•°æ®
            df_raw = pd.read_csv(self.analytical_panel_path)
            logger.info(f"ğŸ“Š åŸå§‹æ•°æ®åŠ è½½å®Œæˆ: {df_raw.shape[0]} è¡Œ Ã— {df_raw.shape[1]} åˆ—")
            
            if len(df_raw) == 0:
                logger.error("âŒ åˆ†æé¢æ¿æ–‡ä»¶ä¸ºç©º - è¿™æ˜¯è‡´å‘½é”™è¯¯ï¼")
                return self._create_empty_dataframe()
            
            # æ­¥éª¤1.5ï¼šç´§æ€¥ä¿®å¤ovi_gasæ•°æ®ä¸¢å¤±é—®é¢˜
            df_fixed = self._fix_ovi_gas_data_integration(df_raw)
            
            # æ­¥éª¤1.6ï¼šåŠ è½½å¹¶åˆå¹¶ä»·æ ¼æ•°é‡å˜é‡
            df_with_price_qty = self._load_and_merge_price_quantity_data(df_fixed)
            
            # æ­¥éª¤1.7ï¼šåŠ è½½å¹¶åˆå¹¶node_dli_uså’Œvul_uså˜é‡
            df_with_dli_vul = self._load_and_merge_dli_vul_data(df_with_price_qty)
            
            # æ­¥éª¤2ï¼šä¸¥æ ¼çš„å¹³è¡¡é¢æ¿æ•°æ®æ¸…æ´—
            df_cleaned = self._enforce_balanced_panel_constraints(df_with_dli_vul)
            
            # æ­¥éª¤3ï¼šç¡®ä¿åˆ—å®Œæ•´æ€§
            df_final = self._ensure_expected_columns(df_cleaned)
            
            logger.info(f"âœ… å¹³è¡¡é¢æ¿æ•°æ®å‡†å¤‡å®Œæˆ: {df_final.shape}")
            return df_final
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åˆ†æé¢æ¿æ•°æ®å¤±è´¥: {str(e)}")
            return self._create_empty_dataframe()
    
    def _enforce_balanced_panel_constraints(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ‰§è¡Œä¸¥æ ¼çš„å¹³è¡¡é¢æ¿çº¦æŸæ¡ä»¶
        ç¡®ä¿æ‰€æœ‰ç”¨äºå›å½’åˆ†æçš„æ ¸å¿ƒå˜é‡éƒ½æ²¡æœ‰ç¼ºå¤±å€¼
        """
        logger.info("ğŸ”§ å¼€å§‹æ‰§è¡Œä¸¥æ ¼çš„å¹³è¡¡é¢æ¿çº¦æŸ...")
        
        # é‡‡ç”¨åˆ†å±‚æ¸…æ´—ç­–ç•¥ï¼šä»…å¯¹æ ¸å¿ƒåˆ†æå˜é‡è¦æ±‚ä¸¥æ ¼æ— ç¼ºå¤±
        # ç¬¬ä¸€å±‚ï¼šæ ¸å¿ƒå›å½’åˆ†æå˜é‡ï¼ˆç»å¯¹ä¸èƒ½æœ‰ç¼ºå¤±å€¼ï¼‰
        core_analysis_variables = [
            'country', 'year',                    # é¢æ¿æ ‡è¯†
            'node_dli_us', 'vul_us',              # æ ¸å¿ƒå› å˜é‡
            'ovi_gas', 'us_prod_shock',           # æ ¸å¿ƒè§£é‡Šå˜é‡
            'log_gdp', 'log_population'           # åŸºç¡€æ§åˆ¶å˜é‡
        ]
        
        # ç¬¬äºŒå±‚ï¼šå¯é€‰æ§åˆ¶å˜é‡ï¼ˆå…è®¸éƒ¨åˆ†ç¼ºå¤±ï¼Œä½†ä¼šå½±å“æ ·æœ¬é‡ï¼‰
        optional_control_variables = [
            'trade_openness_gdp_pct',
            'betweenness_centrality', 'eigenvector_centrality',
            'in_degree', 'out_degree', 'total_degree'
        ]
        
        # æ£€æŸ¥æ ¸å¿ƒå˜é‡æ˜¯å¦å­˜åœ¨
        missing_core_vars = [var for var in core_analysis_variables if var not in df.columns]
        if missing_core_vars:
            logger.error(f"âŒ æ ¸å¿ƒå˜é‡ç¼ºå¤±: {missing_core_vars}")
            logger.error("è¿™è¿åäº†08æ¨¡å—çš„æ•°æ®è¾“å‡ºçº¦å®šï¼")
        
        # æŠ¥å‘ŠåŸå§‹æ•°æ®ç»Ÿè®¡
        original_countries = df['country'].nunique() if 'country' in df.columns else 0
        original_years = df['year'].nunique() if 'year' in df.columns else 0
        logger.info(f"ğŸ“ˆ åŸå§‹æ•°æ®ç»Ÿè®¡: {original_countries} ä¸ªå›½å®¶, {original_years} ä¸ªå¹´ä»½")
        
        # æ­¥éª¤1ï¼šå¯¹æ ¸å¿ƒå˜é‡æ‰§è¡Œä¸¥æ ¼æ¸…æ´—
        available_core_vars = [var for var in core_analysis_variables if var in df.columns]
        logger.info(f"ğŸ¯ ç¬¬ä¸€å±‚æ¸…æ´—ï¼šå¯¹ {len(available_core_vars)} ä¸ªæ ¸å¿ƒå˜é‡æ‰§è¡Œä¸¥æ ¼ç¼ºå¤±å€¼å‰”é™¤...")
        
        df_before = df.copy()
        df_core_cleaned = df.dropna(subset=available_core_vars)
        
        # æŠ¥å‘Šç¬¬ä¸€å±‚æ¸…æ´—ç»“æœ
        core_rows_dropped = len(df_before) - len(df_core_cleaned)
        logger.info(f"   â€¢ æ ¸å¿ƒå˜é‡æ¸…æ´—å‰”é™¤: {core_rows_dropped} è¡Œ")
        logger.info(f"   â€¢ æ ¸å¿ƒæ¸…æ´—åæ ·æœ¬: {len(df_core_cleaned)} è§‚æµ‹å€¼")
        
        # æ­¥éª¤2ï¼šè¯„ä¼°å¯é€‰å˜é‡çš„å¯ç”¨æ€§ï¼ˆä¸å¼ºåˆ¶å‰”é™¤ï¼‰
        logger.info(f"ğŸ” ç¬¬äºŒå±‚è¯„ä¼°ï¼šæ£€æŸ¥å¯é€‰æ§åˆ¶å˜é‡çš„æ•°æ®å®Œæ•´æ€§...")
        available_optional_vars = []
        for var in optional_control_variables:
            if var in df_core_cleaned.columns:
                non_null_count = df_core_cleaned[var].notna().sum()
                coverage_rate = non_null_count / len(df_core_cleaned)
                logger.info(f"   â€¢ {var}: {non_null_count}/{len(df_core_cleaned)} ({coverage_rate:.1%})")
                if coverage_rate >= 0.7:  # è‡³å°‘70%çš„æ•°æ®å¯ç”¨
                    available_optional_vars.append(var)
        
        logger.info(f"   â€¢ é«˜è´¨é‡å¯é€‰å˜é‡: {available_optional_vars}")
        
        # æœ€ç»ˆæ¸…æ´—å†³ç­–ï¼šåŸºäºæ ¸å¿ƒå˜é‡çš„ä¸¥æ ¼æ¸…æ´—ç»“æœ
        df_cleaned = df_core_cleaned.copy()
        
        # æŠ¥å‘Šæ¸…æ´—ç»“æœ
        rows_dropped = len(df_before) - len(df_cleaned)
        final_countries = df_cleaned['country'].nunique() if 'country' in df_cleaned.columns else 0
        final_years = df_cleaned['year'].nunique() if 'year' in df_cleaned.columns else 0
        
        logger.info(f"ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆ:")
        logger.info(f"   â€¢ å‰”é™¤è§‚æµ‹å€¼: {rows_dropped} è¡Œ")
        logger.info(f"   â€¢ æœ€ç»ˆæ ·æœ¬: {len(df_cleaned)} è§‚æµ‹å€¼")
        logger.info(f"   â€¢ æœ€ç»ˆå›½å®¶æ•°: {final_countries}")
        logger.info(f"   â€¢ æœ€ç»ˆå¹´ä»½æ•°: {final_years}")
        
        # éªŒè¯æ˜¯å¦ç¬¦åˆé¢„æœŸçš„å¹³è¡¡é¢æ¿è§„æ ¼ï¼ˆ45å›½å®¶ Ã— 669è§‚æµ‹å€¼ï¼‰
        if len(df_cleaned) == 669 and final_countries == 45:
            logger.info("âœ… æ•°æ®å®Œå…¨ç¬¦åˆé¢„æœŸçš„å¹³è¡¡é¢æ¿è§„æ ¼ (45å›½å®¶ Ã— 669è§‚æµ‹å€¼)")
        else:
            logger.warning(f"âš ï¸ æ•°æ®ä¸ç¬¦åˆé¢„æœŸè§„æ ¼:")
            logger.warning(f"   æœŸæœ›: 45å›½å®¶ Ã— 669è§‚æµ‹å€¼")
            logger.warning(f"   å®é™…: {final_countries}å›½å®¶ Ã— {len(df_cleaned)}è§‚æµ‹å€¼")
        
        return df_cleaned
    
    def _create_empty_dataframe(self) -> pd.DataFrame:
        """
        åˆ›å»ºç©ºä½†ç»“æ„æ­£ç¡®çš„DataFrame
        
        Returns:
            åŒ…å«æ‰€æœ‰é¢„æœŸåˆ—ä½†æ— æ•°æ®çš„DataFrame
        """
        logger.info("   æ„å»ºç©ºDataFrameæ¡†æ¶...")
        
        # åˆ›å»ºç©ºDataFrameä½†åŒ…å«æ‰€æœ‰é¢„æœŸåˆ—
        df = pd.DataFrame(columns=self.expected_columns)
        
        # è®¾ç½®æ­£ç¡®çš„æ•°æ®ç±»å‹
        type_mapping = {
            'year': 'int64',
            'country': 'str',
            'country_name': 'str',
            'gdp_current_usd': 'float64',
            'population_total': 'float64',
            'trade_openness_gdp_pct': 'float64',
            'log_gdp': 'float64',
            'log_population': 'float64',
            'node_dli_us': 'float64',
            'vul_us': 'float64',
            'ovi_gas': 'float64', # å·²ä» 'ovi' æ›´æ–°
            'us_prod_shock': 'float64'
        }
        
        for col, dtype in type_mapping.items():
            if col in df.columns:
                if dtype == 'str':
                    df[col] = df[col].astype('object')
                else:
                    df[col] = df[col].astype(dtype)
        
        logger.info(f"   ç©ºDataFrameæ¡†æ¶åˆ›å»ºå®Œæˆ: {len(self.expected_columns)} åˆ—")
        return df
    
    def _ensure_expected_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç¡®ä¿DataFrameåŒ…å«æ‰€æœ‰é¢„æœŸåˆ—
        
        Args:
            df: è¾“å…¥DataFrame
            
        Returns:
            åŒ…å«æ‰€æœ‰é¢„æœŸåˆ—çš„DataFrame
        """
        missing_cols = set(self.expected_columns) - set(df.columns)
        
        if missing_cols:
            logger.info(f"   æ·»åŠ ç¼ºå¤±åˆ—: {sorted(missing_cols)}")
            for col in missing_cols:
                df[col] = np.nan
        
        # é‡æ–°æ’åºåˆ—ä»¥åŒ¹é…é¢„æœŸé¡ºåº
        available_expected_cols = [col for col in self.expected_columns if col in df.columns]
        other_cols = [col for col in df.columns if col not in self.expected_columns]
        df = df[available_expected_cols + other_cols]
        
        return df
    
    def get_data_summary(self, df: pd.DataFrame) -> Dict:
        """è·å–æ•°æ®æ‘˜è¦ç»Ÿè®¡ï¼ŒåŒ…å«æè¿°æ€§ç»Ÿè®¡"""
        if len(df) == 0:
            return {
                'total_rows': 0, 'total_cols': len(df.columns), 'year_range': 'N/A',
                'countries': 0, 'key_variables_available': [], 'data_status': 'empty',
                'descriptive_stats': {}
            }
        
        key_variables = ['node_dli_us', 'vul_us', 'ovi_gas', 'us_prod_shock']
        available_key_vars_info = [f"{var} ({df[var].notna().sum()}/{len(df)})" for var in key_variables if var in df.columns and df[var].notna().any()]
        
        desc_stats = {}
        if available_key_vars_info:
            desc_df = df[key_variables].dropna(how='all').describe().transpose()
            desc_stats = desc_df[['count', 'mean', 'std', 'min', 'max']].to_dict('index')

        return {
            'total_rows': len(df),
            'total_cols': len(df.columns),
            'year_range': f"{int(df['year'].min())}-{int(df['year'].max())}" if 'year' in df.columns and df['year'].notna().any() else 'N/A',
            'countries': df['country'].nunique() if 'country' in df.columns else 0,
            'key_variables_available': available_key_vars_info,
            'data_status': 'available' if available_key_vars_info else 'missing_key_vars',
            'descriptive_stats': desc_stats,
            'raw_panel_data': df # ä¼ é€’åŸå§‹æ•°æ®ç”¨äºå¯è§†åŒ–
        }
    
    def validate_data_for_analysis(self, df: pd.DataFrame) -> Dict:
        """éªŒè¯æ•°æ®æ˜¯å¦é€‚åˆè¿›è¡Œè®¡é‡åˆ†æ"""
        validation = {'is_valid_for_analysis': False, 'issues': [], 'recommendations': []}
        if len(df) == 0:
            validation['issues'].append("æ•°æ®é›†ä¸ºç©º")
            return validation

        key_variables = ['node_dli_us', 'vul_us', 'ovi_gas', 'us_prod_shock']
        missing_key_vars = [var for var in key_variables if var not in df.columns or df[var].isna().all()]
        if missing_key_vars:
            validation['issues'].append(f"å…³é”®ç ”ç©¶å˜é‡ç¼ºå¤±æˆ–å…¨ä¸ºç©º: {missing_key_vars}")
        
        if 'year' not in df.columns or 'country' not in df.columns:
            validation['issues'].append("ç¼ºå°‘é¢æ¿æ•°æ®å¿…éœ€çš„ 'year' æˆ– 'country' æ ‡è¯†")
        
        validation['is_valid_for_analysis'] = not validation['issues']
        return validation


def get_data_status() -> Dict:
    """ä¾¿æ·å‡½æ•°ï¼šåŠ è½½æ•°æ®å¹¶è·å–å…¶çŠ¶æ€æ‘˜è¦"""
    loader = DataLoader()
    df = loader.load_analytical_panel()
    summary = loader.get_data_summary(df)
    validation = loader.validate_data_for_analysis(df)
    
    return {'summary': summary, 'validation': validation}


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    print("ğŸ”¬ 09_econometric_analysis æ•°æ®åŠ è½½å™¨æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    loader = DataLoader()
    df = loader.load_analytical_panel()
    
    print(f"\nğŸ“Š æ•°æ®åŠ è½½ç»“æœ:")
    print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"åˆ—å: {list(df.columns)}")
    
    # æ•°æ®æ‘˜è¦
    summary = loader.get_data_summary(df)
    print(f"\nğŸ“ˆ æ•°æ®æ‘˜è¦:")
    for key, value in summary.items():
        print(f"  {key}: {value}")
    
    # æ•°æ®éªŒè¯
    validation = loader.validate_data_for_analysis(df)
    print(f"\nâœ… æ•°æ®éªŒè¯:")
    print(f"  é€‚åˆåˆ†æ: {validation['is_valid_for_analysis']}")
    if validation['issues']:
        print(f"  é—®é¢˜: {validation['issues']}")
    if validation['recommendations']:
        print(f"  å»ºè®®: {validation['recommendations']}")
    
    print("\nğŸ‰ æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ!")