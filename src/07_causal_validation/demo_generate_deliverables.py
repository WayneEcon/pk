#!/usr/bin/env python3
"""
å› æœéªŒè¯åˆ†æ - å…³é”®äº§å‡ºç”Ÿæˆæ¼”ç¤º
===============================

å¿«é€Ÿç”Ÿæˆæ ¸å¿ƒäº§å‡ºï¼š
1. network_resilience.csv - ç½‘ç»œéŸ§æ€§æ•°æ®åº“
2. causal_validation_report.md - å­¦æœ¯çº§éªŒè¯æŠ¥å‘Š

æœ¬è„šæœ¬ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºå®Œæ•´åˆ†ææµç¨‹ã€‚
"""

import sys
from pathlib import Path
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯¼å…¥åˆ†ææ¨¡å—
from resilience_calculator import generate_resilience_database, NetworkResilienceCalculator
from causal_model import run_causal_validation, CausalAnalyzer
import pandas as pd
import numpy as np
import networkx as nx

def create_demo_networks():
    """åˆ›å»ºæ¼”ç¤ºç”¨çš„èƒ½æºç½‘ç»œæ•°æ®"""
    
    logger.info("ğŸ­ åˆ›å»ºæ¼”ç¤ºç½‘ç»œæ•°æ®...")
    
    # ä¸»è¦èƒ½æºå›½å®¶
    countries = ['USA', 'CHN', 'RUS', 'SAU', 'CAN', 'DEU', 'JPN', 'GBR', 'FRA', 'IND', 
                'BRA', 'MEX', 'AUS', 'NOR', 'ARE', 'KWT', 'IRN', 'IRQ', 'VEN', 'QAT']
    years = list(range(2010, 2025))
    
    networks = {}
    np.random.seed(42)  # ç¡®ä¿å¯é‡ç°æ€§
    
    for year in years:
        G = nx.DiGraph()
        G.add_nodes_from(countries)
        
        # ç”Ÿæˆç°å®çš„å¼‚è´¨æ€§è´¸æ˜“ç½‘ç»œ - ä¿®å¤å…±çº¿æ€§é—®é¢˜
        # å®šä¹‰å›½å®¶è§’è‰²å’Œç‰¹å¾ï¼ˆå¢åŠ å·®å¼‚æ€§ï¼‰
        country_roles = {
            'USA': {'type': 'hub_importer', 'centrality_factor': 1.5, 'volatility': 0.1},
            'CHN': {'type': 'growing_hub', 'centrality_factor': 1.3, 'volatility': 0.15},
            'RUS': {'type': 'major_exporter', 'centrality_factor': 1.2, 'volatility': 0.2},
            'SAU': {'type': 'oil_exporter', 'centrality_factor': 1.0, 'volatility': 0.25},
            'DEU': {'type': 'industrial_hub', 'centrality_factor': 1.1, 'volatility': 0.08},
            'JPN': {'type': 'island_importer', 'centrality_factor': 0.9, 'volatility': 0.12},
            'CAN': {'type': 'resource_exporter', 'centrality_factor': 0.8, 'volatility': 0.15},
            'GBR': {'type': 'financial_hub', 'centrality_factor': 0.7, 'volatility': 0.1},
            'FRA': {'type': 'european_hub', 'centrality_factor': 0.75, 'volatility': 0.1},
            'IND': {'type': 'emerging_importer', 'centrality_factor': 0.85, 'volatility': 0.18},
            'BRA': {'type': 'regional_hub', 'centrality_factor': 0.6, 'volatility': 0.2},
            'MEX': {'type': 'transit_hub', 'centrality_factor': 0.5, 'volatility': 0.15},
            'AUS': {'type': 'resource_exporter', 'centrality_factor': 0.45, 'volatility': 0.12},
            'NOR': {'type': 'oil_exporter', 'centrality_factor': 0.4, 'volatility': 0.2},
            'ARE': {'type': 'oil_hub', 'centrality_factor': 0.5, 'volatility': 0.3},
            'KWT': {'type': 'oil_exporter', 'centrality_factor': 0.35, 'volatility': 0.25},
            'IRN': {'type': 'constrained_exporter', 'centrality_factor': 0.3, 'volatility': 0.4},
            'IRQ': {'type': 'unstable_exporter', 'centrality_factor': 0.25, 'volatility': 0.5},
            'VEN': {'type': 'declining_exporter', 'centrality_factor': 0.2, 'volatility': 0.6},
            'QAT': {'type': 'lng_exporter', 'centrality_factor': 0.4, 'volatility': 0.2}
        }
        
        # æ—¶é—´æ•ˆåº”ï¼šä¸åŒäº‹ä»¶å¯¹ä¸åŒå›½å®¶çš„å½±å“
        time_effects = {
            2014: {'RUS': -0.3, 'IRN': -0.2},  # åˆ¶è£
            2016: {'USA': 0.1, 'CAN': 0.05},   # é¡µå²©æ²¹é©å‘½
            2018: {'IRN': -0.4, 'VEN': -0.3},  # åˆ¶è£åŠ å‰§
            2020: {'all': -0.15},               # ç–«æƒ…å†²å‡»
            2022: {'RUS': -0.5, 'USA': 0.2}    # åœ°ç¼˜æ”¿æ²»å†²å‡»
        }
        
        # æ·»åŠ å¼‚è´¨æ€§è´¸æ˜“è¾¹
        for exporter in countries:
            if exporter not in country_roles:
                continue
                
            exporter_role = country_roles[exporter]
            
            for importer in countries:
                if exporter == importer or importer not in country_roles:
                    continue
                    
                importer_role = country_roles[importer]
                
                # åŸºäºè§’è‰²ç¡®å®šè´¸æ˜“æ¦‚ç‡
                trade_prob = 0.1  # åŸºç¡€æ¦‚ç‡
                
                # å‡ºå£å›½ç±»å‹å½±å“
                if exporter_role['type'] in ['major_exporter', 'oil_exporter', 'resource_exporter']:
                    trade_prob += 0.3
                    
                # è¿›å£å›½ç±»å‹å½±å“  
                if importer_role['type'] in ['hub_importer', 'growing_hub', 'industrial_hub']:
                    trade_prob += 0.2
                
                # åœ°ç†å’Œæ”¿æ²»å…³ç³»
                if (exporter == 'CAN' and importer == 'USA') or \
                   (exporter == 'MEX' and importer == 'USA') or \
                   (exporter in ['SAU', 'ARE', 'KWT'] and importer in ['CHN', 'JPN', 'IND']):
                    trade_prob += 0.4
                
                if np.random.random() < trade_prob:
                    # åŸºç¡€è´¸æ˜“æµé‡ï¼ˆåŸºäºå›½å®¶è§’è‰²ï¼‰
                    base_flow = np.random.lognormal(
                        1 + exporter_role['centrality_factor'], 
                        exporter_role['volatility']
                    )
                    
                    # æ—¶é—´è¶‹åŠ¿æ•ˆåº”
                    year_trend = 1 + 0.02 * (year - 2010)
                    
                    # ç‰¹å®šå¹´ä»½å†²å‡»
                    shock_effect = 1.0
                    if year in time_effects:
                        if exporter in time_effects[year]:
                            shock_effect *= (1 + time_effects[year][exporter])
                        elif 'all' in time_effects[year]:
                            shock_effect *= (1 + time_effects[year]['all'])
                    
                    # æœ€ç»ˆè´¸æ˜“æµé‡
                    final_flow = base_flow * year_trend * shock_effect * (1 + np.random.normal(0, 0.2))
                    
                    if final_flow > 0:
                        G.add_edge(exporter, importer, weight=final_flow)
        
        # æ·»åŠ ä¸€äº›åå‘æµåŠ¨ï¼ˆæˆå“æ²¹ã€LNGç­‰ï¼‰
        major_importers = ['USA', 'CHN', 'JPN', 'DEU', 'GBR', 'FRA', 'IND']
        major_exporters = ['SAU', 'RUS', 'CAN', 'NOR', 'IRN', 'IRQ', 'VEN', 'ARE', 'KWT', 'QAT']
        
        for importer in major_importers:
            for exporter in major_exporters:
                if importer in countries and exporter in countries:
                    if np.random.random() < 0.15:  # 15%æ¦‚ç‡æœ‰åå‘è´¸æ˜“
                        reverse_flow = np.random.lognormal(1, 0.5)
                        if G.has_edge(importer, exporter):
                            G[importer][exporter]['weight'] += reverse_flow
                        else:
                            G.add_edge(importer, exporter, weight=reverse_flow)
        
        networks[year] = G
        logger.info(f"   {year}å¹´: {G.number_of_nodes()}èŠ‚ç‚¹, {G.number_of_edges()}è¾¹")
    
    return networks

def create_demo_dli_data(networks):
    """åŸºäºç½‘ç»œæ•°æ®åˆ›å»ºæ¼”ç¤ºDLIæ•°æ®"""
    
    logger.info("ğŸ“Š åˆ›å»ºæ¼”ç¤ºDLIæ•°æ®...")
    
    dli_data = []
    np.random.seed(42)
    
    # å›½å®¶åŸºç¡€DLIç‰¹å¾ï¼ˆå¢åŠ å·®å¼‚æ€§ï¼‰
    country_base_dli = {
        'USA': 0.25,  # èƒ½æºç‹¬ç«‹æ€§è¾ƒå¼º
        'CHN': 0.65,  # é«˜åº¦ä¾èµ–è¿›å£
        'RUS': 0.15,  # ä¸»è¦å‡ºå£å›½
        'SAU': 0.10,  # çŸ³æ²¹å‡ºå£å¤§å›½
        'DEU': 0.55,  # å·¥ä¸šå›½å®¶ï¼Œä¾èµ–è¿›å£
        'JPN': 0.70,  # å²›å›½ï¼Œé«˜åº¦ä¾èµ–
        'CAN': 0.20,  # èµ„æºä¸°å¯Œ
        'GBR': 0.60,  # åå·¥ä¸šåŒ–ï¼Œä¾èµ–è¿›å£
        'FRA': 0.50,  # æ··åˆèƒ½æºç»“æ„
        'IND': 0.58,  # æ–°å…´ç»æµä½“ï¼Œéœ€æ±‚å¢é•¿
        'BRA': 0.35,  # å—ç¾åœ°åŒºå¤§å›½
        'MEX': 0.45,  # è¿‡æ¸¡å‹ç»æµ
        'AUS': 0.18,  # èµ„æºå‡ºå£å›½
        'NOR': 0.12,  # çŸ³æ²¹å‡ºå£å›½
        'ARE': 0.08,  # æµ·æ¹¾çŸ³æ²¹å›½
        'KWT': 0.06,  # çŸ³æ²¹å‡ºå£å›½
        'IRN': 0.22,  # å—åˆ¶è£å½±å“
        'IRQ': 0.30,  # æ”¿å±€ä¸ç¨³
        'VEN': 0.40,  # ç»æµå›°éš¾
        'QAT': 0.14   # å°å‹å¯Œå›½
    }
    
    for year, G in networks.items():
        for country in G.nodes():
            if country not in country_base_dli:
                continue
                
            # åŸºç¡€DLIï¼ˆå›½å®¶ç‰¹å¾ï¼‰
            base_dli = country_base_dli[country]
            
            # ç½‘ç»œç»“æ„å½±å“ï¼ˆå¢åŠ ç°å®æ€§ï¼‰
            network_effect = 0
            if G.number_of_edges() > 0:
                # å…¥åº¦é›†ä¸­åº¦ï¼ˆä¾›åº”å•†ä¾èµ–ï¼‰
                in_edges = [(s, d['weight']) for s, t, d in G.in_edges(country, data=True)]
                if in_edges:
                    total_imports = sum(weight for _, weight in in_edges)
                    if total_imports > 0:
                        import_shares = [weight/total_imports for _, weight in in_edges]
                        supply_concentration = sum(share**2 for share in import_shares)
                        network_effect += 0.2 * supply_concentration
                
                # å‡ºåº¦å¤šæ ·åŒ–ï¼ˆå‡ºå£èƒ½åŠ›ï¼‰
                out_edges = [(t, d['weight']) for s, t, d in G.out_edges(country, data=True)]
                if out_edges:
                    total_exports = sum(weight for _, weight in out_edges)
                    export_diversity = len(out_edges) / max(1, G.number_of_nodes() - 1)
                    network_effect -= 0.1 * export_diversity  # å‡ºå£å¤šæ ·åŒ–é™ä½é”å®š
            
            # æ—¶é—´è¶‹åŠ¿æ•ˆåº”ï¼ˆéçº¿æ€§ï¼‰
            time_base = (year - 2010) / 15.0  # æ ‡å‡†åŒ–åˆ°[0,1]
            
            # ä¸åŒç±»å‹å›½å®¶çš„æ—¶é—´è¶‹åŠ¿ä¸åŒ
            if country in ['USA', 'CAN', 'NOR']:  # èƒ½æºç‹¬ç«‹æ”¹å–„
                time_trend = -0.05 * time_base + 0.02 * time_base**2
            elif country in ['CHN', 'IND']:  # ä¾èµ–åº¦å…ˆå‡åé™
                time_trend = 0.1 * time_base - 0.08 * time_base**2
            elif country in ['IRN', 'VEN', 'IRQ']:  # åˆ¶è£å’Œå±æœºå½±å“
                time_trend = 0.15 * time_base * np.sin(2 * np.pi * time_base)
            else:  # å…¶ä»–å›½å®¶ç¼“æ…¢å˜åŒ–
                time_trend = 0.02 * time_base * (1 + 0.5 * np.sin(np.pi * time_base))
            
            # ç‰¹å®šäº‹ä»¶å†²å‡»ï¼ˆå¢åŠ æ—¶é—´å¼‚è´¨æ€§ï¼‰
            event_shock = 0
            if year == 2014 and country in ['RUS', 'IRN']:  # åˆ¶è£å¼€å§‹
                event_shock = 0.1
            elif year == 2016 and country == 'USA':  # é¡µå²©æ²¹é©å‘½
                event_shock = -0.08
            elif year == 2018 and country in ['IRN', 'VEN']:  # åˆ¶è£åŠ å‰§
                event_shock = 0.15
            elif year == 2020:  # ç–«æƒ…å½±å“ï¼ˆå…¨çƒï¼‰
                event_shock = 0.05 * (1 + np.random.normal(0, 0.5))
            elif year >= 2022 and country == 'RUS':  # åœ°ç¼˜æ”¿æ²»å†²å‡»
                event_shock = 0.2
            
            # éšæœºæ³¢åŠ¨ï¼ˆå›½å®¶ç‰¹å®šçš„æ³¢åŠ¨æ€§ï¼‰
            volatility = {
                'USA': 0.02, 'CHN': 0.03, 'RUS': 0.08, 'SAU': 0.06,
                'DEU': 0.02, 'JPN': 0.025, 'CAN': 0.03, 'GBR': 0.025,
                'FRA': 0.02, 'IND': 0.04, 'BRA': 0.05, 'MEX': 0.04,
                'AUS': 0.03, 'NOR': 0.05, 'ARE': 0.07, 'KWT': 0.06,
                'IRN': 0.12, 'IRQ': 0.15, 'VEN': 0.18, 'QAT': 0.05
            }.get(country, 0.04)
            
            random_shock = np.random.normal(0, volatility)
            
            # æœ€ç»ˆDLIå¾—åˆ†ï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿå˜å¼‚ï¼‰
            dli_score = base_dli + network_effect + time_trend + event_shock + random_shock
            dli_score = np.clip(dli_score, 0.05, 0.95)  # é¿å…æå€¼
            
            dli_data.append({
                'year': year,
                'country': country,
                'dli_score': dli_score
            })
    
    dli_df = pd.DataFrame(dli_data)
    logger.info(f"   DLIæ•°æ®: {dli_df.shape}")
    
    return dli_df

def main():
    """ä¸»æ¼”ç¤ºæµç¨‹"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                           â•‘
â•‘    ğŸ¯ å› æœéªŒè¯åˆ†æ - å…³é”®äº§å‡ºç”Ÿæˆæ¼”ç¤º                       â•‘
â•‘   Causal Validation Analysis - Key Deliverables Demo    â•‘
â•‘                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    try:
        # 1. åˆ›å»ºæ¼”ç¤ºæ•°æ®
        logger.info("=" * 50)
        logger.info("ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ¼”ç¤ºæ•°æ®")
        logger.info("=" * 50)
        
        networks = create_demo_networks()
        dli_data = create_demo_dli_data(networks)
        
        # 2. ç”Ÿæˆç½‘ç»œéŸ§æ€§æ•°æ®åº“
        logger.info("\n" + "=" * 50)
        logger.info("ç¬¬äºŒæ­¥ï¼šç”Ÿæˆç½‘ç»œéŸ§æ€§æ•°æ®åº“")
        logger.info("=" * 50)
        
        resilience_db = generate_resilience_database(
            networks, 
            output_path="outputs/network_resilience.csv",
            countries=['USA', 'CHN', 'RUS', 'SAU', 'DEU', 'JPN']  # é‡ç‚¹å›½å®¶
        )
        
        print(f"âœ… ç½‘ç»œéŸ§æ€§æ•°æ®åº“å·²ç”Ÿæˆ: outputs/network_resilience.csv")
        print(f"   æ•°æ®ç»´åº¦: {resilience_db.shape}")
        print(f"   æ—¶é—´è·¨åº¦: {resilience_db['year'].min()}-{resilience_db['year'].max()}")
        
        # 3. è¿è¡Œå› æœéªŒè¯åˆ†æ
        logger.info("\n" + "=" * 50)  
        logger.info("ç¬¬ä¸‰æ­¥ï¼šå› æœéªŒè¯åˆ†æ")
        logger.info("=" * 50)
        
        causal_results = run_causal_validation(
            resilience_db,
            dli_data,
            output_dir="outputs"
        )
        
        # 4. ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š
        logger.info("\n" + "=" * 50)
        logger.info("ç¬¬å››æ­¥ï¼šç”Ÿæˆå­¦æœ¯æŠ¥å‘Š")
        logger.info("=" * 50)
        
        from main import CausalValidationPipeline
        pipeline = CausalValidationPipeline(output_dir="outputs")
        pipeline.resilience_data = resilience_db
        pipeline.dli_data = dli_data
        pipeline.causal_results = causal_results
        
        report_file = pipeline.generate_academic_report()
        
        # 5. æ€»ç»“è¾“å‡º
        print("\n" + "ğŸ‰" + "=" * 50 + "ğŸ‰")
        print("    å…³é”®äº§å‡ºç”Ÿæˆå®Œæˆï¼")
        print("  Key Deliverables Generated Successfully!")
        print("=" * 52)
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   1. ç½‘ç»œéŸ§æ€§æ•°æ®åº“: outputs/network_resilience.csv")
        print(f"   2. å› æœéªŒè¯æŠ¥å‘Š: {Path(report_file).name}")
        print(f"   3. å›å½’ç»“æœè¡¨æ ¼: outputs/regression_results.csv")
        print(f"   4. åŸå§‹åˆ†æç»“æœ: outputs/causal_validation_results.json")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        overall_assessment = causal_results.get('overall_assessment', {})
        evidence_strength = overall_assessment.get('causal_evidence_strength', 'unknown')
        
        print(f"\nğŸ¯ åˆ†ææ‘˜è¦:")
        print(f"   â€¢ å› æœè¯æ®å¼ºåº¦: {evidence_strength.upper()}")
        print(f"   â€¢ ç»Ÿè®¡æ˜¾è‘—æ€§: {'é€šè¿‡' if overall_assessment.get('statistical_significance') else 'æœªé€šè¿‡'}")
        print(f"   â€¢ åˆ†æäº† {len(networks)} å¹´ç½‘ç»œæ•°æ®")
        print(f"   â€¢ ç”Ÿæˆäº† {len(resilience_db)} ä¸ªéŸ§æ€§è§‚æµ‹")
        
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥: æŸ¥çœ‹ outputs/ ç›®å½•ä¸­çš„æ‰€æœ‰åˆ†æç»“æœ")
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()