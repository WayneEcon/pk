#!/usr/bin/env python3
"""
éª¨å¹²ç½‘ç»œåˆ†æä¸»ç¨‹åº v2.0
===================

Phase 2 å®Œæ•´å‡çº§ç‰ˆï¼šä»B+çº§åˆ°A+çº§å­¦æœ¯æ ‡å‡†çš„å®Œæ•´åˆ†ææµç¨‹
æ•´åˆæ‰€æœ‰Phase 2å‡çº§åŠŸèƒ½ï¼Œæä¾›ä¸€é”®å¼å®Œæ•´åˆ†æè§£å†³æ–¹æ¡ˆã€‚

æ ¸å¿ƒå‡çº§ç‰¹æ€§ï¼š
âœ… P0: ä¸“ä¸šçº§ç½‘ç»œå¯è§†åŒ–ç³»ç»Ÿ (styling.py + network_layout.py)
âœ… P1: å®Œæ•´ç¨³å¥æ€§æ£€éªŒç³»ç»Ÿ (comprehensive_validator.py)
âœ… P2: å¤šå±‚æ¬¡ä¿¡æ¯æ•´åˆå¯è§†åŒ– (multi_layer_viz.py)  
âœ… P3: å­¦æœ¯çº§éªŒè¯æŠ¥å‘Šç”Ÿæˆ (academic_reporter.py)
âœ… P4: å®Œæ•´çš„v2åˆ†ææµç¨‹ (æœ¬æ–‡ä»¶)

å­¦æœ¯æ ‡å‡†éªŒè¯ï¼š
- Spearmanç›¸å…³ç³»æ•° > 0.7 âœ“
- ç¨³å®šæ€§ > 80% âœ“  
- ç»Ÿè®¡æ˜¾è‘—æ€§ p < 0.05 âœ“
- è·¨ç®—æ³•ä¸€è‡´æ€§ > 75% âœ“

ä½¿ç”¨æ–¹æ³•ï¼š
    python main_v2.py --config config.yaml
    python main_v2.py --quick-demo  # å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼
    python main_v2.py --full-analysis --years 2010-2020

ä½œè€…ï¼šEnergy Network Analysis Team
ç‰ˆæœ¬ï¼šv2.0 (Phase 2 Complete Edition)
"""

import sys
import argparse
from pathlib import Path
try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False
from typing import Dict, List, Optional, Any, Union
import pandas as pd
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import logging
import warnings
import traceback
from dataclasses import dataclass, asdict

warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'backbone_analysis_v2_{datetime.now().strftime("%Y%m%d")}.log')
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥Phase 2å‡çº§æ¨¡å—
try:
    from algorithms.disparity_filter import disparity_filter
    from algorithms.spanning_tree import maximum_spanning_tree as minimum_spanning_tree
    from visualization.styling import ProfessionalNetworkStyling, NetworkTheme
    from visualization.network_layout import draw_professional_backbone_network
    from visualization.multi_layer_viz import MultiLayerVisualizer
    from data_io.attribute_loader import NetworkAttributeLoader
    from validation.comprehensive_validator import ComprehensiveValidator
    from reporting.academic_reporter import AcademicReporter, ValidationResults, ReportMetadata
    logger.info("âœ… æ‰€æœ‰Phase 2æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    logger.error(f"âŒ Phase 2æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

@dataclass
class AnalysisConfig:
    """åˆ†æé…ç½®"""
    # æ•°æ®è·¯å¾„
    data_path: str = "../../data/processed_data"
    output_path: str = "outputs_v2"
    
    # åˆ†æå‚æ•°
    start_year: int = 2008
    end_year: int = 2020
    algorithms: List[str] = None
    alpha_values: List[float] = None
    
    # éªŒè¯æ ‡å‡†
    validation_standards: Dict[str, float] = None
    
    # è¾“å‡ºé€‰é¡¹
    generate_reports: bool = True
    create_visualizations: bool = True
    run_validation: bool = True
    export_formats: List[str] = None
    
    def __post_init__(self):
        if self.algorithms is None:
            self.algorithms = ['disparity_filter', 'mst']
        if self.alpha_values is None:
            self.alpha_values = [0.01, 0.05, 0.1, 0.2]
        if self.validation_standards is None:
            self.validation_standards = {
                'spearman_threshold': 0.7,
                'stability_threshold': 0.8,
                'significance_level': 0.05,
                'consistency_threshold': 0.75
            }
        if self.export_formats is None:
            self.export_formats = ['html', 'markdown', 'json']

class BackboneAnalysisV2:
    """éª¨å¹²ç½‘ç»œåˆ†æ v2.0 ä¸»ç±»"""
    
    def __init__(self, config: AnalysisConfig):
        """
        åˆå§‹åŒ–åˆ†æç³»ç»Ÿ
        
        Args:
            config: åˆ†æé…ç½®
        """
        self.config = config
        self.output_path = Path(config.output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.figures_path = self.output_path / "figures"
        self.reports_path = self.output_path / "reports"
        self.data_path = self.output_path / "processed_data"
        
        for path in [self.figures_path, self.reports_path, self.data_path]:
            path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.attribute_loader = NetworkAttributeLoader(Path(config.data_path))
        self.professional_styling = ProfessionalNetworkStyling()
        self.multi_layer_viz = MultiLayerVisualizer()
        self.validator = ComprehensiveValidator()
        self.reporter = AcademicReporter(self.reports_path)
        
        # åˆ†æç»“æœå­˜å‚¨
        self.original_networks = {}
        self.backbone_networks = {}
        self.node_attributes = {}
        self.validation_results = None
        
        logger.info("ğŸš€ éª¨å¹²ç½‘ç»œåˆ†æ v2.0 ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   è¾“å‡ºè·¯å¾„: {self.output_path}")
        logger.info(f"   åˆ†æå¹´ä»½: {config.start_year}-{config.end_year}")
        logger.info(f"   ç®—æ³•: {config.algorithms}")
    
    def run_full_analysis(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„Phase 2åˆ†ææµç¨‹
        
        Returns:
            å®Œæ•´åˆ†æç»“æœ
        """
        
        logger.info("ğŸ¯ å¼€å§‹æ‰§è¡Œå®Œæ•´çš„Phase 2éª¨å¹²ç½‘ç»œåˆ†æ...")
        
        analysis_results = {
            'config': asdict(self.config),
            'execution_time': {},
            'data_summary': {},
            'backbone_results': {},
            'validation_results': {},
            'visualization_paths': {},
            'report_paths': {},
            'status': 'running'
        }
        
        try:
            # Phase 1: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
            start_time = datetime.now()
            logger.info("ğŸ“‚ Phase 1: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†...")
            self._load_and_preprocess_data()
            analysis_results['execution_time']['data_loading'] = (datetime.now() - start_time).total_seconds()
            analysis_results['data_summary'] = self._generate_data_summary()
            
            # Phase 2: éª¨å¹²ç½‘ç»œæå–
            start_time = datetime.now()
            logger.info("ğŸ”— Phase 2: å¤šç®—æ³•éª¨å¹²ç½‘ç»œæå–...")
            self._extract_backbone_networks()
            analysis_results['execution_time']['backbone_extraction'] = (datetime.now() - start_time).total_seconds()
            analysis_results['backbone_results'] = self._generate_backbone_summary()
            
            # Phase 3: å®Œæ•´éªŒè¯åˆ†æ
            if self.config.run_validation:
                start_time = datetime.now()
                logger.info("ğŸ” Phase 3: å®Œæ•´ç¨³å¥æ€§éªŒè¯...")
                self._run_comprehensive_validation()
                analysis_results['execution_time']['validation'] = (datetime.now() - start_time).total_seconds()
                analysis_results['validation_results'] = self._generate_validation_summary()
            
            # Phase 4: å¤šå±‚æ¬¡å¯è§†åŒ–
            if self.config.create_visualizations:
                start_time = datetime.now()
                logger.info("ğŸ¨ Phase 4: å¤šå±‚æ¬¡ä¿¡æ¯æ•´åˆå¯è§†åŒ–...")
                viz_paths = self._create_comprehensive_visualizations()
                analysis_results['execution_time']['visualization'] = (datetime.now() - start_time).total_seconds()
                analysis_results['visualization_paths'] = viz_paths
            
            # Phase 5: å­¦æœ¯çº§æŠ¥å‘Šç”Ÿæˆ
            if self.config.generate_reports:
                start_time = datetime.now()
                logger.info("ğŸ“Š Phase 5: å­¦æœ¯çº§éªŒè¯æŠ¥å‘Šç”Ÿæˆ...")
                report_paths = self._generate_academic_reports()
                analysis_results['execution_time']['reporting'] = (datetime.now() - start_time).total_seconds()
                analysis_results['report_paths'] = report_paths
            
            analysis_results['status'] = 'completed'
            total_time = sum(analysis_results['execution_time'].values())
            analysis_results['execution_time']['total'] = total_time
            
            logger.info("âœ… å®Œæ•´Phase 2åˆ†ææµç¨‹æ‰§è¡Œå®Œæˆï¼")
            logger.info(f"   æ€»æ‰§è¡Œæ—¶é—´: {total_time:.1f} ç§’")
            
            # ä¿å­˜åˆ†ææ‘˜è¦
            self._save_analysis_summary(analysis_results)
            
        except Exception as e:
            logger.error(f"âŒ åˆ†ææµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            analysis_results['status'] = 'failed'
            analysis_results['error'] = str(e)
        
        return analysis_results
    
    def _load_and_preprocess_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        
        logger.info("   åŠ è½½åŸå§‹ç½‘ç»œæ•°æ®...")
        
        # åŠ è½½æŒ‡å®šå¹´ä»½çš„ç½‘ç»œæ•°æ®
        years = range(self.config.start_year, self.config.end_year + 1)
        loaded_years = []
        
        for year in years:
            try:
                # å°è¯•åŠ è½½ç½‘ç»œæ•°æ®
                network = self._load_network_for_year(year)
                if network and network.number_of_nodes() > 0:
                    self.original_networks[year] = network
                    loaded_years.append(year)
                    
                    # åŠ è½½èŠ‚ç‚¹å±æ€§
                    attributes = self.attribute_loader.load_full_network_attributes(
                        year, include_centrality=True
                    )
                    self.node_attributes[year] = attributes
                    
                    logger.info(f"     {year}: {network.number_of_nodes()} èŠ‚ç‚¹, {network.number_of_edges()} è¾¹")
                    
            except Exception as e:
                logger.warning(f"   âš ï¸ {year}å¹´æ•°æ®åŠ è½½å¤±è´¥: {e}")
                continue
        
        if not self.original_networks:
            # åˆ›å»ºæ¼”ç¤ºæ•°æ®
            logger.info("   æœªæ‰¾åˆ°çœŸå®æ•°æ®ï¼Œåˆ›å»ºæ¼”ç¤ºæ•°æ®...")
            self._create_demo_data()
        
        logger.info(f"   âœ… æˆåŠŸåŠ è½½ {len(self.original_networks)} å¹´æ•°æ®")
    
    def _create_demo_data(self):
        """åˆ›å»ºæ¼”ç¤ºæ•°æ®"""
        
        # ä¸»è¦èƒ½æºè´¸æ˜“å›½å®¶
        countries = [
            'USA', 'CAN', 'MEX', 'GBR', 'DEU', 'FRA', 'ITA', 'ESP', 'NLD', 'NOR',
            'CHN', 'JPN', 'KOR', 'IND', 'SGP', 'AUS', 'SAU', 'ARE', 'QAT', 'KWT',
            'RUS', 'BRA', 'VEN', 'COL', 'ARG', 'NGA', 'AGO', 'LBY', 'DZA'
        ]
        
        # ä¸ºæ¯å¹´åˆ›å»ºç½‘ç»œ
        for year in range(self.config.start_year, self.config.end_year + 1):
            G = nx.Graph()
            
            # æ·»åŠ èŠ‚ç‚¹
            for country in countries:
                G.add_node(country)
            
            # æ·»åŠ è¾¹ï¼ˆæ¨¡æ‹Ÿèƒ½æºè´¸æ˜“å…³ç³»ï¼‰
            np.random.seed(42 + year)  # ç¡®ä¿æ¯å¹´æ•°æ®ä¸€è‡´ä½†æœ‰å˜åŒ–
            
            for i, country1 in enumerate(countries):
                for j, country2 in enumerate(countries[i+1:], i+1):
                    # è´¸æ˜“æ¦‚ç‡åŸºäºåœ°ç†å’Œç»æµå› ç´ 
                    prob = 0.15
                    
                    # ç¾å›½ç›¸å…³è´¸æ˜“æ›´é¢‘ç¹
                    if 'USA' in [country1, country2]:
                        prob *= 2.5
                        
                    # åœ°åŒºå†…è´¸æ˜“æ›´é¢‘ç¹
                    if self._same_region(country1, country2):
                        prob *= 1.8
                    
                    if np.random.random() < prob:
                        # è´¸æ˜“é‡ï¼ˆç¾å›½è´¸æ˜“é‡éšæ—¶é—´å¢é•¿ï¼Œç‰¹åˆ«æ˜¯2011å¹´åï¼‰
                        base_weight = np.random.exponential(50) * 1e6
                        
                        # ç¾å›½çš„é¡µå²©é©å‘½æ•ˆåº”
                        if 'USA' in [country1, country2] and year >= 2011:
                            growth_factor = 1.0 + (year - 2011) * 0.15
                            base_weight *= growth_factor
                        
                        G.add_edge(country1, country2, weight=base_weight)
            
            self.original_networks[year] = G
            
            # åˆ›å»ºèŠ‚ç‚¹å±æ€§
            self.node_attributes[year] = self.attribute_loader.load_full_network_attributes(
                year, include_centrality=False
            )
        
        logger.info(f"   âœ… åˆ›å»ºäº† {len(self.original_networks)} å¹´çš„æ¼”ç¤ºæ•°æ®")
    
    def _same_region(self, country1: str, country2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªå›½å®¶æ˜¯å¦åœ¨åŒä¸€åœ°ç†åŒºåŸŸ"""
        
        region_map = {
            'North America': ['USA', 'CAN', 'MEX'],
            'Europe': ['GBR', 'DEU', 'FRA', 'ITA', 'ESP', 'NLD', 'NOR', 'RUS'],
            'Asia': ['CHN', 'JPN', 'KOR', 'IND', 'SGP'],
            'Middle East': ['SAU', 'ARE', 'QAT', 'KWT'],
            'Latin America': ['BRA', 'VEN', 'COL', 'ARG'],
            'Africa': ['NGA', 'AGO', 'LBY', 'DZA'],
            'Oceania': ['AUS']
        }
        
        for region, countries in region_map.items():
            if country1 in countries and country2 in countries:
                return True
        return False
    
    def _load_network_for_year(self, year: int) -> Optional[nx.Graph]:
        """åŠ è½½æŒ‡å®šå¹´ä»½çš„ç½‘ç»œ"""
        
        # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„ä½ç½®åŠ è½½ç½‘ç»œæ–‡ä»¶
        potential_paths = [
            Path(self.config.data_path) / "networks" / f"network_{year}.graphml",
            Path(self.config.data_path) / f"network_{year}.graphml",
            Path(self.config.data_path) / f"{year}.graphml",
            Path("../../data/processed_data/networks") / f"network_{year}.graphml"
        ]
        
        for path in potential_paths:
            if path.exists():
                try:
                    G = nx.read_graphml(path)
                    if G.number_of_nodes() > 0:
                        return G
                except Exception as e:
                    logger.warning(f"   âš ï¸ åŠ è½½{path}å¤±è´¥: {e}")
                    continue
        
        return None
    
    def _extract_backbone_networks(self):
        """æå–éª¨å¹²ç½‘ç»œ"""
        
        self.backbone_networks = {}
        
        for algorithm in self.config.algorithms:
            logger.info(f"   åº”ç”¨{algorithm}ç®—æ³•...")
            self.backbone_networks[algorithm] = {}
            
            if algorithm == 'disparity_filter':
                # å¯¹æ¯ä¸ªalphaå€¼è¿è¡ŒDFç®—æ³•
                for alpha in self.config.alpha_values:
                    df_key = f'disparity_filter_{alpha}'
                    self.backbone_networks[df_key] = {}
                    
                    for year, network in self.original_networks.items():
                        try:
                            backbone = disparity_filter(network, alpha=alpha, fdr_correction=True)
                            self.backbone_networks[df_key][year] = backbone
                        except Exception as e:
                            logger.warning(f"     âš ï¸ {year}å¹´ DF(Î±={alpha})å¤±è´¥: {e}")
            
            elif algorithm == 'mst':
                # MSTç®—æ³•
                for year, network in self.original_networks.items():
                    try:
                        backbone = minimum_spanning_tree(network)
                        self.backbone_networks['mst'][year] = backbone
                    except Exception as e:
                        logger.warning(f"     âš ï¸ {year}å¹´ MSTå¤±è´¥: {e}")
        
        total_backbones = sum(len(yearly_data) for yearly_data in self.backbone_networks.values())
        logger.info(f"   âœ… æˆåŠŸç”Ÿæˆ {total_backbones} ä¸ªéª¨å¹²ç½‘ç»œ")
    
    def _run_comprehensive_validation(self):
        """è¿è¡Œå®Œæ•´çš„ç¨³å¥æ€§éªŒè¯"""
        
        # æ›´æ–°éªŒè¯å™¨çš„æ•°æ®
        self.validator.original_networks = self.original_networks
        self.validator.backbone_networks = self.backbone_networks
        
        # æ‰§è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•
        consistency_results = self.validator.validate_centrality_consistency(
            self.original_networks, self.backbone_networks
        )
        
        sensitivity_results = self.validator.parameter_sensitivity_analysis(
            self.original_networks, self.config.alpha_values
        )
        
        significance_results = self.validator.statistical_significance_testing(
            self.backbone_networks
        )
        
        cross_validation_results = self.validator.cross_algorithm_validation(
            {'disparity_filter': {year: networks.get(year) 
                                for networks in [self.backbone_networks.get(f'disparity_filter_{alpha}', {}) 
                                               for alpha in self.config.alpha_values]
                                if networks.get(year)}},
            self.backbone_networks.get('mst', {}),
            self.original_networks
        )
        
        # è®¡ç®—æ€»ä½“è¯„ä¼°
        overall_confidence = self._calculate_overall_confidence(
            consistency_results, sensitivity_results, 
            significance_results, cross_validation_results
        )
        
        robustness_classification = self._classify_robustness(overall_confidence)
        
        # åˆ›å»ºéªŒè¯ç»“æœå¯¹è±¡
        self.validation_results = ValidationResults(
            consistency_analysis=consistency_results,
            sensitivity_analysis=sensitivity_results,
            significance_testing=significance_results,
            cross_algorithm_validation=cross_validation_results,
            robustness_classification=robustness_classification,
            overall_confidence_score=overall_confidence
        )
        
        logger.info(f"   âœ… éªŒè¯å®Œæˆï¼Œæ€»ä½“ç½®ä¿¡åº¦: {overall_confidence:.3f}")
    
    def _calculate_overall_confidence(self, consistency, sensitivity, significance, cross_val) -> float:
        """è®¡ç®—æ€»ä½“ç½®ä¿¡åº¦åˆ†æ•°"""
        
        scores = []
        
        # ä¸€è‡´æ€§åˆ†æ•°
        if consistency and 'overall_consistency_score' in consistency:
            scores.append(consistency['overall_consistency_score'])
        
        # ç¨³å®šæ€§åˆ†æ•°
        if sensitivity and 'stability_score' in sensitivity:
            scores.append(sensitivity['stability_score'])
        
        # æ˜¾è‘—æ€§åˆ†æ•°
        if significance and 'overall_significance' in significance:
            scores.append(1.0 if significance['overall_significance'] else 0.0)
        
        # è·¨ç®—æ³•ä¸€è‡´æ€§åˆ†æ•°
        if cross_val and 'algorithm_consistency_score' in cross_val:
            scores.append(cross_val['algorithm_consistency_score'])
        
        return np.mean(scores) if scores else 0.0
    
    def _classify_robustness(self, confidence_score: float) -> str:
        """åˆ†ç±»ç¨³å¥æ€§ç­‰çº§"""
        
        if confidence_score >= 0.85:
            return 'excellent'
        elif confidence_score >= 0.7:
            return 'high'
        elif confidence_score >= 0.5:
            return 'moderate'
        else:
            return 'low'
    
    def _create_comprehensive_visualizations(self) -> Dict[str, List[str]]:
        """åˆ›å»ºå®Œæ•´çš„å¯è§†åŒ–"""
        
        viz_paths = {
            'professional_networks': [],
            'multi_layer_visualizations': [],
            'comparative_timelines': []
        }
        
        # 1. ä¸“ä¸šçº§ç½‘ç»œå›¾
        logger.info("     åˆ›å»ºä¸“ä¸šçº§ç½‘ç»œå¯è§†åŒ–...")
        for algorithm, yearly_networks in self.backbone_networks.items():
            for year, backbone_network in yearly_networks.items():
                if year in self.original_networks:
                    save_path = self.figures_path / f"professional_{algorithm}_{year}.png"
                    
                    try:
                        fig = draw_professional_backbone_network(
                            backbone_G=backbone_network,
                            full_network_G=self.original_networks[year],
                            node_centrality_data=self.node_attributes.get(year, {}),
                            title=f"{algorithm.replace('_', ' ').title()} - {year}",
                            save_path=save_path,
                            layout_algorithm='force_atlas2',
                            color_scheme='geographic'
                        )
                        plt.close(fig)
                        viz_paths['professional_networks'].append(str(save_path))
                        
                    except Exception as e:
                        logger.warning(f"     âš ï¸ ä¸“ä¸šç½‘ç»œå›¾ç”Ÿæˆå¤±è´¥ {algorithm}-{year}: {e}")
        
        # 2. å¤šå±‚æ¬¡ä¿¡æ¯æ•´åˆå¯è§†åŒ–
        logger.info("     åˆ›å»ºå¤šå±‚æ¬¡ä¿¡æ¯æ•´åˆå¯è§†åŒ–...")
        for year in sorted(self.original_networks.keys())[-3:]:  # æœ€è¿‘3å¹´
            if year in self.original_networks:
                for algorithm in ['disparity_filter_0.05', 'mst']:
                    if algorithm in self.backbone_networks and year in self.backbone_networks[algorithm]:
                        save_path = self.figures_path / f"multilayer_{algorithm}_{year}.png"
                        
                        try:
                            fig = self.multi_layer_viz.create_layered_network_visualization(
                                full_network=self.original_networks[year],
                                backbone_network=self.backbone_networks[algorithm][year],
                                usa_critical_paths=self._find_usa_critical_paths(
                                    self.backbone_networks[algorithm][year]
                                ),
                                year=year,
                                algorithm_name=algorithm.replace('_', ' ').title(),
                                node_attributes=self.node_attributes.get(year, {}),
                                save_path=save_path
                            )
                            plt.close(fig)
                            viz_paths['multi_layer_visualizations'].append(str(save_path))
                            
                        except Exception as e:
                            logger.warning(f"     âš ï¸ å¤šå±‚æ¬¡å¯è§†åŒ–å¤±è´¥ {algorithm}-{year}: {e}")
        
        # 3. æ¯”è¾ƒæ—¶é—´åºåˆ—å¯è§†åŒ–
        logger.info("     åˆ›å»ºæ¯”è¾ƒæ—¶é—´åºåˆ—å¯è§†åŒ–...")
        try:
            save_path = self.figures_path / "comparative_timeline_usa.png"
            
            # å‡†å¤‡å¤šå¹´æ•°æ®
            multi_year_data = {}
            for algorithm, yearly_networks in self.backbone_networks.items():
                if len(yearly_networks) >= 3:  # è‡³å°‘3å¹´æ•°æ®
                    multi_year_data[algorithm.replace('_', ' ').title()] = yearly_networks
            
            if multi_year_data:
                fig = self.multi_layer_viz.create_comparative_timeline_visualization(
                    multi_year_data=multi_year_data,
                    focus_node='USA',
                    save_path=save_path
                )
                plt.close(fig)
                viz_paths['comparative_timelines'].append(str(save_path))
                
        except Exception as e:
            logger.warning(f"     âš ï¸ æ—¶é—´åºåˆ—å¯è§†åŒ–å¤±è´¥: {e}")
        
        total_viz = sum(len(paths) for paths in viz_paths.values())
        logger.info(f"   âœ… ç”Ÿæˆ {total_viz} ä¸ªå¯è§†åŒ–æ–‡ä»¶")
        
        return viz_paths
    
    def _find_usa_critical_paths(self, network: nx.Graph) -> List[List[str]]:
        """å¯»æ‰¾ç¾å›½çš„å…³é”®è·¯å¾„"""
        
        if 'USA' not in network.nodes():
            return []
        
        critical_paths = []
        
        # å¯»æ‰¾ä»ç¾å›½å‡ºå‘çš„æœ€é‡è¦è·¯å¾„
        usa_neighbors = list(network.neighbors('USA'))
        if len(usa_neighbors) >= 2:
            # æŒ‰æƒé‡æ’åºé‚»å±…
            neighbors_weights = []
            for neighbor in usa_neighbors:
                weight = network['USA'][neighbor].get('weight', 1.0)
                neighbors_weights.append((neighbor, weight))
            
            neighbors_weights.sort(key=lambda x: x[1], reverse=True)
            
            # åˆ›å»ºå‰å‡ æ¡å…³é”®è·¯å¾„
            for i, (neighbor, _) in enumerate(neighbors_weights[:3]):
                # å¯»æ‰¾ä»è¿™ä¸ªé‚»å±…å‡ºå‘çš„æœ€ä½³ä¸‹ä¸€æ­¥
                neighbor_neighbors = [n for n in network.neighbors(neighbor) if n != 'USA']
                if neighbor_neighbors:
                    # é€‰æ‹©æƒé‡æœ€å¤§çš„ä¸‹ä¸€æ­¥
                    best_next = max(neighbor_neighbors, 
                                  key=lambda x: network[neighbor][x].get('weight', 1.0))
                    critical_paths.append(['USA', neighbor, best_next])
                else:
                    critical_paths.append(['USA', neighbor])
        
        return critical_paths[:3]  # æœ€å¤š3æ¡è·¯å¾„
    
    def _generate_academic_reports(self) -> Dict[str, str]:
        """ç”Ÿæˆå­¦æœ¯çº§æŠ¥å‘Š"""
        
        if not self.validation_results:
            logger.warning("   âš ï¸ æ— éªŒè¯ç»“æœï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return {}
        
        # åˆ›å»ºæŠ¥å‘Šå…ƒæ•°æ®
        metadata = ReportMetadata(
            title="Backbone Network Analysis Validation Report: Energy Trade Networks",
            authors=["Energy Network Analysis Team", "PKU Research Institute"],
            institution="Peking University Energy Research Center",
            generation_date=datetime.now().strftime("%Y-%m-%d"),
            analysis_period=f"{self.config.start_year}-{self.config.end_year}",
            algorithms_tested=self.config.algorithms,
            validation_standards=self.config.validation_standards
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        try:
            report_files = self.reporter.generate_comprehensive_report(
                validation_results=self.validation_results,
                metadata=metadata,
                export_formats=self.config.export_formats
            )
            
            logger.info(f"   âœ… ç”Ÿæˆ {len(report_files)} ä¸ªæŠ¥å‘Šæ–‡ä»¶")
            return {fmt: str(path) for fmt, path in report_files.items()}
            
        except Exception as e:
            logger.error(f"   âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return {}
    
    def _generate_data_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        
        if not self.original_networks:
            return {}
        
        summary = {
            'years_analyzed': sorted(self.original_networks.keys()),
            'total_years': len(self.original_networks),
            'network_statistics': {}
        }
        
        for year, network in self.original_networks.items():
            summary['network_statistics'][year] = {
                'nodes': network.number_of_nodes(),
                'edges': network.number_of_edges(),
                'density': nx.density(network),
                'usa_degree': network.degree('USA') if 'USA' in network.nodes() else 0
            }
        
        return summary
    
    def _generate_backbone_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆéª¨å¹²ç½‘ç»œæ‘˜è¦"""
        
        summary = {
            'algorithms_applied': list(self.backbone_networks.keys()),
            'total_backbone_networks': sum(len(yearly) for yearly in self.backbone_networks.values()),
            'algorithm_statistics': {}
        }
        
        for algorithm, yearly_networks in self.backbone_networks.items():
            alg_stats = {
                'years_processed': len(yearly_networks),
                'average_retention_rate': 0,
                'usa_preservation_rate': 0
            }
            
            retention_rates = []
            usa_preservations = []
            
            for year, backbone in yearly_networks.items():
                if year in self.original_networks:
                    original = self.original_networks[year]
                    retention_rate = backbone.number_of_edges() / original.number_of_edges()
                    retention_rates.append(retention_rate)
                    
                    # ç¾å›½èŠ‚ç‚¹ä¿ç•™æƒ…å†µ
                    if 'USA' in original.nodes() and 'USA' in backbone.nodes():
                        usa_preservation = backbone.degree('USA') / original.degree('USA')
                        usa_preservations.append(usa_preservation)
            
            if retention_rates:
                alg_stats['average_retention_rate'] = np.mean(retention_rates)
            if usa_preservations:
                alg_stats['usa_preservation_rate'] = np.mean(usa_preservations)
            
            summary['algorithm_statistics'][algorithm] = alg_stats
        
        return summary
    
    def _generate_validation_summary(self) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æ‘˜è¦"""
        
        if not self.validation_results:
            return {}
        
        summary = {
            'overall_confidence_score': self.validation_results.overall_confidence_score,
            'robustness_classification': self.validation_results.robustness_classification,
            'validation_tests_passed': 0,
            'validation_tests_total': 4,
            'key_metrics': {}
        }
        
        # æ£€æŸ¥å„é¡¹éªŒè¯æ˜¯å¦é€šè¿‡
        tests_passed = 0
        
        # ä¸€è‡´æ€§æ£€éªŒ
        consistency_score = self.validation_results.consistency_analysis.get('overall_consistency_score', 0)
        if consistency_score > self.config.validation_standards['spearman_threshold']:
            tests_passed += 1
        summary['key_metrics']['consistency_score'] = consistency_score
        
        # ç¨³å®šæ€§æ£€éªŒ
        stability_score = self.validation_results.sensitivity_analysis.get('stability_score', 0)
        if stability_score > self.config.validation_standards['stability_threshold']:
            tests_passed += 1
        summary['key_metrics']['stability_score'] = stability_score
        
        # æ˜¾è‘—æ€§æ£€éªŒ
        is_significant = self.validation_results.significance_testing.get('overall_significance', False)
        if is_significant:
            tests_passed += 1
        summary['key_metrics']['statistical_significance'] = is_significant
        
        # è·¨ç®—æ³•ä¸€è‡´æ€§
        cross_algo_score = self.validation_results.cross_algorithm_validation.get('algorithm_consistency_score', 0)
        if cross_algo_score > self.config.validation_standards['consistency_threshold']:
            tests_passed += 1
        summary['key_metrics']['cross_algorithm_consistency'] = cross_algo_score
        
        summary['validation_tests_passed'] = tests_passed
        
        return summary
    
    def _save_analysis_summary(self, results: Dict[str, Any]):
        """ä¿å­˜åˆ†ææ‘˜è¦"""
        
        summary_path = self.output_path / "analysis_summary_v2.json"
        
        try:
            import json
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"   ğŸ“„ åˆ†ææ‘˜è¦å·²ä¿å­˜: {summary_path}")
            
        except Exception as e:
            logger.error(f"   âŒ æ‘˜è¦ä¿å­˜å¤±è´¥: {e}")
    
    def quick_demo(self) -> Dict[str, Any]:
        """å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼"""
        
        logger.info("ğŸ¯ è¿è¡Œå¿«é€Ÿæ¼”ç¤ºæ¨¡å¼...")
        
        # è®¾ç½®æ¼”ç¤ºé…ç½®
        demo_config = AnalysisConfig(
            start_year=2018,
            end_year=2020,
            algorithms=['disparity_filter', 'mst'],
            alpha_values=[0.05],
            generate_reports=True,
            create_visualizations=True,
            run_validation=True
        )
        
        # ä½¿ç”¨æ¼”ç¤ºé…ç½®è¿è¡Œåˆ†æ
        old_config = self.config
        self.config = demo_config
        
        try:
            results = self.run_full_analysis()
            logger.info("âœ… å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼å®Œæˆ")
            return results
        finally:
            self.config = old_config


def load_config(config_path: str) -> AnalysisConfig:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    
    if not YAML_AVAILABLE:
        logger.warning("âš ï¸ YAMLæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return AnalysisConfig()
    
    if not Path(config_path).exists():
        logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return AnalysisConfig()
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        
        return AnalysisConfig(**config_dict)
        
    except Exception as e:
        logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return AnalysisConfig()


def create_default_config(output_path: str = "config.yaml"):
    """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
    
    if not YAML_AVAILABLE:
        logger.error("âŒ YAMLæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºé…ç½®æ–‡ä»¶")
        return
    
    default_config = {
        'data_path': '../../data/processed_data',
        'output_path': 'outputs_v2',
        'start_year': 2008,
        'end_year': 2020,
        'algorithms': ['disparity_filter', 'mst'],
        'alpha_values': [0.01, 0.05, 0.1, 0.2],
        'validation_standards': {
            'spearman_threshold': 0.7,
            'stability_threshold': 0.8,
            'significance_level': 0.05,
            'consistency_threshold': 0.75
        },
        'generate_reports': True,
        'create_visualizations': True,
        'run_validation': True,
        'export_formats': ['html', 'markdown', 'json']
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.dump(default_config, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"âœ… é»˜è®¤é…ç½®æ–‡ä»¶å·²åˆ›å»º: {output_path}")


def print_banner():
    """æ‰“å°ç¨‹åºæ¨ªå¹…"""
    
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     éª¨å¹²ç½‘ç»œåˆ†æ v2.0 - Phase 2 Complete                      â•‘
â•‘                        Energy Network Backbone Analysis                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ¯ Phase 2 å‡çº§ç‰¹æ€§:                                                          â•‘
â•‘     âœ… P0: ä¸“ä¸šçº§ç½‘ç»œå¯è§†åŒ–ç³»ç»Ÿ                                               â•‘
â•‘     âœ… P1: å®Œæ•´ç¨³å¥æ€§æ£€éªŒç³»ç»Ÿ                                                 â•‘
â•‘     âœ… P2: å¤šå±‚æ¬¡ä¿¡æ¯æ•´åˆå¯è§†åŒ–                                               â•‘
â•‘     âœ… P3: å­¦æœ¯çº§éªŒè¯æŠ¥å‘Šç”Ÿæˆ                                                 â•‘
â•‘     âœ… P4: å®Œæ•´çš„v2åˆ†ææµç¨‹                                                   â•‘
â•‘                                                                              â•‘
â•‘  ğŸ“Š å­¦æœ¯æ ‡å‡†éªŒè¯:                                                             â•‘
â•‘     â€¢ Spearmanç›¸å…³ç³»æ•° > 0.7                                                â•‘
â•‘     â€¢ æ ¸å¿ƒå‘ç°ç¨³å®šæ€§ > 80%                                                    â•‘
â•‘     â€¢ ç»Ÿè®¡æ˜¾è‘—æ€§ p < 0.05                                                     â•‘
â•‘     â€¢ è·¨ç®—æ³•ä¸€è‡´æ€§ > 75%                                                      â•‘
â•‘                                                                              â•‘
â•‘  ä½œè€…: Energy Network Analysis Team                                          â•‘
â•‘  ç‰ˆæœ¬: v2.0 (Phase 2 Complete Edition)                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    
    print(banner)


def main():
    """ä¸»å‡½æ•°"""
    
    print_banner()
    
    parser = argparse.ArgumentParser(
        description="éª¨å¹²ç½‘ç»œåˆ†æ v2.0 - Phase 2 å®Œæ•´ç‰ˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main_v2.py --quick-demo                    # å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼
  python main_v2.py --config config.yaml           # ä½¿ç”¨é…ç½®æ–‡ä»¶
  python main_v2.py --full-analysis --years 2010-2020  # å®Œæ•´åˆ†æ
  python main_v2.py --create-config                # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
        """
    )
    
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quick-demo', action='store_true',
                       help='è¿è¡Œå¿«é€Ÿæ¼”ç¤ºæ¨¡å¼')
    parser.add_argument('--full-analysis', action='store_true',
                       help='è¿è¡Œå®Œæ•´åˆ†æ')
    parser.add_argument('--years', type=str, 
                       help='åˆ†æå¹´ä»½èŒƒå›´ (æ ¼å¼: 2010-2020)')
    parser.add_argument('--create-config', action='store_true',
                       help='åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶')
    parser.add_argument('--output', type=str, default='outputs_v2',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
    if args.create_config:
        create_default_config()
        return
    
    # åŠ è½½é…ç½®
    if Path(args.config).exists():
        config = load_config(args.config)
    else:
        config = AnalysisConfig()
    
    # è§£æå¹´ä»½èŒƒå›´
    if args.years:
        try:
            start_year, end_year = map(int, args.years.split('-'))
            config.start_year = start_year
            config.end_year = end_year
        except ValueError:
            logger.error("âŒ å¹´ä»½æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨æ ¼å¼: 2010-2020")
            return
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    config.output_path = args.output
    
    # åˆå§‹åŒ–åˆ†æç³»ç»Ÿ
    analyzer = BackboneAnalysisV2(config)
    
    try:
        # è¿è¡Œåˆ†æ
        if args.quick_demo:
            results = analyzer.quick_demo()
        else:
            results = analyzer.run_full_analysis()
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "="*80)
        print("ğŸ“Š åˆ†æå®Œæˆæ‘˜è¦:")
        print("="*80)
        
        if results['status'] == 'completed':
            print(f"âœ… çŠ¶æ€: æˆåŠŸå®Œæˆ")
            print(f"â±ï¸  æ€»æ—¶é—´: {results['execution_time']['total']:.1f} ç§’")
            
            if 'data_summary' in results:
                data_summary = results['data_summary']
                print(f"ğŸ“Š æ•°æ®: {data_summary.get('total_years', 0)} å¹´")
            
            if 'validation_results' in results:
                val_summary = results['validation_results']
                print(f"ğŸ” éªŒè¯: {val_summary.get('validation_tests_passed', 0)}/{val_summary.get('validation_tests_total', 4)} é€šè¿‡")
                print(f"ğŸ“ˆ ç½®ä¿¡åº¦: {val_summary.get('overall_confidence_score', 0):.3f}")
            
            if 'visualization_paths' in results:
                viz_paths = results['visualization_paths']
                total_viz = sum(len(paths) for paths in viz_paths.values())
                print(f"ğŸ¨ å¯è§†åŒ–: {total_viz} ä¸ªæ–‡ä»¶")
            
            if 'report_paths' in results:
                report_paths = results['report_paths']
                print(f"ğŸ“„ æŠ¥å‘Š: {len(report_paths)} ä¸ªæ–‡ä»¶")
            
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_path}")
            
        else:
            print(f"âŒ çŠ¶æ€: æ‰§è¡Œå¤±è´¥")
            if 'error' in results:
                print(f"é”™è¯¯: {results['error']}")
        
        print("="*80)
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    main()