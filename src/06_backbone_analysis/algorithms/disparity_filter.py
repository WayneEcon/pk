#!/usr/bin/env python3
"""
Disparity Filterç®—æ³•å®ç°
=============================

åŸºäºSerrano et al. (2009)çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒæ–¹æ³•ï¼Œ
è¯†åˆ«ç½‘ç»œä¸­æ¯ä¸ªèŠ‚ç‚¹çš„"å¼‚å¸¸å¼º"è¿æ¥ï¼Œè¿‡æ»¤æ‰ç»Ÿè®¡ä¸Šä¸æ˜¾è‘—çš„å™ªå£°è¿æ¥ã€‚

ç†è®ºåŸºç¡€ï¼š
- é›¶å‡è®¾H0: æƒé‡æŒ‰èŠ‚ç‚¹å¼ºåº¦éšæœºåˆ†é…
- æ£€éªŒç»Ÿè®¡é‡: åŸºäºå¤šé¡¹åˆ†å¸ƒçš„på€¼è®¡ç®—
- å¤šé‡æ¯”è¾ƒæ ¡æ­£: Benjamini-Hochberg FDRæ§åˆ¶

å‚è€ƒæ–‡çŒ®ï¼š
Serrano, M. Ã., BogunÃ¡, M., & Vespignani, A. (2009). 
Extracting the multiscale backbone of complex weighted networks. 
Proceedings of the national academy of sciences, 106(16), 6483-6488.

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import numpy as np
import pandas as pd
import networkx as nx
from typing import Dict, List, Tuple, Optional, Union
from scipy import stats
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def calculate_disparity_pvalue(weight: float, node_strength: float, node_degree: int) -> float:
    """
    è®¡ç®—å•æ¡è¾¹çš„Disparity Filter på€¼
    
    åŸºäºSerrano et al. (2009)çš„æ­£ç¡®å…¬å¼ï¼š
    åœ¨null modelä¸‹ï¼Œæƒé‡æŒ‰èŠ‚ç‚¹å¼ºåº¦éšæœºåˆ†é…ï¼Œæ¯æ¡è¾¹æƒé‡æ¯”ä¾‹éµå¾ªBetaåˆ†å¸ƒ
    
    Args:
        weight: è¾¹çš„æƒé‡
        node_strength: èŠ‚ç‚¹çš„æ€»å¼ºåº¦ï¼ˆæ‰€æœ‰å‡ºè¾¹æƒé‡ä¹‹å’Œï¼‰
        node_degree: èŠ‚ç‚¹çš„åº¦æ•°ï¼ˆå‡ºè¾¹æ•°é‡ï¼‰
        
    Returns:
        è¯¥è¾¹åœ¨null modelä¸‹çš„på€¼
    """
    
    if node_degree <= 1 or node_strength <= 0 or weight <= 0:
        return 1.0  # åº¦ä¸º1ã€å¼ºåº¦ä¸º0æˆ–æƒé‡ä¸º0æ—¶ï¼Œæ— æ³•è®¡ç®—æ˜¾è‘—æ€§
    
    # è®¡ç®—å½’ä¸€åŒ–æƒé‡æ¯”ä¾‹
    p_ij = weight / node_strength
    
    # Serrano et al. (2009)çš„æ­£ç¡®å…¬å¼
    # P(X >= p_ij) = (1 - p_ij)^(k-1)
    # è¿™åŸºäºåœ¨kä¸ªè¾¹ä¸­éšæœºåˆ†é…æƒé‡çš„null model
    k = node_degree
    
    # é¿å…æ•°å€¼è®¡ç®—é—®é¢˜
    if p_ij >= 1.0:
        return 0.0
    if p_ij <= 0.0:
        return 1.0
    
    # è®¡ç®—på€¼ï¼šP(proportion >= p_ij | null model)
    p_value = (1.0 - p_ij) ** (k - 1)
    
    return min(p_value, 1.0)

def benjamini_hochberg_fdr(p_values: np.ndarray, alpha: float = 0.05) -> np.ndarray:
    """
    Benjamini-Hochberg FDRå¤šé‡æ¯”è¾ƒæ ¡æ­£
    
    Args:
        p_values: åŸå§‹på€¼æ•°ç»„
        alpha: FDRæ°´å¹³
        
    Returns:
        FDRæ ¡æ­£åçš„æ˜¾è‘—æ€§åˆ¤æ–­ï¼ˆå¸ƒå°”æ•°ç»„ï¼‰
    """
    
    n = len(p_values)
    if n == 0:
        return np.array([], dtype=bool)
    
    # æŒ‰på€¼æ’åº
    sorted_indices = np.argsort(p_values)
    sorted_p_values = p_values[sorted_indices]
    
    # BHä¸´ç•Œå€¼ï¼šp_i <= (i/n) * alpha
    critical_values = (np.arange(1, n + 1) / n) * alpha
    
    # æ‰¾åˆ°æœ€å¤§çš„æ»¡è¶³æ¡ä»¶çš„ç´¢å¼•
    significant_sorted = sorted_p_values <= critical_values
    
    # å¦‚æœæœ‰æ˜¾è‘—çš„på€¼ï¼Œæ‰¾åˆ°æœ€å¤§çš„ç´¢å¼•
    if np.any(significant_sorted):
        max_significant_idx = np.max(np.where(significant_sorted)[0])
        # å‰max_significant_idx+1ä¸ªéƒ½æ˜¯æ˜¾è‘—çš„
        significant_sorted[:max_significant_idx + 1] = True
        significant_sorted[max_significant_idx + 1:] = False
    
    # æ¢å¤åŸå§‹é¡ºåº
    significant = np.empty(n, dtype=bool)
    significant[sorted_indices] = significant_sorted
    
    return significant

def disparity_filter(G: nx.Graph, 
                    alpha: float = 0.05,
                    fdr_correction: bool = True,
                    weight_attr: str = 'weight',
                    directed: bool = None) -> nx.Graph:
    """
    å¯¹ç½‘ç»œåº”ç”¨Disparity Filterç®—æ³•
    
    Args:
        G: è¾“å…¥ç½‘ç»œï¼ˆNetworkXå›¾å¯¹è±¡ï¼‰
        alpha: æ˜¾è‘—æ€§æ°´å¹³
        fdr_correction: æ˜¯å¦åº”ç”¨FDRå¤šé‡æ¯”è¾ƒæ ¡æ­£
        weight_attr: è¾¹æƒé‡å±æ€§å
        directed: æ˜¯å¦ä½œä¸ºæœ‰å‘å›¾å¤„ç†ï¼ŒNoneæ—¶è‡ªåŠ¨æ£€æµ‹
        
    Returns:
        è¿‡æ»¤åçš„éª¨å¹²ç½‘ç»œ
    """
    
    logger.info(f"ğŸ” å¼€å§‹åº”ç”¨Disparity Filter (Î±={alpha}, FDR={fdr_correction})...")
    
    if directed is None:
        directed = G.is_directed()
    
    # å¤åˆ¶å›¾ä»¥é¿å…ä¿®æ”¹åŸå›¾
    G_filtered = G.copy()
    
    # æ”¶é›†æ‰€æœ‰è¾¹çš„på€¼
    edge_pvalues = []
    edge_list = []
    
    # å¤„ç†æ¯ä¸ªèŠ‚ç‚¹çš„å‡ºè¾¹
    for node in G.nodes():
        if directed:
            # æœ‰å‘å›¾ï¼šåˆ†åˆ«å¤„ç†å‡ºè¾¹å’Œå…¥è¾¹
            out_edges = list(G.out_edges(node, data=True))
            in_edges = list(G.in_edges(node, data=True))
            
            # å¤„ç†å‡ºè¾¹
            if len(out_edges) > 1:
                out_strength = sum([data.get(weight_attr, 1.0) for _, _, data in out_edges])
                out_degree = len(out_edges)
                
                for source, target, data in out_edges:
                    weight = data.get(weight_attr, 1.0)
                    p_value = calculate_disparity_pvalue(weight, out_strength, out_degree)
                    edge_pvalues.append(p_value)
                    edge_list.append((source, target, 'out'))
            
            # å¤„ç†å…¥è¾¹
            if len(in_edges) > 1:
                in_strength = sum([data.get(weight_attr, 1.0) for _, _, data in in_edges])
                in_degree = len(in_edges)
                
                for source, target, data in in_edges:
                    weight = data.get(weight_attr, 1.0)
                    p_value = calculate_disparity_pvalue(weight, in_strength, in_degree)
                    edge_pvalues.append(p_value)
                    edge_list.append((source, target, 'in'))
        
        else:
            # æ— å‘å›¾ï¼šåªå¤„ç†åº¦å¤§äº1çš„èŠ‚ç‚¹
            neighbors = list(G.neighbors(node))
            if len(neighbors) > 1:
                node_strength = sum([G[node][neighbor].get(weight_attr, 1.0) 
                                   for neighbor in neighbors])
                node_degree = len(neighbors)
                
                for neighbor in neighbors:
                    # é¿å…é‡å¤è®¡ç®—åŒä¸€æ¡è¾¹
                    if node < neighbor or directed:
                        weight = G[node][neighbor].get(weight_attr, 1.0)
                        p_value = calculate_disparity_pvalue(weight, node_strength, node_degree)
                        edge_pvalues.append(p_value)
                        edge_list.append((node, neighbor, 'undirected'))
    
    if not edge_pvalues:
        logger.warning("âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„è¾¹è¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒ")
        return G_filtered
    
    edge_pvalues = np.array(edge_pvalues)
    
    # åº”ç”¨FDRæ ¡æ­£æˆ–ç›´æ¥æ¯”è¾ƒalpha
    if fdr_correction:
        significant_edges = benjamini_hochberg_fdr(edge_pvalues, alpha)
        logger.info(f"ğŸ“Š FDRæ ¡æ­£åä¿ç•™è¾¹æ•°: {np.sum(significant_edges)}/{len(edge_pvalues)}")
    else:
        significant_edges = edge_pvalues <= alpha
        logger.info(f"ğŸ“Š ç›´æ¥æ¯”è¾ƒåä¿ç•™è¾¹æ•°: {np.sum(significant_edges)}/{len(edge_pvalues)}")
    
    # æ„å»ºè¦åˆ é™¤çš„è¾¹é›†åˆ
    edges_to_remove = set()
    
    for i, (source, target, direction) in enumerate(edge_list):
        if not significant_edges[i]:
            if direction == 'undirected':
                edges_to_remove.add((source, target))
                if not directed:  # æ— å‘å›¾çš„å¯¹ç§°æ€§
                    edges_to_remove.add((target, source))
            else:
                edges_to_remove.add((source, target))
    
    # åˆ é™¤ä¸æ˜¾è‘—çš„è¾¹
    G_filtered.remove_edges_from(edges_to_remove)
    
    # æ·»åŠ è¿‡æ»¤ä¿¡æ¯åˆ°å›¾å±æ€§
    G_filtered.graph['backbone_method'] = 'disparity_filter'
    G_filtered.graph['alpha'] = alpha
    G_filtered.graph['fdr_correction'] = fdr_correction
    G_filtered.graph['original_edges'] = G.number_of_edges()
    G_filtered.graph['filtered_edges'] = G_filtered.number_of_edges()
    G_filtered.graph['retention_rate'] = G_filtered.number_of_edges() / G.number_of_edges()
    
    logger.info(f"âœ… Disparity Filterå®Œæˆ:")
    logger.info(f"   åŸå§‹è¾¹æ•°: {G.number_of_edges():,}")
    logger.info(f"   ä¿ç•™è¾¹æ•°: {G_filtered.number_of_edges():,}")
    logger.info(f"   ä¿ç•™ç‡: {G_filtered.graph['retention_rate']:.1%}")
    
    return G_filtered

def apply_disparity_filter_batch(networks: Dict[int, nx.Graph],
                                alpha_values: List[float] = [0.01, 0.05, 0.1],
                                fdr_correction: bool = True,
                                weight_attr: str = 'weight') -> Dict[str, Dict[int, nx.Graph]]:
    """
    æ‰¹é‡åº”ç”¨Disparity Filteråˆ°å¤šå¹´ç½‘ç»œæ•°æ®
    
    Args:
        networks: å¹´ä»½åˆ°ç½‘ç»œçš„æ˜ å°„å­—å…¸
        alpha_values: è¦æµ‹è¯•çš„æ˜¾è‘—æ€§æ°´å¹³åˆ—è¡¨
        fdr_correction: æ˜¯å¦åº”ç”¨FDRæ ¡æ­£
        weight_attr: è¾¹æƒé‡å±æ€§å
        
    Returns:
        åµŒå¥—å­—å…¸: {f'alpha_{alpha}': {year: backbone_network}}
    """
    
    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡Disparity Filteråˆ†æ...")
    logger.info(f"   å¹´ä»½èŒƒå›´: {min(networks.keys())}-{max(networks.keys())}")
    logger.info(f"   Alphaå€¼: {alpha_values}")
    logger.info(f"   FDRæ ¡æ­£: {fdr_correction}")
    
    results = {}
    
    for alpha in alpha_values:
        alpha_key = f'alpha_{alpha}'
        results[alpha_key] = {}
        
        logger.info(f"âš¡ å¤„ç†Î±={alpha}...")
        
        for year in sorted(networks.keys()):
            G = networks[year]
            logger.info(f"   å¤„ç†{year}å¹´ç½‘ç»œ ({G.number_of_nodes()}èŠ‚ç‚¹, {G.number_of_edges()}è¾¹)...")
            
            try:
                G_backbone = disparity_filter(
                    G, 
                    alpha=alpha,
                    fdr_correction=fdr_correction,
                    weight_attr=weight_attr
                )
                
                results[alpha_key][year] = G_backbone
                
            except Exception as e:
                logger.error(f"âŒ {year}å¹´æ•°æ®å¤„ç†å¤±è´¥: {e}")
                continue
    
    # ç»Ÿè®¡æ‘˜è¦
    logger.info("ğŸ“Š æ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦:")
    for alpha_key, year_networks in results.items():
        retention_rates = [G.graph['retention_rate'] for G in year_networks.values()]
        if retention_rates:
            avg_retention = np.mean(retention_rates)
            logger.info(f"   {alpha_key}: å¹³å‡ä¿ç•™ç‡ = {avg_retention:.1%}")
    
    return results

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logger.info("ğŸ§ª æµ‹è¯•Disparity Filterç®—æ³•...")
    
    # åˆ›å»ºæµ‹è¯•ç½‘ç»œ
    G_test = nx.DiGraph()
    
    # æ·»åŠ æµ‹è¯•è¾¹ï¼ˆæƒé‡å·®å¼‚å¾ˆå¤§ï¼‰
    edges = [
        ('A', 'B', 10.0), ('A', 'C', 1.0), ('A', 'D', 0.1),
        ('B', 'C', 5.0), ('B', 'D', 2.0),
        ('C', 'D', 8.0)
    ]
    
    for source, target, weight in edges:
        G_test.add_edge(source, target, weight=weight)
    
    # åº”ç”¨Disparity Filter
    G_backbone = disparity_filter(G_test, alpha=0.05, fdr_correction=True)
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"åŸå§‹å›¾: {G_test.number_of_edges()} æ¡è¾¹")
    print(f"éª¨å¹²å›¾: {G_backbone.number_of_edges()} æ¡è¾¹")
    print(f"ä¿ç•™çš„è¾¹: {list(G_backbone.edges())}")