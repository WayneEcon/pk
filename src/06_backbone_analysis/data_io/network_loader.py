#!/usr/bin/env python3
"""
ç½‘ç»œæ•°æ®åŠ è½½æ¨¡å—
================

ä»é¡¹ç›®çš„02_net_analysisæ¨¡å—åŠ è½½å¹´åº¦ç½‘ç»œæ•°æ®ï¼Œ
å¹¶æä¾›ç»Ÿä¸€çš„æ•°æ®æ¥å£ä¾›éª¨å¹²ç½‘ç»œåˆ†æä½¿ç”¨ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½02æ¨¡å—ç”Ÿæˆçš„å¹´åº¦ç½‘ç»œæ–‡ä»¶
2. åŠ è½½03æ¨¡å—çš„èŠ‚ç‚¹ä¸­å¿ƒæ€§æ•°æ®
3. æ•°æ®æ ¼å¼æ ‡å‡†åŒ–å’ŒéªŒè¯
4. æä¾›æ‰¹é‡æ•°æ®åŠ è½½åŠŸèƒ½

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import pandas as pd
import networkx as nx
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import logging
import json
import pickle
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
NET_ANALYSIS_DIR = PROJECT_ROOT / "data" / "processed_data" / "networks"
CENTRALITY_DIR = PROJECT_ROOT / "src" / "03_centrality_analysis" 

def load_annual_network(year: int, 
                       networks_dir: Path = None,
                       file_format: str = 'graphml') -> Optional[nx.DiGraph]:
    """
    åŠ è½½æŒ‡å®šå¹´ä»½çš„ç½‘ç»œæ•°æ®
    
    Args:
        year: å¹´ä»½
        networks_dir: ç½‘ç»œæ–‡ä»¶ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨02æ¨¡å—ç›®å½•
        file_format: æ–‡ä»¶æ ¼å¼ ('graphml', 'gexf', 'pickle')
        
    Returns:
        NetworkXæœ‰å‘å›¾å¯¹è±¡ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    
    if networks_dir is None:
        networks_dir = NET_ANALYSIS_DIR
    
    # å°è¯•ä¸åŒçš„æ–‡ä»¶åæ¨¡å¼
    possible_filenames = [
        f"energy_network_{year}.{file_format}",
        f"network_{year}.{file_format}",
        f"{year}.{file_format}",
        f"trade_network_{year}.{file_format}"
    ]
    
    for filename in possible_filenames:
        filepath = networks_dir / filename
        
        if filepath.exists():
            logger.info(f"ğŸ“‚ åŠ è½½{year}å¹´ç½‘ç»œ: {filepath}")
            
            try:
                if file_format == 'graphml':
                    G = nx.read_graphml(filepath)
                elif file_format == 'gexf':
                    G = nx.read_gexf(filepath)
                elif file_format == 'pickle':
                    with open(filepath, 'rb') as f:
                        G = pickle.load(f)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
                
                # ç¡®ä¿æ˜¯æœ‰å‘å›¾
                if not isinstance(G, nx.DiGraph):
                    G = G.to_directed()
                
                # éªŒè¯ç½‘ç»œåŸºæœ¬ä¿¡æ¯
                logger.info(f"   èŠ‚ç‚¹æ•°: {G.number_of_nodes():,}")
                logger.info(f"   è¾¹æ•°: {G.number_of_edges():,}")
                
                # éªŒè¯æƒé‡å±æ€§
                weight_attrs = []
                for _, _, data in G.edges(data=True):
                    weight_attrs.extend(data.keys())
                    break
                
                if weight_attrs:
                    logger.info(f"   è¾¹å±æ€§: {weight_attrs}")
                
                return G
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½{filepath}å¤±è´¥: {e}")
                continue
    
    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{year}å¹´çš„ç½‘ç»œæ–‡ä»¶")
    return None

def load_annual_networks(year_range: Tuple[int, int] = (2001, 2024),
                        networks_dir: Path = None,
                        file_format: str = 'graphml') -> Dict[int, nx.DiGraph]:
    """
    æ‰¹é‡åŠ è½½å¹´åº¦ç½‘ç»œæ•°æ®
    
    Args:
        year_range: å¹´ä»½èŒƒå›´ (start, end) åŒ…å«è¾¹ç•Œ
        networks_dir: ç½‘ç»œæ–‡ä»¶ç›®å½•
        file_format: æ–‡ä»¶æ ¼å¼
        
    Returns:
        å¹´ä»½åˆ°ç½‘ç»œçš„æ˜ å°„å­—å…¸
    """
    
    logger.info(f"ğŸš€ æ‰¹é‡åŠ è½½ç½‘ç»œæ•°æ® ({year_range[0]}-{year_range[1]})...")
    
    networks = {}
    start_year, end_year = year_range
    
    for year in range(start_year, end_year + 1):
        G = load_annual_network(year, networks_dir, file_format)
        if G is not None:
            networks[year] = G
    
    logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(networks)} ä¸ªå¹´ä»½çš„ç½‘ç»œæ•°æ®")
    logger.info(f"   è¦†ç›–å¹´ä»½: {sorted(networks.keys())}")
    
    if len(networks) == 0:
        logger.error("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•ç½‘ç»œæ•°æ®")
        
        # å°è¯•åˆ—å‡ºç›®å½•ä¸­çš„æ–‡ä»¶å¸®åŠ©è°ƒè¯•
        if networks_dir and networks_dir.exists():
            logger.info(f"ğŸ“ ç›®å½• {networks_dir} ä¸­çš„æ–‡ä»¶:")
            for file in sorted(networks_dir.glob("*")):
                if file.is_file():
                    logger.info(f"   {file.name}")
    
    return networks

def load_centrality_data(year: int,
                        centrality_dir: Path = None) -> Optional[pd.DataFrame]:
    """
    åŠ è½½æŒ‡å®šå¹´ä»½çš„èŠ‚ç‚¹ä¸­å¿ƒæ€§æ•°æ®
    
    Args:
        year: å¹´ä»½
        centrality_dir: ä¸­å¿ƒæ€§æ•°æ®ç›®å½•
        
    Returns:
        åŒ…å«ä¸­å¿ƒæ€§æŒ‡æ ‡çš„DataFrameï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    
    if centrality_dir is None:
        centrality_dir = CENTRALITY_DIR
    
    # å°è¯•ä¸åŒçš„æ–‡ä»¶åæ¨¡å¼
    possible_filenames = [
        f"centrality_metrics_{year}.csv",
        f"node_metrics_{year}.csv",
        f"{year}_centrality.csv"
    ]
    
    for filename in possible_filenames:
        filepath = centrality_dir / filename
        
        if filepath.exists():
            logger.info(f"ğŸ“Š åŠ è½½{year}å¹´ä¸­å¿ƒæ€§æ•°æ®: {filepath}")
            
            try:
                df = pd.read_csv(filepath)
                logger.info(f"   èŠ‚ç‚¹æ•°: {len(df)}")
                logger.info(f"   æŒ‡æ ‡åˆ—: {list(df.columns)}")
                return df
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½{filepath}å¤±è´¥: {e}")
                continue
    
    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{year}å¹´çš„ä¸­å¿ƒæ€§æ•°æ®")
    return None

def load_country_metadata() -> Optional[pd.DataFrame]:
    """
    åŠ è½½å›½å®¶å…ƒæ•°æ®ï¼ˆåœ°ç†åŒºåŸŸã€ç»æµåˆ†ç±»ç­‰ï¼‰
    
    Returns:
        å›½å®¶å…ƒæ•°æ®DataFrameï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    
    # å°è¯•ä»ä¸åŒä½ç½®åŠ è½½å…ƒæ•°æ®
    possible_paths = [
        PROJECT_ROOT / "data" / "country_metadata.csv",
        PROJECT_ROOT / "src" / "country_metadata.csv",
        NET_ANALYSIS_DIR / "country_metadata.csv",
        CENTRALITY_DIR / "country_metadata.csv"
    ]
    
    for filepath in possible_paths:
        if filepath.exists():
            logger.info(f"ğŸŒ åŠ è½½å›½å®¶å…ƒæ•°æ®: {filepath}")
            
            try:
                df = pd.read_csv(filepath)
                logger.info(f"   å›½å®¶æ•°: {len(df)}")
                return df
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½{filepath}å¤±è´¥: {e}")
                continue
    
    logger.warning("âš ï¸ æœªæ‰¾åˆ°å›½å®¶å…ƒæ•°æ®æ–‡ä»¶")
    return None

def validate_network_consistency(networks: Dict[int, nx.DiGraph]) -> Dict[str, any]:
    """
    éªŒè¯å¤šå¹´ç½‘ç»œæ•°æ®çš„ä¸€è‡´æ€§
    
    Args:
        networks: å¹´ä»½åˆ°ç½‘ç»œçš„æ˜ å°„å­—å…¸
        
    Returns:
        ä¸€è‡´æ€§æ£€æŸ¥ç»“æœå­—å…¸
    """
    
    logger.info("ğŸ” éªŒè¯ç½‘ç»œæ•°æ®ä¸€è‡´æ€§...")
    
    if not networks:
        return {'status': 'empty', 'message': 'æ²¡æœ‰ç½‘ç»œæ•°æ®'}
    
    years = sorted(networks.keys())
    results = {
        'years': years,
        'node_consistency': {},
        'edge_attributes': {},
        'graph_attributes': {},
        'issues': []
    }
    
    # 1. æ£€æŸ¥èŠ‚ç‚¹ä¸€è‡´æ€§
    all_nodes = set()
    for year, G in networks.items():
        year_nodes = set(G.nodes())
        all_nodes.update(year_nodes)
        results['node_consistency'][year] = len(year_nodes)
    
    results['total_unique_nodes'] = len(all_nodes)
    
    # æ£€æŸ¥èŠ‚ç‚¹æ•°é‡å˜åŒ–
    node_counts = list(results['node_consistency'].values())
    if max(node_counts) - min(node_counts) > len(all_nodes) * 0.1:
        results['issues'].append('èŠ‚ç‚¹æ•°é‡å˜åŒ–è¾ƒå¤§')
    
    # 2. æ£€æŸ¥è¾¹å±æ€§ä¸€è‡´æ€§
    edge_attrs = {}
    for year, G in networks.items():
        year_attrs = set()
        for _, _, data in G.edges(data=True):
            year_attrs.update(data.keys())
            break  # åªæ£€æŸ¥ç¬¬ä¸€æ¡è¾¹
        edge_attrs[year] = year_attrs
    
    results['edge_attributes'] = edge_attrs
    
    # æ£€æŸ¥å±æ€§ä¸€è‡´æ€§
    if len(edge_attrs) > 1:
        common_attrs = set.intersection(*edge_attrs.values())
        if len(common_attrs) == 0:
            results['issues'].append('æ²¡æœ‰å…±åŒçš„è¾¹å±æ€§')
    
    # 3. æ£€æŸ¥å›¾å±æ€§
    graph_attrs = {}
    for year, G in networks.items():
        graph_attrs[year] = dict(G.graph)
    
    results['graph_attributes'] = graph_attrs
    
    # 4. ç»Ÿè®¡æ‘˜è¦
    edge_counts = [G.number_of_edges() for G in networks.values()]
    results['edge_count_range'] = (min(edge_counts), max(edge_counts))
    results['avg_edge_count'] = sum(edge_counts) / len(edge_counts)
    
    # çŠ¶æ€åˆ¤æ–­
    if results['issues']:
        results['status'] = 'issues_found'
        logger.warning(f"âš ï¸ å‘ç° {len(results['issues'])} ä¸ªé—®é¢˜:")
        for issue in results['issues']:
            logger.warning(f"   {issue}")
    else:
        results['status'] = 'consistent'
        logger.info("âœ… ç½‘ç»œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
    
    logger.info("ğŸ“Š ç½‘ç»œæ•°æ®ç»Ÿè®¡:")
    logger.info(f"   å¹´ä»½èŒƒå›´: {min(years)} - {max(years)}")
    logger.info(f"   èŠ‚ç‚¹æ•°èŒƒå›´: {min(node_counts)} - {max(node_counts)}")
    logger.info(f"   è¾¹æ•°èŒƒå›´: {results['edge_count_range'][0]:,} - {results['edge_count_range'][1]:,}")
    
    return results

def save_backbone_network(G: nx.Graph, 
                         filepath: Path,
                         file_format: str = 'graphml',
                         include_metadata: bool = True) -> bool:
    """
    ä¿å­˜éª¨å¹²ç½‘ç»œåˆ°æ–‡ä»¶
    
    Args:
        G: éª¨å¹²ç½‘ç»œ
        filepath: ä¿å­˜è·¯å¾„
        file_format: æ–‡ä»¶æ ¼å¼
        include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®
        
    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    
    try:
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        if file_format == 'graphml':
            nx.write_graphml(G, filepath)
        elif file_format == 'gexf':
            nx.write_gexf(G, filepath)
        elif file_format == 'pickle':
            with open(filepath, 'wb') as f:
                pickle.dump(G, f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
        
        logger.info(f"ğŸ’¾ éª¨å¹²ç½‘ç»œå·²ä¿å­˜: {filepath}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜éª¨å¹²ç½‘ç»œå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½
    logger.info("ğŸ§ª æµ‹è¯•ç½‘ç»œæ•°æ®åŠ è½½...")
    
    # æµ‹è¯•åŠ è½½å•å¹´ç½‘ç»œ
    test_year = 2020
    G = load_annual_network(test_year)
    
    if G:
        print(f"âœ… æˆåŠŸåŠ è½½{test_year}å¹´ç½‘ç»œ")
        print(f"   èŠ‚ç‚¹æ•°: {G.number_of_nodes()}")
        print(f"   è¾¹æ•°: {G.number_of_edges()}")
    
    # æµ‹è¯•æ‰¹é‡åŠ è½½
    networks = load_annual_networks((2018, 2020))
    
    if networks:
        print(f"âœ… æˆåŠŸæ‰¹é‡åŠ è½½: {list(networks.keys())}")
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        consistency_results = validate_network_consistency(networks)
        print(f"ä¸€è‡´æ€§çŠ¶æ€: {consistency_results['status']}")
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")