#!/usr/bin/env python3
"""
å®Œæ•´ç½‘ç»œå±æ€§åŠ è½½å™¨
================

ä»03æ¨¡å—åŠ è½½å®Œæ•´çš„èŠ‚ç‚¹å±æ€§æ•°æ®ï¼Œç¡®ä¿éª¨å¹²ç½‘ç»œå¯è§†åŒ–çš„ä¿¡æ¯ä¿çœŸæ€§ã€‚
ä¸“é—¨å¤„ç†ä¸è½¨é“ä¸€åˆ†æç»“æœçš„æ•°æ®æ•´åˆé—®é¢˜ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åŠ è½½å®Œæ•´ç½‘ç»œçš„èŠ‚ç‚¹å¼ºåº¦å’Œä¸­å¿ƒæ€§æ•°æ®
2. æä¾›åœ°ç†åŒºåŸŸåˆ†ç±»ä¿¡æ¯
3. æ•´åˆä¸­å¿ƒæ€§æ’åæ•°æ®
4. æ”¯æŒè·¨æ¨¡å—æ•°æ®ä¸€è‡´æ€§éªŒè¯

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import pandas as pd
import networkx as nx
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
import logging
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NetworkAttributeLoader:
    """ç½‘ç»œå±æ€§åŠ è½½å™¨"""
    
    def __init__(self, base_data_path: Path = None):
        """
        åˆå§‹åŒ–å±æ€§åŠ è½½å™¨
        
        Args:
            base_data_path: åŸºç¡€æ•°æ®è·¯å¾„
        """
        
        if base_data_path is None:
            # è‡ªåŠ¨æ£€æµ‹æ•°æ®è·¯å¾„
            current_path = Path(__file__).parent
            potential_paths = [
                current_path / "../../../data/processed_data",
                current_path / "../../../../data/processed_data", 
                current_path / "../../../03_network_analysis/outputs",
                current_path / "../../../../03_network_analysis/outputs"
            ]
            
            for path in potential_paths:
                if path.exists():
                    base_data_path = path
                    break
        
        self.base_data_path = Path(base_data_path) if base_data_path else None
        self.networks_path = None
        self.track1_path = None
        
        if self.base_data_path and self.base_data_path.exists():
            # å°è¯•æ‰¾åˆ°ç½‘ç»œæ•°æ®è·¯å¾„
            networks_candidates = [
                self.base_data_path / "networks",
                self.base_data_path / "network_data",
                self.base_data_path / "../networks"
            ]
            
            for candidate in networks_candidates:
                if candidate.exists():
                    self.networks_path = candidate
                    break
            
            # å°è¯•æ‰¾åˆ°è½¨é“ä¸€ç»“æœè·¯å¾„
            track1_candidates = [
                self.base_data_path / "usa_centrality_analysis.csv",
                self.base_data_path / "centrality_analysis",
                self.base_data_path / "../03_network_analysis/outputs"
            ]
            
            for candidate in track1_candidates:
                if candidate.exists():
                    self.track1_path = candidate
                    break
        
        logger.info(f"ğŸ”§ ç½‘ç»œå±æ€§åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   åŸºç¡€è·¯å¾„: {self.base_data_path}")
        logger.info(f"   ç½‘ç»œè·¯å¾„: {self.networks_path}")
        logger.info(f"   è½¨é“ä¸€è·¯å¾„: {self.track1_path}")
    
    def load_full_network_attributes(self, 
                                   year: int,
                                   include_centrality: bool = True) -> Dict[str, Any]:
        """
        åŠ è½½æŒ‡å®šå¹´ä»½çš„å®Œæ•´ç½‘ç»œå±æ€§
        
        Args:
            year: å¹´ä»½
            include_centrality: æ˜¯å¦åŒ…å«ä¸­å¿ƒæ€§æ•°æ®
            
        Returns:
            å±æ€§å­—å…¸åŒ…å«ï¼š
            - total_strength: èŠ‚ç‚¹æ€»å¼ºåº¦
            - geographic_region: åœ°ç†åŒºåŸŸåˆ†ç±»
            - centrality_rankings: ä¸­å¿ƒæ€§æ’åï¼ˆå¦‚æœå¯ç”¨ï¼‰
            - trade_partners_list: è´¸æ˜“ä¼™ä¼´åˆ—è¡¨
        """
        
        logger.info(f"ğŸ“‚ åŠ è½½{year}å¹´å®Œæ•´ç½‘ç»œå±æ€§...")
        
        attributes = {
            'total_strength': {},
            'geographic_region': {},
            'centrality_rankings': {},
            'trade_partners_list': {},
            'pagerank': {},
            'betweenness': {},
            'closeness': {}
        }
        
        # 1. åŠ è½½ç½‘ç»œæ•°æ®è·å–åŸºç¡€å±æ€§
        network = self._load_network_for_year(year)
        
        if network is not None:
            # è®¡ç®—èŠ‚ç‚¹å¼ºåº¦
            for node in network.nodes():
                attributes['total_strength'][node] = network.degree(node, weight='weight')
                attributes['trade_partners_list'][node] = list(network.neighbors(node))
            
            # åˆ†é…åœ°ç†åŒºåŸŸ
            from ..visualization.styling import ProfessionalNetworkStyling
            styling = ProfessionalNetworkStyling()
            
            for node in network.nodes():
                region = styling.COUNTRY_TO_REGION.get(node, 'Other')
                attributes['geographic_region'][node] = region
            
            # è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡ï¼ˆå¦‚æœéœ€è¦ä¸”ç½‘ç»œä¸å¤ªå¤§ï¼‰
            if include_centrality and network.number_of_nodes() <= 300:
                try:
                    logger.info("   è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡...")
                    
                    # PageRank
                    pagerank = nx.pagerank(network, weight='weight')
                    attributes['pagerank'] = pagerank
                    
                    # Betweennessï¼ˆè®¡ç®—è¾ƒæ…¢ï¼Œå¯é€‰ï¼‰
                    if network.number_of_nodes() <= 150:
                        betweenness = nx.betweenness_centrality(network, weight='weight')
                        attributes['betweenness'] = betweenness
                    
                    # Closeness
                    if network.number_of_nodes() <= 200:
                        closeness = nx.closeness_centrality(network, distance='weight')
                        attributes['closeness'] = closeness
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¸­å¿ƒæ€§è®¡ç®—å¤±è´¥: {e}")
        
        # 2. å°è¯•åŠ è½½è½¨é“ä¸€çš„ä¸­å¿ƒæ€§æ•°æ®
        track1_data = self._load_track1_centrality_data(year)
        if track1_data:
            # åˆå¹¶è½¨é“ä¸€çš„ä¸­å¿ƒæ€§æ•°æ®
            for metric in ['pagerank', 'betweenness', 'closeness']:
                if metric in track1_data:
                    if metric not in attributes or not attributes[metric]:
                        attributes[metric] = track1_data[metric]
            
            # åŠ è½½æ’åä¿¡æ¯
            if 'rankings' in track1_data:
                attributes['centrality_rankings'] = track1_data['rankings']
        
        logger.info(f"âœ… {year}å¹´ç½‘ç»œå±æ€§åŠ è½½å®Œæˆ")
        logger.info(f"   èŠ‚ç‚¹æ•°: {len(attributes['total_strength'])}")
        logger.info(f"   åœ°ç†åŒºåŸŸ: {len(set(attributes['geographic_region'].values()))}")
        logger.info(f"   ä¸­å¿ƒæ€§æŒ‡æ ‡: {len([k for k, v in attributes.items() if k.endswith('ness') or k == 'pagerank' and v])}")
        
        return attributes
    
    def _load_network_for_year(self, year: int) -> Optional[nx.Graph]:
        """åŠ è½½æŒ‡å®šå¹´ä»½çš„ç½‘ç»œ"""
        
        if not self.networks_path or not self.networks_path.exists():
            logger.warning(f"âš ï¸ ç½‘ç»œæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {self.networks_path}")
            return None
        
        # å°è¯•ä¸åŒçš„æ–‡ä»¶åæ ¼å¼
        potential_files = [
            self.networks_path / f"network_{year}.graphml",
            self.networks_path / f"network_{year}.gml", 
            self.networks_path / f"network_{year}.gpickle",
            self.networks_path / f"{year}.graphml",
            self.networks_path / f"energy_network_{year}.graphml"
        ]
        
        for file_path in potential_files:
            if file_path.exists():
                try:
                    if file_path.suffix == '.graphml':
                        G = nx.read_graphml(file_path)
                    elif file_path.suffix == '.gml':
                        G = nx.read_gml(file_path)
                    elif file_path.suffix == '.gpickle':
                        G = nx.read_gpickle(file_path)
                    else:
                        continue
                    
                    logger.info(f"   æˆåŠŸåŠ è½½ç½‘ç»œ: {file_path.name}")
                    return G
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ åŠ è½½{file_path}å¤±è´¥: {e}")
                    continue
        
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{year}å¹´çš„ç½‘ç»œæ–‡ä»¶")
        return None
    
    def _load_track1_centrality_data(self, year: int) -> Optional[Dict]:
        """åŠ è½½è½¨é“ä¸€çš„ä¸­å¿ƒæ€§æ•°æ®"""
        
        if not self.track1_path:
            return None
        
        try:
            # å°è¯•åŠ è½½CSVæ ¼å¼çš„è½¨é“ä¸€æ•°æ®
            csv_files = [
                self.track1_path / "usa_centrality_analysis.csv",
                self.track1_path / "centrality_analysis.csv",
                self.track1_path / f"centrality_{year}.csv"
            ]
            
            if self.track1_path.suffix == '.csv':
                csv_files = [self.track1_path]
            
            for csv_file in csv_files:
                if csv_file.exists():
                    df = pd.read_csv(csv_file)
                    
                    # è¿‡æ»¤æŒ‡å®šå¹´ä»½çš„æ•°æ®
                    if 'year' in df.columns:
                        year_data = df[df['year'] == year]
                        if len(year_data) == 0:
                            continue
                    else:
                        year_data = df
                    
                    # æå–ä¸­å¿ƒæ€§æ•°æ®
                    centrality_data = {}
                    
                    # æå–å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡
                    for metric in ['pagerank', 'betweenness_centrality', 'closeness_centrality']:
                        if metric in year_data.columns:
                            # å‡è®¾æ•°æ®æ ¼å¼ä¸ºæ¯è¡Œä¸€ä¸ªå›½å®¶
                            if 'country' in year_data.columns:
                                metric_dict = dict(zip(year_data['country'], year_data[metric]))
                            elif 'node' in year_data.columns:
                                metric_dict = dict(zip(year_data['node'], year_data[metric]))
                            else:
                                # å¦‚æœåªæœ‰ä¸€è¡Œæ•°æ®ï¼ˆå¦‚ç¾å›½æ•°æ®ï¼‰ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                                metric_dict = {'USA': year_data[metric].iloc[0]} if len(year_data) > 0 else {}
                            
                            centrality_data[metric.replace('_centrality', '')] = metric_dict
                    
                    logger.info(f"   æˆåŠŸåŠ è½½è½¨é“ä¸€æ•°æ®: {csv_file.name}")
                    return centrality_data
                    
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½è½¨é“ä¸€æ•°æ®å¤±è´¥: {e}")
        
        return None
    
    def load_batch_attributes(self, 
                            years: List[int],
                            include_centrality: bool = True) -> Dict[int, Dict[str, Any]]:
        """
        æ‰¹é‡åŠ è½½å¤šå¹´ä»½ç½‘ç»œå±æ€§
        
        Args:
            years: å¹´ä»½åˆ—è¡¨
            include_centrality: æ˜¯å¦åŒ…å«ä¸­å¿ƒæ€§æ•°æ®
            
        Returns:
            å¹´ä»½åˆ°å±æ€§çš„æ˜ å°„å­—å…¸
        """
        
        logger.info(f"ğŸš€ æ‰¹é‡åŠ è½½ç½‘ç»œå±æ€§ ({len(years)}å¹´)...")
        
        batch_attributes = {}
        
        for year in sorted(years):
            try:
                attributes = self.load_full_network_attributes(year, include_centrality)
                batch_attributes[year] = attributes
            except Exception as e:
                logger.error(f"âŒ {year}å¹´å±æ€§åŠ è½½å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ… æ‰¹é‡åŠ è½½å®Œæˆ ({len(batch_attributes)}/{len(years)} å¹´)")
        return batch_attributes
    
    def verify_data_consistency(self, 
                              attributes: Dict[str, Any],
                              backbone_network: nx.Graph) -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®ä¸€è‡´æ€§
        
        Args:
            attributes: å±æ€§æ•°æ®
            backbone_network: éª¨å¹²ç½‘ç»œ
            
        Returns:
            ä¸€è‡´æ€§æ£€éªŒç»“æœ
        """
        
        logger.info("ğŸ” éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
        
        consistency_report = {
            'node_coverage': 0,
            'missing_nodes': [],
            'attribute_completeness': {},
            'data_quality_score': 0
        }
        
        # æ£€æŸ¥èŠ‚ç‚¹è¦†ç›–ç‡
        backbone_nodes = set(backbone_network.nodes())
        attribute_nodes = set(attributes['total_strength'].keys())
        
        covered_nodes = backbone_nodes.intersection(attribute_nodes)
        missing_nodes = backbone_nodes - attribute_nodes
        
        consistency_report['node_coverage'] = len(covered_nodes) / len(backbone_nodes) if backbone_nodes else 0
        consistency_report['missing_nodes'] = list(missing_nodes)
        
        # æ£€æŸ¥å±æ€§å®Œæ•´æ€§
        for attr_name, attr_data in attributes.items():
            if isinstance(attr_data, dict):
                completeness = len(attr_data) / len(backbone_nodes) if backbone_nodes else 0
                consistency_report['attribute_completeness'][attr_name] = completeness
        
        # è®¡ç®—æ€»ä½“æ•°æ®è´¨é‡åˆ†æ•°
        completeness_scores = list(consistency_report['attribute_completeness'].values())
        if completeness_scores:
            avg_completeness = np.mean(completeness_scores)
            consistency_report['data_quality_score'] = (consistency_report['node_coverage'] + avg_completeness) / 2
        
        logger.info(f"âœ… æ•°æ®ä¸€è‡´æ€§æ£€éªŒå®Œæˆ")
        logger.info(f"   èŠ‚ç‚¹è¦†ç›–ç‡: {consistency_report['node_coverage']:.1%}")
        logger.info(f"   ç¼ºå¤±èŠ‚ç‚¹: {len(consistency_report['missing_nodes'])}")
        logger.info(f"   æ•°æ®è´¨é‡åˆ†æ•°: {consistency_report['data_quality_score']:.3f}")
        
        return consistency_report
    
    def create_attribute_summary(self, 
                               attributes: Dict[str, Any]) -> pd.DataFrame:
        """
        åˆ›å»ºå±æ€§æ•°æ®æ‘˜è¦è¡¨
        
        Args:
            attributes: å±æ€§æ•°æ®
            
        Returns:
            æ‘˜è¦DataFrame
        """
        
        summary_data = []
        
        for node in attributes['total_strength'].keys():
            row = {
                'country': node,
                'total_strength': attributes['total_strength'].get(node, 0),
                'geographic_region': attributes['geographic_region'].get(node, 'Unknown'),
                'trade_partners_count': len(attributes['trade_partners_list'].get(node, [])),
                'pagerank': attributes['pagerank'].get(node, 0),
                'betweenness': attributes['betweenness'].get(node, 0),
                'closeness': attributes['closeness'].get(node, 0)
            }
            summary_data.append(row)
        
        df = pd.DataFrame(summary_data)
        
        # æŒ‰æ€»å¼ºåº¦æ’åº
        df = df.sort_values('total_strength', ascending=False)
        
        return df

if __name__ == "__main__":
    # æµ‹è¯•å±æ€§åŠ è½½å™¨
    logger.info("ğŸ§ª æµ‹è¯•ç½‘ç»œå±æ€§åŠ è½½å™¨...")
    
    # åˆå§‹åŒ–åŠ è½½å™¨
    loader = NetworkAttributeLoader()
    
    # æµ‹è¯•åŠ è½½2018å¹´æ•°æ®
    if loader.networks_path and loader.networks_path.exists():
        attributes_2018 = loader.load_full_network_attributes(2018)
        
        print("ğŸ‰ å±æ€§åŠ è½½å™¨æµ‹è¯•å®Œæˆ!")
        print(f"æ€»å¼ºåº¦æ•°æ®: {len(attributes_2018['total_strength'])} ä¸ªèŠ‚ç‚¹")
        print(f"åœ°ç†åŒºåŸŸ: {len(set(attributes_2018['geographic_region'].values()))} ä¸ªåŒºåŸŸ")
        print(f"PageRankæ•°æ®: {len(attributes_2018['pagerank'])} ä¸ªèŠ‚ç‚¹")
        
        # åˆ›å»ºæ‘˜è¦è¡¨
        summary_df = loader.create_attribute_summary(attributes_2018)
        print(f"\nå‰5åå›½å®¶ï¼ˆæŒ‰æ€»å¼ºåº¦ï¼‰:")
        print(summary_df.head()[['country', 'total_strength', 'geographic_region']].to_string(index=False))
        
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ç½‘ç»œæ•°æ®è·¯å¾„ï¼Œè·³è¿‡å®é™…æ•°æ®æµ‹è¯•")
        print("å±æ€§åŠ è½½å™¨ç»“æ„æµ‹è¯•é€šè¿‡ï¼")