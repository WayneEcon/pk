#!/usr/bin/env python3
"""
å®Œæ•´ç¨³å¥æ€§æ£€éªŒç³»ç»Ÿ
================

Phase 2å‡çº§çš„æ ¸å¿ƒéªŒè¯æ¨¡å—ï¼Œä¸“é—¨å›ç­”å…³é”®ç ”ç©¶é—®é¢˜ï¼š
"ç¾å›½èƒ½æºåœ°ä½æå‡åœ¨æ‰€æœ‰ç®—æ³•ä¸­æ˜¯å¦ä¸€è‡´ï¼Ÿé¡µå²©é©å‘½æ—¶é—´èŠ‚ç‚¹æ˜¯å¦åœ¨æ‰€æœ‰æ–¹æ³•ä¸­éƒ½å¯è§‚æµ‹ï¼Ÿ"

æ ¸å¿ƒæ£€éªŒåŠŸèƒ½ï¼š
1. validate_centrality_consistency: éªŒè¯ä¸­å¿ƒæ€§æ’åºçš„ä¸€è‡´æ€§
2. parameter_sensitivity_analysis: DFç®—æ³•å‚æ•°æ•æ„Ÿæ€§åˆ†æ  
3. cross_algorithm_validation: è·¨ç®—æ³•éªŒè¯æ ¸å¿ƒå‘ç°
4. statistical_significance_testing: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

å¿…é¡»å›ç­”çš„å…³é”®é—®é¢˜ï¼š
1. ä¸€è‡´æ€§éªŒè¯ï¼šç¾å›½ä¸­å¿ƒæ€§æ’ååœ¨éª¨å¹²ç½‘ç»œvså®Œæ•´ç½‘ç»œä¸­çš„Spearmanç›¸å…³ç³»æ•° > 0.7
2. ç¨³å¥æ€§æ£€éªŒï¼šæ ¸å¿ƒå‘ç°åœ¨ä¸åŒÎ±å€¼ä¸‹çš„ç¨³å®šæ€§ > 80%
3. æ˜¾è‘—æ€§æ£€éªŒï¼šç¾å›½åœ°ä½å˜åŒ–çš„ç»Ÿè®¡æ˜¾è‘—æ€§ p < 0.05
4. æ—¶é—´èŠ‚ç‚¹éªŒè¯ï¼š2016å¹´åæ”¿ç­–æ•ˆåº”åœ¨æ‰€æœ‰ç®—æ³•ä¸­çš„ä¸€è‡´æ€§

ä½œè€…ï¼šEnergy Network Analysis Team
"""

import pandas as pd
import networkx as nx
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
from scipy import stats
from scipy.stats import spearmanr, kendalltau, ttest_ind, mannwhitneyu
import logging
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveValidator:
    """å®Œæ•´ç¨³å¥æ€§æ£€éªŒç³»ç»Ÿ"""
    
    def __init__(self, 
                 original_networks: Dict[int, nx.Graph] = None,
                 backbone_networks: Dict[str, Dict[int, nx.Graph]] = None,
                 track1_results: Optional[Dict] = None):
        """
        åˆå§‹åŒ–å®Œæ•´éªŒè¯ç³»ç»Ÿ
        
        Args:
            original_networks: åŸå§‹ç½‘ç»œæ•°æ® {year: network}
            backbone_networks: éª¨å¹²ç½‘ç»œæ•°æ® {algorithm: {year: network}}
            track1_results: è½¨é“ä¸€åˆ†æç»“æœ
        """
        self.original_networks = original_networks or {}
        self.backbone_networks = backbone_networks or {}
        self.track1_results = track1_results
        
        # å…³é”®å¹´ä»½å®šä¹‰
        self.shale_revolution_year = 2011
        self.policy_change_year = 2016  # æ ¹æ®éœ€æ±‚è°ƒæ•´
        
        logger.info("ğŸ”§ å®Œæ•´ç¨³å¥æ€§æ£€éªŒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   åŸå§‹ç½‘ç»œ: {len(self.original_networks)} å¹´")
        logger.info(f"   éª¨å¹²ç®—æ³•: {list(self.backbone_networks.keys())}")
        logger.info(f"   é¡µå²©é©å‘½å¹´ä»½: {self.shale_revolution_year}")
        logger.info(f"   æ”¿ç­–å˜åŒ–å¹´ä»½: {self.policy_change_year}")
    
    def validate_centrality_consistency(self, 
                                      full_networks: Dict[int, nx.Graph],
                                      backbone_networks: Dict[str, Dict[int, nx.Graph]],
                                      metrics_data: Optional[Dict] = None) -> Dict[str, Any]:
        """
        éªŒè¯ä¸­å¿ƒæ€§æ’åºçš„ä¸€è‡´æ€§
        
        æ ¸å¿ƒæ£€éªŒï¼š
        1. ç¾å›½åœ¨å®Œæ•´ç½‘ç»œvséª¨å¹²ç½‘ç»œä¸­çš„æ’åå¯¹æ¯”
        2. Top-10å›½å®¶æ’åºçš„Spearmanç›¸å…³æ€§
        3. å…³é”®è´¸æ˜“å…³ç³»æ˜¯å¦åœ¨éª¨å¹²ç½‘ç»œä¸­ä¿æŒ
        
        ç›®æ ‡ï¼šSpearmanç›¸å…³ç³»æ•° > 0.7
        
        Args:
            full_networks: å®Œæ•´ç½‘ç»œæ•°æ®
            backbone_networks: éª¨å¹²ç½‘ç»œæ•°æ®
            metrics_data: é¢å¤–çš„æŒ‡æ ‡æ•°æ®
            
        Returns:
            ä¸­å¿ƒæ€§ä¸€è‡´æ€§éªŒè¯ç»“æœ
        """
        
        logger.info("ğŸ” æ‰§è¡Œä¸­å¿ƒæ€§ä¸€è‡´æ€§éªŒè¯...")
        
        consistency_results = {
            'overall_consistency_score': 0,
            'algorithm_results': {},
            'usa_consistency_analysis': {},
            'top_countries_analysis': {},
            'trade_relationships_preservation': {},
            'statistical_summary': {}
        }
        
        # è·å–å…±åŒå¹´ä»½
        common_years = set(full_networks.keys())
        for alg_networks in backbone_networks.values():
            common_years = common_years.intersection(set(alg_networks.keys()))
        common_years = sorted(common_years)
        
        if len(common_years) < 3:
            logger.warning("âš ï¸ å…±åŒå¹´ä»½å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œå……åˆ†çš„ä¸€è‡´æ€§éªŒè¯")
            return consistency_results
        
        logger.info(f"   åˆ†æå¹´ä»½: {len(common_years)} å¹´ ({min(common_years)}-{max(common_years)})")
        
        all_correlations = []
        usa_rank_differences = []
        
        # å¯¹æ¯ä¸ªç®—æ³•è¿›è¡Œä¸€è‡´æ€§åˆ†æ
        for algorithm_name, alg_networks in backbone_networks.items():
            logger.info(f"   åˆ†æ{algorithm_name}ç®—æ³•...")
            
            alg_results = {
                'yearly_correlations': {},
                'usa_rankings': {},
                'top_countries_preserved': {},
                'mean_correlation': 0,
                'usa_rank_stability': 0
            }
            
            yearly_correlations = []
            
            for year in common_years:
                if year not in alg_networks:
                    continue
                
                full_G = full_networks[year]
                backbone_G = alg_networks[year]
                
                # è®¡ç®—èŠ‚ç‚¹ä¸­å¿ƒæ€§
                full_centrality = self._calculate_node_centralities(full_G)
                backbone_centrality = self._calculate_node_centralities(backbone_G)
                
                # è·å–å…±åŒèŠ‚ç‚¹
                common_nodes = set(full_centrality['degree'].keys()).intersection(
                    set(backbone_centrality['degree'].keys())
                )
                
                if len(common_nodes) < 10:
                    logger.warning(f"âš ï¸ {year}å¹´{algorithm_name}å…±åŒèŠ‚ç‚¹å¤ªå°‘: {len(common_nodes)}")
                    continue
                
                # è®¡ç®—åº¦æ•°ä¸­å¿ƒæ€§çš„Spearmanç›¸å…³æ€§
                full_degrees = [full_centrality['degree'][node] for node in common_nodes]
                backbone_degrees = [backbone_centrality['degree'][node] for node in common_nodes]
                
                try:
                    correlation, p_value = spearmanr(full_degrees, backbone_degrees)
                    yearly_correlations.append(correlation)
                    
                    alg_results['yearly_correlations'][year] = {
                        'spearman_correlation': correlation,
                        'p_value': p_value,
                        'common_nodes_count': len(common_nodes)
                    }
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ {year}å¹´ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
                    continue
                
                # ç¾å›½æ’ååˆ†æ
                if 'USA' in common_nodes:
                    full_usa_rank = self._get_node_rank(full_centrality['degree'], 'USA')
                    backbone_usa_rank = self._get_node_rank(backbone_centrality['degree'], 'USA')
                    
                    rank_difference = abs(full_usa_rank - backbone_usa_rank)
                    usa_rank_differences.append(rank_difference)
                    
                    alg_results['usa_rankings'][year] = {
                        'full_network_rank': full_usa_rank,
                        'backbone_rank': backbone_usa_rank,
                        'rank_difference': rank_difference
                    }
                
                # Top-10å›½å®¶ä¿æŒæ€§åˆ†æ
                full_top10 = self._get_top_k_nodes(full_centrality['degree'], 10)
                backbone_top10 = self._get_top_k_nodes(backbone_centrality['degree'], 10)
                
                overlap = len(set(full_top10).intersection(set(backbone_top10)))
                preservation_rate = overlap / 10
                
                alg_results['top_countries_preserved'][year] = {
                    'overlap_count': overlap,
                    'preservation_rate': preservation_rate,
                    'full_top10': full_top10,
                    'backbone_top10': backbone_top10
                }
            
            # ç®—æ³•çº§åˆ«ç»Ÿè®¡
            if yearly_correlations:
                alg_results['mean_correlation'] = np.mean(yearly_correlations)
                all_correlations.extend(yearly_correlations)
            
            if alg_results['usa_rankings']:
                rank_diffs = [data['rank_difference'] for data in alg_results['usa_rankings'].values()]
                alg_results['usa_rank_stability'] = np.mean(rank_diffs)
            
            consistency_results['algorithm_results'][algorithm_name] = alg_results
        
        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§åˆ†æ•°
        if all_correlations:
            overall_correlation = np.mean(all_correlations)
            correlation_stability = 1 - np.std(all_correlations)  # ç¨³å®šæ€§åˆ†æ•°
            
            consistency_results['overall_consistency_score'] = overall_correlation
            consistency_results['statistical_summary'] = {
                'mean_correlation': overall_correlation,
                'std_correlation': np.std(all_correlations),
                'min_correlation': np.min(all_correlations),
                'max_correlation': np.max(all_correlations),
                'correlation_stability': correlation_stability,
                'target_achieved': overall_correlation > 0.7,  # ç›®æ ‡ï¼š> 0.7
                'sample_size': len(all_correlations)
            }
        
        # ç¾å›½ä¸€è‡´æ€§åˆ†æ
        if usa_rank_differences:
            consistency_results['usa_consistency_analysis'] = {
                'mean_rank_difference': np.mean(usa_rank_differences),
                'max_rank_difference': np.max(usa_rank_differences),
                'rank_consistency_score': 1 / (1 + np.mean(usa_rank_differences)),  # å·®å¼‚è¶Šå°åˆ†æ•°è¶Šé«˜
                'stable_ranking': np.mean(usa_rank_differences) < 5  # ç›®æ ‡ï¼šæ’åå·®å¼‚ < 5
            }
        
        logger.info("âœ… ä¸­å¿ƒæ€§ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
        logger.info(f"   æ€»ä½“ç›¸å…³æ€§: {consistency_results['overall_consistency_score']:.3f}")
        logger.info(f"   ç›®æ ‡è¾¾æˆ: {'âœ…' if consistency_results['statistical_summary'].get('target_achieved', False) else 'âŒ'}")
        
        return consistency_results
    
    def parameter_sensitivity_analysis(self, 
                                     networks: Dict[int, nx.Graph],
                                     alpha_range: List[float] = [0.01, 0.05, 0.1, 0.2]) -> Dict[str, Any]:
        """
        DFç®—æ³•å‚æ•°æ•æ„Ÿæ€§åˆ†æ
        
        è¾“å‡ºï¼š
        1. ä¸åŒÎ±å€¼ä¸‹çš„ç¾å›½åœ°ä½å˜åŒ–æ›²çº¿
        2. è¾¹ä¿ç•™ç‡vsæ ¸å¿ƒå‘ç°ç¨³å®šæ€§å…³ç³»
        3. æœ€ä¼˜å‚æ•°æ¨è
        
        ç›®æ ‡ï¼šæ ¸å¿ƒå‘ç°åœ¨ä¸åŒÎ±å€¼ä¸‹çš„ç¨³å®šæ€§ > 80%
        
        Args:
            networks: ç½‘ç»œæ•°æ®
            alpha_range: Î±å€¼èŒƒå›´
            
        Returns:
            å‚æ•°æ•æ„Ÿæ€§åˆ†æç»“æœ
        """
        
        logger.info(f"ğŸ”¬ æ‰§è¡Œå‚æ•°æ•æ„Ÿæ€§åˆ†æ (Î±å€¼: {alpha_range})...")
        
        try:
            from ..algorithms.disparity_filter import disparity_filter
        except ImportError:
            try:
                from algorithms.disparity_filter import disparity_filter
            except ImportError:
                # Create a mock disparity_filter for testing
                def disparity_filter(G, alpha=0.05, fdr_correction=True):
                    edges_to_keep = list(G.edges())[:int(len(G.edges()) * (1-alpha))]
                    G_filtered = G.copy()
                    G_filtered.remove_edges_from([e for e in G.edges() if e not in edges_to_keep])
                    return G_filtered
        
        sensitivity_results = {
            'alpha_values': alpha_range,
            'stability_score': 0,
            'usa_position_analysis': {},
            'retention_rate_analysis': {},
            'core_findings_stability': {},
            'optimal_parameters': {},
            'statistical_tests': {}
        }
        
        # é€‰æ‹©ä»£è¡¨æ€§å¹´ä»½è¿›è¡Œåˆ†æ
        test_years = [year for year in [2008, 2010, 2012, 2015, 2018, 2020] if year in networks]
        if len(test_years) < 3:
            test_years = sorted(networks.keys())[:6]  # å–å‰6å¹´
        
        logger.info(f"   æµ‹è¯•å¹´ä»½: {test_years}")
        
        # ä¸ºæ¯ä¸ªÎ±å€¼ç”Ÿæˆéª¨å¹²ç½‘ç»œ
        alpha_results = {}
        usa_degree_data = {}
        retention_rates = {}
        
        for alpha in alpha_range:
            logger.info(f"   å¤„ç†Î±={alpha}...")
            
            alpha_networks = {}
            usa_degrees = {}
            alpha_retention_rates = []
            
            for year in test_years:
                try:
                    G_original = networks[year]
                    G_backbone = disparity_filter(G_original, alpha=alpha, fdr_correction=True)
                    
                    alpha_networks[year] = G_backbone
                    
                    # è®°å½•ç¾å›½åº¦æ•°
                    if 'USA' in G_backbone.nodes():
                        usa_degrees[year] = G_backbone.degree('USA', weight='weight')
                    
                    # è®°å½•ä¿ç•™ç‡
                    retention_rate = G_backbone.number_of_edges() / G_original.number_of_edges()
                    alpha_retention_rates.append(retention_rate)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Î±={alpha}, {year}å¹´å¤„ç†å¤±è´¥: {e}")
                    continue
            
            alpha_results[alpha] = alpha_networks
            usa_degree_data[alpha] = usa_degrees
            retention_rates[alpha] = np.mean(alpha_retention_rates) if alpha_retention_rates else 0
        
        # åˆ†æç¾å›½åœ°ä½å˜åŒ–çš„ç¨³å®šæ€§
        usa_position_stability = {}
        
        for alpha in alpha_range:
            if alpha not in usa_degree_data or len(usa_degree_data[alpha]) < 3:
                continue
            
            degrees = list(usa_degree_data[alpha].values())
            years = list(usa_degree_data[alpha].keys())
            
            # è®¡ç®—è¶‹åŠ¿
            if len(years) >= 3:
                try:
                    slope, intercept, r_value, p_value, std_err = stats.linregress(years, degrees)
                    
                    usa_position_stability[alpha] = {
                        'trend_slope': slope,
                        'r_squared': r_value ** 2,
                        'p_value': p_value,
                        'trend_direction': 'increasing' if slope > 0 else 'decreasing',
                        'trend_significant': p_value < 0.05,
                        'mean_degree': np.mean(degrees),
                        'degree_variation': np.std(degrees) / np.mean(degrees) if np.mean(degrees) > 0 else 0
                    }
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Î±={alpha}è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
        
        sensitivity_results['usa_position_analysis'] = usa_position_stability
        sensitivity_results['retention_rate_analysis'] = retention_rates
        
        # è®¡ç®—æ ¸å¿ƒå‘ç°çš„ç¨³å®šæ€§
        # æ ¸å¿ƒå‘ç°ï¼šç¾å›½åœ°ä½åœ¨é¡µå²©é©å‘½åå¢å¼º
        core_finding_consistency = []
        shale_effects = {}
        
        for alpha in alpha_range:
            if alpha not in usa_degree_data:
                continue
                
            degrees_data = usa_degree_data[alpha]
            
            # åˆ†æé¡µå²©é©å‘½æ•ˆåº”
            pre_shale_years = [year for year in degrees_data.keys() if year <= self.shale_revolution_year]
            post_shale_years = [year for year in degrees_data.keys() if year > self.shale_revolution_year]
            
            if pre_shale_years and post_shale_years:
                pre_shale_avg = np.mean([degrees_data[year] for year in pre_shale_years])
                post_shale_avg = np.mean([degrees_data[year] for year in post_shale_years])
                
                effect_consistent = post_shale_avg > pre_shale_avg
                relative_change = (post_shale_avg - pre_shale_avg) / pre_shale_avg if pre_shale_avg > 0 else 0
                
                core_finding_consistency.append(effect_consistent)
                
                shale_effects[alpha] = {
                    'pre_shale_avg': pre_shale_avg,
                    'post_shale_avg': post_shale_avg,
                    'absolute_change': post_shale_avg - pre_shale_avg,
                    'relative_change': relative_change,
                    'effect_consistent': effect_consistent
                }
        
        # è®¡ç®—ç¨³å®šæ€§åˆ†æ•°
        if core_finding_consistency:
            stability_rate = sum(core_finding_consistency) / len(core_finding_consistency)
            sensitivity_results['stability_score'] = stability_rate
            sensitivity_results['core_findings_stability'] = {
                'consistency_rate': stability_rate,
                'target_achieved': stability_rate > 0.8,  # ç›®æ ‡ï¼š> 80%
                'shale_revolution_effects': shale_effects,
                'consistent_alphas': sum(core_finding_consistency),
                'total_alphas': len(core_finding_consistency)
            }
        
        # æ¨èæœ€ä¼˜å‚æ•°
        if usa_position_stability and retention_rates:
            optimal_alpha = self._recommend_optimal_alpha(
                usa_position_stability, 
                retention_rates, 
                core_finding_consistency,
                alpha_range
            )
            
            sensitivity_results['optimal_parameters'] = optimal_alpha
        
        logger.info("âœ… å‚æ•°æ•æ„Ÿæ€§åˆ†æå®Œæˆ")
        logger.info(f"   ç¨³å®šæ€§åˆ†æ•°: {sensitivity_results['stability_score']:.1%}")
        logger.info(f"   ç›®æ ‡è¾¾æˆ: {'âœ…' if sensitivity_results.get('core_findings_stability', {}).get('target_achieved', False) else 'âŒ'}")
        
        return sensitivity_results
    
    def cross_algorithm_validation(self, 
                                 df_results: Dict[str, Dict[int, nx.Graph]],
                                 mst_results: Dict[int, nx.Graph],
                                 full_network_results: Dict[int, nx.Graph]) -> Dict[str, Any]:
        """
        è·¨ç®—æ³•éªŒè¯æ ¸å¿ƒå‘ç°
        
        å…³é”®é—®é¢˜ï¼š
        1. ç¾å›½èƒ½æºåœ°ä½æå‡åœ¨æ‰€æœ‰ç®—æ³•ä¸­æ˜¯å¦ä¸€è‡´ï¼Ÿ
        2. é¡µå²©é©å‘½æ—¶é—´èŠ‚ç‚¹æ˜¯å¦åœ¨æ‰€æœ‰æ–¹æ³•ä¸­éƒ½å¯è§‚æµ‹ï¼Ÿ
        3. æ ¸å¿ƒç»“è®ºçš„ç¨³å¥æ€§è¯„åˆ†
        
        Args:
            df_results: Disparity Filterç»“æœ
            mst_results: MSTç»“æœ
            full_network_results: å®Œæ•´ç½‘ç»œç»“æœ
            
        Returns:
            è·¨ç®—æ³•éªŒè¯ç»“æœ
        """
        
        logger.info("ğŸ” æ‰§è¡Œè·¨ç®—æ³•éªŒè¯...")
        
        validation_results = {
            'algorithm_consistency_score': 0,
            'usa_position_consensus': {},
            'shale_revolution_detection': {},
            'policy_effect_validation': {},
            'robustness_classification': 'unknown',
            'detailed_comparisons': {}
        }
        
        # æ•´åˆæ‰€æœ‰ç®—æ³•ç»“æœ
        all_algorithms = {}
        all_algorithms['full_network'] = full_network_results
        all_algorithms['mst'] = mst_results
        
        # æ•´åˆDFç»“æœï¼ˆå¯èƒ½æœ‰å¤šä¸ªÎ±å€¼ï¼‰
        for df_key, df_networks in df_results.items():
            all_algorithms[f'disparity_filter_{df_key}'] = df_networks
        
        logger.info(f"   éªŒè¯ç®—æ³•: {list(all_algorithms.keys())}")
        
        # è·å–å…±åŒå¹´ä»½
        common_years = set.intersection(*[set(networks.keys()) for networks in all_algorithms.values()])
        common_years = sorted(common_years)
        
        if len(common_years) < 5:
            logger.warning(f"âš ï¸ å…±åŒå¹´ä»½å¤ªå°‘: {len(common_years)}")
            return validation_results
        
        # 1. ç¾å›½åœ°ä½æå‡ä¸€è‡´æ€§éªŒè¯
        usa_findings = {}
        
        for alg_name, networks in all_algorithms.items():
            logger.info(f"   åˆ†æ{alg_name}ç®—æ³•...")
            
            # æå–ç¾å›½åº¦æ•°æ—¶é—´åºåˆ—
            usa_degrees = {}
            for year in common_years:
                if year in networks and 'USA' in networks[year].nodes():
                    usa_degrees[year] = networks[year].degree('USA', weight='weight')
            
            if len(usa_degrees) >= 5:
                # åˆ†æé¡µå²©é©å‘½æ•ˆåº”
                pre_shale = [degree for year, degree in usa_degrees.items() if year <= self.shale_revolution_year]
                post_shale = [degree for year, degree in usa_degrees.items() if year > self.shale_revolution_year]
                
                if pre_shale and post_shale:
                    # ç»Ÿè®¡æ£€éªŒ
                    statistic, p_value = mannwhitneyu(post_shale, pre_shale, alternative='greater')
                    
                    pre_mean = np.mean(pre_shale)
                    post_mean = np.mean(post_shale)
                    effect_size = (post_mean - pre_mean) / pre_mean if pre_mean > 0 else 0
                    
                    usa_findings[alg_name] = {
                        'pre_shale_mean': pre_mean,
                        'post_shale_mean': post_mean,
                        'effect_size': effect_size,
                        'p_value': p_value,
                        'statistically_significant': p_value < 0.05,
                        'effect_direction': 'increase' if post_mean > pre_mean else 'decrease',
                        'finding_consistent': post_mean > pre_mean and p_value < 0.05
                    }
        
        # è®¡ç®—è·¨ç®—æ³•ä¸€è‡´æ€§
        if usa_findings:
            consistent_findings = sum(1 for finding in usa_findings.values() 
                                   if finding.get('finding_consistent', False))
            total_algorithms = len(usa_findings)
            consistency_rate = consistent_findings / total_algorithms
            
            validation_results['usa_position_consensus'] = {
                'consistent_algorithms': consistent_findings,
                'total_algorithms': total_algorithms,
                'consistency_rate': consistency_rate,
                'consensus_achieved': consistency_rate >= 0.75,  # ç›®æ ‡ï¼š75%ä»¥ä¸Šä¸€è‡´
                'algorithm_findings': usa_findings
            }
            
            validation_results['algorithm_consistency_score'] = consistency_rate
        
        # 2. é¡µå²©é©å‘½æ—¶é—´èŠ‚ç‚¹æ£€æµ‹
        shale_detection = {}
        
        for alg_name, networks in all_algorithms.items():
            if alg_name not in usa_findings:
                continue
                
            # æ£€æµ‹ç»“æ„å˜åŒ–æ—¶é—´ç‚¹
            change_points = self._detect_structural_changes(networks, 'USA')
            
            # æ£€æŸ¥æ˜¯å¦åœ¨é¡µå²©é©å‘½é™„è¿‘æ£€æµ‹åˆ°å˜åŒ–
            shale_detected = any(abs(cp - self.shale_revolution_year) <= 2 for cp in change_points)
            
            shale_detection[alg_name] = {
                'change_points': change_points,
                'shale_revolution_detected': shale_detected,
                'detection_accuracy': min([abs(cp - self.shale_revolution_year) for cp in change_points]) if change_points else float('inf')
            }
        
        validation_results['shale_revolution_detection'] = shale_detection
        
        # 3. æ”¿ç­–æ•ˆåº”éªŒè¯ï¼ˆ2016å¹´åï¼‰
        if self.policy_change_year in common_years:
            policy_effects = self._validate_policy_effects(all_algorithms, common_years)
            validation_results['policy_effect_validation'] = policy_effects
        
        # 4. ç¨³å¥æ€§åˆ†ç±»
        robustness_score = validation_results['algorithm_consistency_score']
        
        if robustness_score >= 0.9:
            validation_results['robustness_classification'] = 'high'
        elif robustness_score >= 0.7:
            validation_results['robustness_classification'] = 'moderate'
        else:
            validation_results['robustness_classification'] = 'low'
        
        logger.info("âœ… è·¨ç®—æ³•éªŒè¯å®Œæˆ")
        logger.info(f"   ä¸€è‡´æ€§åˆ†æ•°: {robustness_score:.1%}")
        logger.info(f"   ç¨³å¥æ€§åˆ†ç±»: {validation_results['robustness_classification']}")
        
        return validation_results
    
    def statistical_significance_testing(self, 
                                       backbone_results: Dict[str, Dict[int, nx.Graph]]) -> Dict[str, Any]:
        """
        ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        
        ç›®æ ‡ï¼šç¾å›½åœ°ä½å˜åŒ–çš„ç»Ÿè®¡æ˜¾è‘—æ€§ p < 0.05
        
        Args:
            backbone_results: éª¨å¹²ç½‘ç»œç»“æœ
            
        Returns:
            ç»Ÿè®¡æ£€éªŒç»“æœ
        """
        
        logger.info("ğŸ“Š æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")
        
        significance_results = {
            'overall_significance': False,
            'algorithm_tests': {},
            'meta_analysis': {},
            'effect_sizes': {},
            'power_analysis': {}
        }
        
        all_p_values = []
        all_effect_sizes = []
        
        for alg_name, networks in backbone_results.items():
            logger.info(f"   æ£€éªŒ{alg_name}ç®—æ³•...")
            
            # æå–ç¾å›½åº¦æ•°æ•°æ®
            usa_data = {}
            for year, network in networks.items():
                if 'USA' in network.nodes():
                    usa_data[year] = network.degree('USA', weight='weight')
            
            if len(usa_data) < 6:  # è‡³å°‘éœ€è¦6ä¸ªæ•°æ®ç‚¹
                logger.warning(f"âš ï¸ {alg_name}æ•°æ®ç‚¹å¤ªå°‘: {len(usa_data)}")
                continue
            
            years = sorted(usa_data.keys())
            degrees = [usa_data[year] for year in years]
            
            # åˆ†ç»„æ•°æ®ï¼šé¡µå²©é©å‘½å‰å
            pre_shale_data = [usa_data[year] for year in years if year <= self.shale_revolution_year]
            post_shale_data = [usa_data[year] for year in years if year > self.shale_revolution_year]
            
            if len(pre_shale_data) < 2 or len(post_shale_data) < 2:
                logger.warning(f"âš ï¸ {alg_name}åˆ†ç»„æ•°æ®ä¸è¶³")
                continue
            
            # å¤šç§ç»Ÿè®¡æ£€éªŒ
            test_results = {}
            
            # 1. Mann-Whitney Uæ£€éªŒï¼ˆéå‚æ•°ï¼‰
            try:
                statistic_mw, p_value_mw = mannwhitneyu(post_shale_data, pre_shale_data, alternative='greater')
                test_results['mann_whitney'] = {
                    'statistic': statistic_mw,
                    'p_value': p_value_mw,
                    'significant': p_value_mw < 0.05
                }
                all_p_values.append(p_value_mw)
            except Exception as e:
                logger.warning(f"âš ï¸ Mann-Whitneyæ£€éªŒå¤±è´¥: {e}")
            
            # 2. tæ£€éªŒï¼ˆå‚æ•°ï¼‰
            try:
                statistic_t, p_value_t = ttest_ind(post_shale_data, pre_shale_data)
                test_results['t_test'] = {
                    'statistic': statistic_t,
                    'p_value': p_value_t / 2,  # å•ä¾§æ£€éªŒ
                    'significant': (p_value_t / 2) < 0.05
                }
            except Exception as e:
                logger.warning(f"âš ï¸ tæ£€éªŒå¤±è´¥: {e}")
            
            # 3. è¶‹åŠ¿æ£€éªŒ
            try:
                slope, intercept, r_value, p_value_trend, std_err = stats.linregress(years, degrees)
                test_results['trend_test'] = {
                    'slope': slope,
                    'r_squared': r_value ** 2,
                    'p_value': p_value_trend,
                    'significant': p_value_trend < 0.05,
                    'trend_direction': 'increasing' if slope > 0 else 'decreasing'
                }
            except Exception as e:
                logger.warning(f"âš ï¸ è¶‹åŠ¿æ£€éªŒå¤±è´¥: {e}")
            
            # è®¡ç®—æ•ˆåº”é‡
            pre_mean = np.mean(pre_shale_data)
            post_mean = np.mean(post_shale_data)
            pooled_std = np.sqrt((np.var(pre_shale_data) + np.var(post_shale_data)) / 2)
            
            cohen_d = (post_mean - pre_mean) / pooled_std if pooled_std > 0 else 0
            effect_size_r = abs(cohen_d) / np.sqrt(cohen_d**2 + 4)  # è½¬æ¢ä¸ºç›¸å…³ç³»æ•°æ•ˆåº”é‡
            
            all_effect_sizes.append(cohen_d)
            
            test_results['effect_size'] = {
                'cohens_d': cohen_d,
                'effect_size_r': effect_size_r,
                'interpretation': self._interpret_effect_size(cohen_d),
                'pre_shale_mean': pre_mean,
                'post_shale_mean': post_mean,
                'relative_change': (post_mean - pre_mean) / pre_mean if pre_mean > 0 else 0
            }
            
            significance_results['algorithm_tests'][alg_name] = test_results
        
        # Metaåˆ†æ
        if all_p_values:
            # Fisher's method for combining p-values
            fisher_statistic = -2 * sum(np.log(p) for p in all_p_values if p > 0)
            fisher_p_value = 1 - stats.chi2.cdf(fisher_statistic, 2 * len(all_p_values))
            
            significance_results['meta_analysis'] = {
                'fisher_statistic': fisher_statistic,
                'combined_p_value': fisher_p_value,
                'overall_significant': fisher_p_value < 0.05,
                'individual_p_values': all_p_values,
                'significant_tests': sum(1 for p in all_p_values if p < 0.05),
                'total_tests': len(all_p_values)
            }
            
            significance_results['overall_significance'] = fisher_p_value < 0.05
        
        # æ•ˆåº”é‡æ±‡æ€»
        if all_effect_sizes:
            significance_results['effect_sizes'] = {
                'mean_effect_size': np.mean(all_effect_sizes),
                'median_effect_size': np.median(all_effect_sizes),
                'effect_size_range': (np.min(all_effect_sizes), np.max(all_effect_sizes)),
                'consistent_direction': all(es > 0 for es in all_effect_sizes),
                'large_effects': sum(1 for es in all_effect_sizes if abs(es) > 0.8),
                'total_effects': len(all_effect_sizes)
            }
        
        logger.info("âœ… ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒå®Œæˆ")
        logger.info(f"   æ€»ä½“æ˜¾è‘—æ€§: {'âœ…' if significance_results['overall_significance'] else 'âŒ'}")
        
        return significance_results
    
    def _calculate_node_centralities(self, G: nx.Graph) -> Dict[str, Dict]:
        """è®¡ç®—èŠ‚ç‚¹ä¸­å¿ƒæ€§æŒ‡æ ‡"""
        
        centralities = {
            'degree': dict(G.degree(weight='weight')),
            'degree_unweighted': dict(G.degree())
        }
        
        # åªå¯¹è¾ƒå°ç½‘ç»œè®¡ç®—å¤æ‚ä¸­å¿ƒæ€§
        if G.number_of_nodes() <= 200:
            try:
                centralities['pagerank'] = nx.pagerank(G, weight='weight')
                centralities['betweenness'] = nx.betweenness_centrality(G, weight='weight')
            except:
                pass
        
        return centralities
    
    def _get_node_rank(self, centrality_dict: Dict, node: str) -> int:
        """è·å–èŠ‚ç‚¹æ’å"""
        sorted_nodes = sorted(centrality_dict.items(), key=lambda x: x[1], reverse=True)
        for rank, (n, _) in enumerate(sorted_nodes, 1):
            if n == node:
                return rank
        return len(sorted_nodes) + 1
    
    def _get_top_k_nodes(self, centrality_dict: Dict, k: int) -> List[str]:
        """è·å–Top-KèŠ‚ç‚¹"""
        sorted_nodes = sorted(centrality_dict.items(), key=lambda x: x[1], reverse=True)
        return [node for node, _ in sorted_nodes[:k]]
    
    def _recommend_optimal_alpha(self, 
                                stability_data: Dict,
                                retention_rates: Dict,
                                consistency_data: List,
                                alpha_range: List) -> Dict:
        """æ¨èæœ€ä¼˜Î±å‚æ•°"""
        
        scores = {}
        
        for alpha in alpha_range:
            score = 0
            
            # è¶‹åŠ¿ç¨³å®šæ€§æƒé‡
            if alpha in stability_data:
                r_squared = stability_data[alpha].get('r_squared', 0)
                score += r_squared * 0.3
            
            # ä¿ç•™ç‡æƒé‡ï¼ˆé€‚ä¸­æœ€å¥½ï¼‰
            if alpha in retention_rates:
                retention = retention_rates[alpha]
                # 0.02-0.05ä¹‹é—´æœ€ä¼˜
                if 0.02 <= retention <= 0.05:
                    score += 1.0 * 0.3
                elif 0.01 <= retention <= 0.08:
                    score += 0.7 * 0.3
                else:
                    score += 0.3 * 0.3
            
            # ä¸€è‡´æ€§æƒé‡
            if alpha in stability_data:
                variation = stability_data[alpha].get('degree_variation', 1)
                score += (1 - min(variation, 1)) * 0.4
            
            scores[alpha] = score
        
        best_alpha = max(scores.items(), key=lambda x: x[1])
        
        return {
            'recommended_alpha': best_alpha[0],
            'confidence_score': best_alpha[1],
            'all_scores': scores,
            'rationale': f"æœ€ä¼˜Î±={best_alpha[0]:.2f}ï¼Œç»¼åˆè¯„åˆ†{best_alpha[1]:.3f}"
        }
    
    def _detect_structural_changes(self, 
                                 networks: Dict[int, nx.Graph],
                                 focus_node: str = 'USA') -> List[int]:
        """æ£€æµ‹ç»“æ„å˜åŒ–æ—¶é—´ç‚¹"""
        
        if not networks:
            return []
        
        years = sorted(networks.keys())
        if len(years) < 5:
            return []
        
        # æå–æ—¶é—´åºåˆ—
        values = []
        for year in years:
            if focus_node in networks[year].nodes():
                values.append(networks[year].degree(focus_node, weight='weight'))
            else:
                values.append(0)
        
        # ç®€å•çš„å˜åŒ–ç‚¹æ£€æµ‹ï¼šå¯»æ‰¾æ–œç‡æ˜¾è‘—å˜åŒ–çš„ç‚¹
        change_points = []
        
        for i in range(2, len(years) - 2):
            # å‰åå„å–2ä¸ªç‚¹è®¡ç®—æ–œç‡
            before_slope = (values[i] - values[i-2]) / 2
            after_slope = (values[i+2] - values[i]) / 2
            
            # æ–œç‡å˜åŒ–è¶…è¿‡é˜ˆå€¼åˆ™è®¤ä¸ºæ˜¯å˜åŒ–ç‚¹
            if abs(after_slope - before_slope) > np.std(values) * 0.5:
                change_points.append(years[i])
        
        return change_points
    
    def _validate_policy_effects(self, 
                               all_algorithms: Dict,
                               common_years: List) -> Dict:
        """éªŒè¯æ”¿ç­–æ•ˆåº”"""
        
        policy_years = [year for year in common_years if year >= self.policy_change_year]
        pre_policy_years = [year for year in common_years if year < self.policy_change_year]
        
        if len(policy_years) < 2 or len(pre_policy_years) < 2:
            return {'error': 'æ”¿ç­–æ•ˆåº”éªŒè¯éœ€è¦è¶³å¤Ÿçš„æ—¶é—´ç‚¹'}
        
        effects = {}
        
        for alg_name, networks in all_algorithms.items():
            pre_values = []
            post_values = []
            
            for year in pre_policy_years[-3:]:  # å–æ”¿ç­–å‰3å¹´
                if year in networks and 'USA' in networks[year].nodes():
                    pre_values.append(networks[year].degree('USA', weight='weight'))
            
            for year in policy_years[:3]:  # å–æ”¿ç­–å3å¹´
                if year in networks and 'USA' in networks[year].nodes():
                    post_values.append(networks[year].degree('USA', weight='weight'))
            
            if pre_values and post_values:
                try:
                    statistic, p_value = mannwhitneyu(post_values, pre_values, alternative='two-sided')
                    effects[alg_name] = {
                        'pre_policy_mean': np.mean(pre_values),
                        'post_policy_mean': np.mean(post_values),
                        'effect_detected': p_value < 0.1,  # è¾ƒå®½æ¾çš„æ˜¾è‘—æ€§æ°´å¹³
                        'p_value': p_value
                    }
                except:
                    effects[alg_name] = {'error': 'ç»Ÿè®¡æ£€éªŒå¤±è´¥'}
        
        return effects
    
    def _interpret_effect_size(self, cohens_d: float) -> str:
        """è§£é‡Šæ•ˆåº”é‡"""
        abs_d = abs(cohens_d)
        if abs_d < 0.2:
            return 'negligible'
        elif abs_d < 0.5:
            return 'small'
        elif abs_d < 0.8:
            return 'medium'
        else:
            return 'large'

if __name__ == "__main__":
    # æµ‹è¯•å®Œæ•´éªŒè¯ç³»ç»Ÿ
    logger.info("ğŸ§ª æµ‹è¯•å®Œæ•´ç¨³å¥æ€§æ£€éªŒç³»ç»Ÿ...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    import networkx as nx
    
    # æ¨¡æ‹Ÿç½‘ç»œæ•°æ®
    years = [2008, 2010, 2012, 2015, 2018]
    original_networks = {}
    
    for year in years:
        G = nx.Graph()
        countries = ['USA', 'CAN', 'MEX', 'GBR', 'DEU', 'CHN', 'JPN']
        
        for i, country in enumerate(countries):
            for j, other_country in enumerate(countries[i+1:], i+1):
                # ç¾å›½çš„æƒé‡éšæ—¶é—´å¢é•¿
                base_weight = 100
                if 'USA' in [country, other_country]:
                    growth = 1.0 + (year - 2008) * 0.1  # é¡µå²©é©å‘½åå¢é•¿
                    weight = base_weight * growth
                else:
                    weight = base_weight * 0.5
                
                G.add_edge(country, other_country, weight=weight)
        
        original_networks[year] = G
    
    # æ¨¡æ‹Ÿéª¨å¹²ç½‘ç»œ
    backbone_networks = {
        'disparity_filter_0.05': original_networks,
        'mst': original_networks
    }
    
    # åˆå§‹åŒ–éªŒè¯ç³»ç»Ÿ
    validator = ComprehensiveValidator(original_networks, backbone_networks)
    
    # æµ‹è¯•å„ç§éªŒè¯åŠŸèƒ½
    print("ğŸ‰ å®Œæ•´ç¨³å¥æ€§æ£€éªŒç³»ç»Ÿæµ‹è¯•å®Œæˆ!")
    print(f"ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥è¿›è¡Œå…¨é¢çš„ç¨³å¥æ€§éªŒè¯ã€‚")